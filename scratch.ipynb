{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is based on: https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport lda_pipeline\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"reddit_wsb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to set multiline=True, I had to update my system so that Spark would use Java 8, not Java 11. Even still, the column body containing commas within quotes containing quotes, and this confused the csv parser. Solved following https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas. In Arch, it was enought to run \"sudo archlinux-java set java-8-openjdk/\" after installing jdk8-openjdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[title: string, score: string, id: string, url: string, comms_num: string, created: string, body: string, timestamp: string]\n"
     ]
    }
   ],
   "source": [
    "# Note: Converting from Pandas df via df = spark.createDataFrame(df_pd) gives\n",
    "# >> WARN  TaskSetManager:66 - Stage 2 contains a task of very large size \n",
    "# >> (1473 KB). The maximum recommended task size is 100 KB.\n",
    "\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1); print(df)\n",
    "# print(f'{df.where(df[\"timestamp\"].isNull()).count()} null timestamp values.')\n",
    "\n",
    "# combine text columns and drop unwanted columns\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "\n",
    "texts = df.select(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]"
     ]
    }
   ],
   "source": [
    "pipeline = lda_pipeline.build_pipeline()\n",
    "processed_texts = pipeline.fit(texts).transform(texts)\n",
    "print(processed_texts)\n",
    "\n",
    "# for fair comparison with SpaCy below, should build pandas dataframe.\n",
    "# will throw TaskSetManager:66 - Stage 4 contains a task of very large size\n",
    "# df_post = processed_texts.toPandas()  \n",
    "# df_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‚Äòt', 'n‚Äôt', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '‚Äòd', '‚Äòll', '‚Äòm', '‚Äòre', '‚Äòs', '‚Äòve', '‚Äôd', '‚Äôll', '‚Äôm', '‚Äôre', '‚Äôs', '‚Äôve']\n"
     ]
    }
   ],
   "source": [
    "L = lda_pipeline.STOP_WORDS\n",
    "L.sort()\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to deal with:\n",
    "   - ‚úì long urls\n",
    "   - repeated characters as in \"holdddddd\" and \"woooooo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                     finished_unigrams|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[exit, system, ceo, nasdaq, push, halt, trade, investor, chance, recalibrate, position, sec, investigate, broker, disallow, buy, institution, flat,...|\n",
      "|                                                                                                                                    [wasnt, meme, gme]|\n",
      "|                                                                                                                            [yall, break, fix, advice]|\n",
      "|                                                                                                                                   [theyre, try, nazi]|\n",
      "|                                                                                                          [gme, blank, screen, shots, cali, time, wtf]|\n",
      "|                                                             [robinhood, literally, shut, ability, buy, certain, gme, effort, help, save, hedge, fund]|\n",
      "|                                                                                                                                         [pltr, thank]|\n",
      "|                                                [nasdaq, ceo, suggest, halt, trade, allow, big, investor, recalibrate, position, combat, reddit, user]|\n",
      "|[gme, war, suit, strike, warn, fellow, autists, obligatory, im, financial, advisor, blah, blah, blah, okay, gme, gang, weve, shorts, rope, theyre, ...|\n",
      "|[personal, need, scare, tactic, total, opposite, effect, use, opportunity, intimidate, isnt, gme, trade, future, know, obsessed, money, far, long, ...|\n",
      "|                                                                                                           [sec, loser, watch, elon, musk, speak, boy]|\n",
      "|[billion, trade, amc, expect, find, thread, miss, yolo, miss, people, amc, billion, trade, im, late, doesnt, appear, momentum, gme, wait, game, lot...|\n",
      "|                                                                       [buy, game, gamestop, donate, charity, goodwill, value, kid, play, video, game]|\n",
      "|                                                                                                                           [fuck, robinhood, palantir]|\n",
      "|                                                                                                                                                    []|\n",
      "|                                                                                         [nice, wake, brokerage, block, buy, gme, amc, em, dont, sell]|\n",
      "|                                                                                    [introduce, mom, reddit, follow, account, love, thank, enjoy, app]|\n",
      "|[issue, buy, gme, robinhood, app, wont, search, result, gme, notice, gme, nok, amc, searchable, robinhood, early, tagline, effect, stock, support, ...|\n",
      "|                                                                                                                         [trade, uk, restrict, access]|\n",
      "|                                                                                                                   [robinhood, big, mistake, thoughts]|\n",
      "|                                                                                                                   [sndl, time, buy, dont, wait, miss]|\n",
      "|                                                                                                                   [dont, sell, fight, hold, nok, gme]|\n",
      "|                                                                           [trade, ban, buy, gme, hop, bb, bus, stat, squeese, bb, hard, group, n, em]|\n",
      "|                                                                                                                              [hold, line, dont, sell]|\n",
      "|                                                                                                            [try, use, futu, momo, instead, robinhood]|\n",
      "|                                                                                                                                [robinhood, start, ww]|\n",
      "|                                                                                                                                        [fricked, lol]|\n",
      "|[block, robinhood, trade, platform, secret, diverse, brokerage, app, act, like, huge, cockblockers, purchase, gme, amc, bbby, halt, prevent, user, ...|\n",
      "|                                                                                                                               [aal, moon, think, yes]|\n",
      "|                                                                                         [robin, hood, spam, help, spam, robinhoods, help, center, bs]|\n",
      "|                                                      [robinhood, update, think, robin, hood, try, manipulate, try, update, app, notification, update]|\n",
      "|[careful, webull, chinaowned, company, theyre, currently, manhattan, base, incredibly, shady, wouldnt, trust, platform, year, work, nsa, intel, ana...|\n",
      "|                                                                                                                        [game, stop, stock, buy, miss]|\n",
      "|                                                             [flood, robinhood, mail, deserve, know, coward, rich, repay, webull, transfer, let, tank]|\n",
      "|                                                                                       [rh, unable, trade, unable, trade, robinhood, gme, amc, bb, aa]|\n",
      "|[dfv, launch, trade, app, witness, huge, manipulation, trade, app, restrict, user, sell, buy, allow, hedge, fund, trade, freely, easily, xb, dfv, d...|\n",
      "|[robinhood, tamper, post, let, know, robinhood, block, new, purchase, open, webull, account, maybe, new, buy, robinhood, complain, theyre, try, sto...|\n",
      "|                                                                                                                     [doge, crypto, doge, fast, right]|\n",
      "|                                                                                                                                    [freshly, squeeze]|\n",
      "|                                                                                                                     [think, like, clue, sht, wiiiild]|\n",
      "|                                                                                                                                   [time, shop, macys]|\n",
      "|                                                                                                                                          [buy, allow]|\n",
      "|                                                                                                                                    [iag, ibex, today]|\n",
      "|                                              [great, meme, stock, war, mm, force, trade, app, dirty, work, absolutely, desperate, hold, line, mongos]|\n",
      "|[rh, let, ppl, purchase, gme, amc, nok, list, bs, let, build, brokerage, fucking, bs, let, build, brokerage, let, ppl, actually, buy, want, doesnt,...|\n",
      "|                                                                                                                                   [robinhoods, fraud]|\n",
      "|                                                                              [bring, gold, moon, global, bank, short, gold, silver, good, investment]|\n",
      "|                                                                                                                                [choose, amc, bb, nok]|\n",
      "|[gme, short, interest, jan, ortex, case, definitively, post, im, post, confused, different, number, people, throw, feel, free, contribute, data, ac...|\n",
      "|                                                              [invest, ctrm, bb, amc, nok, night, robinhood, stock, arent, support, anymore, withdraw]|\n",
      "|                                                                                                                                  [robinhood, succumb]|\n",
      "|[vote, regulation, restriction, k, little, guy, suck, leave, day, trader, hear, protect, little, gay, cnbc, fucking, bullshit, dont, pretend, big, ...|\n",
      "|                                                                                                                    [remove, gme, amc, nok, sndl, way]|\n",
      "|                                                                                [stock, support, robinhood, tell, stock, isnt, support, try, buy, nok]|\n",
      "|                                                                                                                                            [let, fsr]|\n",
      "|                                                                                                                                                    []|\n",
      "|                                                                                                                                [sndl, nakd, thoughts]|\n",
      "|                                                                     [new, money, old, lady, bring, hard, earn, money, life, time, work, dude, yeehaa]|\n",
      "|[want, tell, cant, buy, gme, close, position, purchase, additional, share, know, robinhood, mean, theyre, ratfucking, itd, great, rate, reflect, ra...|\n",
      "|                                                          [robinhood, robhinhood, stop, support, gme, amc, nakd, bb, nok, know, add, fuel, fire, baby]|\n",
      "|                                                                                                                       [pltr, buy, hold, let, fucking]|\n",
      "|                                                                                                                                     [upvote, support]|\n",
      "|                                                                                                       [sec, cnbc, boomer, try, infiltrate, subreddit]|\n",
      "|                                                                                                                                           [amc, rise]|\n",
      "|                     [im, minimum, wage, cant, buy, try, buy, share, gme, cash, like, hour, ago, cant, sofi, hasnt, finish, review, account, hit, try]|\n",
      "|                                                                           [robinhood, robinhood, character, fuck, people, like, buy, doge, buy, doge]|\n",
      "|                                                       [star, review, robinhood, wanna, away, money, ill, away, star, need, hold, accountable, action]|\n",
      "|                                                                                                                                     [push, gme, moon]|\n",
      "|                                                                                                                                                    []|\n",
      "|                                                                                                                     [robinhood, allow, purchase, amc]|\n",
      "|                                                                                [hey, invest, doge, right, pull, stonk, pull, afternoon, trap, boomer]|\n",
      "|                                                               [bbw, build, bear, workshop, offer, high, quality, cotton, linen, friend, child, world]|\n",
      "|                                                                                                                              [buy, gm, gme, gme, gme]|\n",
      "|                                                                               [create, brokerage, blackjack, hooker, seriously, need, broker, people]|\n",
      "|                                              [nok, gnu, let, pay, bullshit, ah, pm, fellow, retard, let, butt, lickers, dont, fuck, break, try, hold]|\n",
      "|                                                                                                                                  [break, second, gme]|\n",
      "|[im, virgin, ape, join, buy, single, stock, read, gme, rocket, night, suit, deserve, lose, lose, big, cant, webull, site, load, robinhood, shut, ca...|\n",
      "|                                                                                 [webull, big, trade, platform, ready, buy, im, switch, yall, bernies]|\n",
      "|                                                                                                        [use, webull, buy, gme, webull, moment, allow]|\n",
      "|                                                                                                  [td, ameritrade, app, crash, tdameritrade, let, log]|\n",
      "|                                                                                                                     [buy, nok, yesterday, worry, pop]|\n",
      "|                                                                                                                                         [hold, boiss]|\n",
      "|                                                                                [ally, investment, account, wont, let, account, dont, gme, stonk, wtf]|\n",
      "|                                                                                                                                     [etoro, open, uk]|\n",
      "|                                       [finally, wire, money, vanguard, buy, gme, share, set, limit, sell, press, f, pay, respect, shorts, bahahahaha]|\n",
      "|                                                                                                             [webull, technical, difficulty, bullshit]|\n",
      "|                                 [im, fcking, sell, isnt, money, anymore, moon, yes, fuck, melvin, fuck, shitadel, fuck, robinhood, im, fucking, sell]|\n",
      "|                                                           [robinhood, anybody, robinhood, cancel, order, night, im, boy, explain, like, im, retarded]|\n",
      "|                                                                                                             [recovery, guy, stonk, daily, high, high]|\n",
      "|                                                                                                                   [little, help, matter, price, hold]|\n",
      "|                                                                                                                                                    []|\n",
      "|                          [warn, robinhood, cancel, pending, trade, wish, use, verbiage, notification, youd, choose, situation, email, fuck, asswipes]|\n",
      "|                                                              [fudge, online, broker, wont, let, buy, gme, amc, sndl, fking, casino, lose, regulation]|\n",
      "|                                                                                                                                           [robinhood]|\n",
      "|                                 [uk, press, create, fear, hold, page, guardian, uk, share, likely, fall, live, gamestop, surge, process, img, veusze]|\n",
      "|                                                                                                                                          [gme, pluto]|\n",
      "|                                                                                                 [break, news, user, world, reportably, fucking, sell]|\n",
      "|                                                                                                                                     [unity, buy, gme]|\n",
      "|                                                                           [robinhood, cancel, order, dump, money, robinhood, fuck, em, let, buy, app]|\n",
      "|                                     [robinhood, square, cash, let, buy, amc, nok, didnt, gme, start, fast, route, set, new, account, fuck, robinhood]|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_unigrams\"]).show(100, truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 3.46 ms, total: 14 ms\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='finished_unigrams',\n",
    "                         outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_texts)\n",
    "tf_result = tf_model.transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.28 ms, total: 12.8 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import IDF\n",
    "idfizer = IDF(inputCol='tf_features', \n",
    "              outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 431 ¬µs, total: 2.89 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "lda = LDA(k=num_topics, \n",
    "          maxIter=max_iter, \n",
    "          featuresCol=\"tf_idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 ms, sys: 9.54 ms, total: 18.8 ms\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------+\n",
      "|topic|                                              topicWords|\n",
      "+-----+--------------------------------------------------------+\n",
      "|    0|                             [buy, gme, short, hold, xb]|\n",
      "|    1|                           [', stock, market, it's, gon]|\n",
      "|    2|[clearinghouse, webull, portfolio, schwab, organization]|\n",
      "|    3|                      [security, margin, gon, uh, cheap]|\n",
      "|    4|                         [fund, hedge, money, let, play]|\n",
      "+-----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 5\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare pipeline time usage with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f58d2553d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f58d2569590>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f58d2830c20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f58d2830d70>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f58d24b7cd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f58d24c6e60>),\n",
       " ('preprocessor', <__main__.FilterTextPreprocessing at 0x7f58d27acdd0>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "data_path = \"reddit_wsb.csv\"\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class FilterTextPreprocessing:\n",
    "    def __init__(self, nlp):\n",
    "        Doc.set_extension('bow', default=[], force=True)\n",
    "        Token.set_extension('keep', default=True, force=True)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        patterns = [\n",
    "            {\"string_id\": \"stop_word\", \"pattern\": [[{\"IS_STOP\": True}]]},\n",
    "            {\"string_id\": \"punctuation\", \"pattern\": [[{\"IS_PUNCT\": True}]]},\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for patt_obj in patterns:\n",
    "            string_id = patt_obj.get('string_id')\n",
    "            pattern = patt_obj.get('pattern')\n",
    "            self.matcher.add(string_id, pattern, on_match=self.on_match)\n",
    "   \n",
    "    def on_match(self, matcher, doc, i, matches):\n",
    "        _, start, end = matches[i]\n",
    "        for tkn in doc[start:end]:\n",
    "            tkn._.keep = False\n",
    "              \n",
    "    def __call__(self, doc) :\n",
    "        self.matcher(doc)\n",
    "        doc._.bow = [tkn.lemma_ for tkn in doc if tkn._.keep]\n",
    "        return doc\n",
    "      \n",
    "#     @classmethod\n",
    "#     def from_pattern_file(cls, nlp, path) :\n",
    "#         patterns = read_json(path)\n",
    "#         return cls(nlp, patterns)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@English.factory(\"preprocessor\")\n",
    "def create_preprocessor(nlp, name):\n",
    "    return FilterTextPreprocessing(nlp)\n",
    "\n",
    "# nlp.select_pipes(enable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"preprocessor\", last=True)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1000\n",
      "i = 2000\n",
      "i = 3000\n",
      "i = 4000\n",
      "i = 5000\n",
      "i = 6000\n",
      "i = 7000\n",
      "i = 8000\n",
      "i = 9000\n",
      "i = 10000\n",
      "i = 11000\n",
      "i = 12000\n",
      "i = 13000\n",
      "i = 14000\n",
      "i = 15000\n",
      "i = 16000\n",
      "i = 17000\n",
      "i = 18000\n",
      "i = 19000\n",
      "i = 20000\n",
      "i = 21000\n",
      "i = 22000\n",
      "i = 23000\n",
      "i = 24000\n",
      "i = 25000\n",
      "CPU times: user 6min 7s, sys: 2.59 s, total: 6min 10s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process(filename):\n",
    "    with open(filename, \"r\") as fobj:\n",
    "        datareader = csv.DictReader(fobj)\n",
    "        for row in datareader:\n",
    "            text = \" \".join([row[\"title\"],\n",
    "                              row[\"body\"]])\n",
    "            yield nlp(text)\n",
    "            \n",
    "gen = process(data_path)\n",
    "\n",
    "words = []\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        doc = next(gen)\n",
    "        words.append(doc._.bow)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f\"i = {i}\")\n",
    "    \n",
    "words =  pd.Series(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [money, send, message, üöÄ, üíé, üôå]\n",
       "1        [Math, Professor, Scott, Steiner, say, number,...\n",
       "2        [exit, system, CEO, NASDAQ, push, halt, tradin...\n",
       "3             [new, SEC, filing, GME, retarded, interpret]\n",
       "4              [distract, GME, think, AMC, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                 [hold, GME, üöÄ, üöÄ, üöÄ]\n",
       "25644                    [AMC, Yolo, Update, Feb, 3, 2021]\n",
       "25645                                         [loss, sell]\n",
       "25646     [post, curiosity, teem, know, store, üëÄ, üíé, üñê, üöÄ]\n",
       "Length: 25647, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [money, send, message]\n",
       "1        [math, professor, scott, steiner, number, spel...\n",
       "2        [exit, system, ceo, nasdaq, push, halt, trade,...\n",
       "3               [new, sec, file, gme, retarded, interpret]\n",
       "4              [distract, gme, think, amc, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                          [hold, gme]\n",
       "25644                             [amc, yolo, update, feb]\n",
       "25645                                         [loss, sell]\n",
       "25646           [dont, post, curiosity, teem, know, store]\n",
       "Name: finished_unigrams, Length: 25647, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.finished_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.946236559139784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "371/18.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed comparison: The sparknlp pipeline took 18.6 seconds, while spaCy took 371 second (20x as long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with Stanza and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 25647 entries, 2021-01-28 21:37:41 to 2021-02-04 07:54:27\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      25647 non-null  object\n",
      " 1   title   25647 non-null  object\n",
      " 2   body    25647 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 801.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.read_csv(data_path,\n",
    "                 index_col=\"timestamp\", \n",
    "                 parse_dates=True, \n",
    "                 keep_default_na=False)\n",
    "# df_pd = df_pd.assign(timestamp=pd.to_datetime(df_pd.timestamp))\n",
    "df_pd = df_pd[[\"id\", \"title\", \"body\"]]\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = df.shape[0]//ddf.shape[0]\n",
    "dfs = [df.iloc[bin_size*i : bin_size*(i+1)] for i in range(ddf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I got in late on GME but I believe in the cause and am willing to lose it all.',\n",
       " \"You guys are amazing. Thank you for sending GME to the moon! I know I'm going to lose most of my money here because I'll hold the line until the end. Let's send a clear message to wall street with GME, BB, AMC, and any others. I've never day traded before but I'm in it now. üöÄ\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = dfs[0]\n",
    "X = df0.iloc[2]\n",
    "X.title, X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 15:42:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-14 15:42:21 INFO: Use device: cpu\n",
      "2021-02-14 15:42:21 INFO: Loading: tokenize\n",
      "2021-02-14 15:42:21 INFO: Loading: pos\n",
      "2021-02-14 15:42:21 INFO: Loading: lemma\n",
      "2021-02-14 15:42:21 INFO: Loading: depparse\n",
      "2021-02-14 15:42:21 INFO: Loading: sentiment\n",
      "2021-02-14 15:42:22 INFO: Loading: ner\n",
      "2021-02-14 15:42:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ You guys are amazing.\n",
      "+ Thank you for sending GME to the moon!\n",
      "- I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "‚ìÉ Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "‚ìÉ I've never day traded before but I'm in it now.\n",
      "‚ìÉ üöÄ\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "text = df.iloc[2].body\n",
    "doc = nlp(text)\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "for sent in doc.sentences:\n",
    "    print(d_sent[sent.sentiment], sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ I just love it when the regulators step in.\n",
      "- That was amazingly boring.\n",
      "+ That was amazingly tolerable.\n",
      "- At least it wasn't boring.\n",
      "CPU times: user 1.43 s, sys: 21.1 ms, total: 1.45 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in [\"I just love it when the regulators step in.\",\n",
    "          \"That was amazingly boring.\",\n",
    "          \"That was amazingly tolerable.\",\n",
    "          \"At least it wasn't boring.\"]:\n",
    "    sentiment = nlp(s).sentences[0].sentiment\n",
    "    print(d_sent[sentiment], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 You guys are amazing.\n",
      "0.0 Thank you for sending GME to the moon!\n",
      "0.0 I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "0.0 Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "0.0 I've never day traded before\n",
      "0.0 but I'm in it now.\n",
      "0.0 üöÄ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = df.iloc[2].body\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.sentiment, sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

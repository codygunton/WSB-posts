{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is based on: https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"reddit_wsb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to set multiline=True, I had to update my system so that Spark would use Java 8, not Java 11. Even still, the column body containing commas within quotes containing quotes, and this confused the csv parser. Solved following https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas. In Arch, it was enought to run \"sudo archlinux-java set java-8-openjdk/\" after installing jdk8-openjdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "from sparknlp.annotator import (Tokenizer, \n",
    "                                Normalizer, \n",
    "                                LemmatizerModel, \n",
    "                                StopWordsCleaner,\n",
    "                                NGramGenerator,\n",
    "                                PerceptronModel)\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS = list(STOP_WORDS)\n",
    "# STOP_WORDS.append(\"endtitle\")\n",
    "\n",
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "     .setInputCol(\"text\")\n",
    "     .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "     .setInputCols(['document'])\n",
    "     .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "normalizer = (\n",
    "    Normalizer()\n",
    "     .setInputCols(['tokenized'])\n",
    "     .setOutputCol('normalized')\n",
    "     .setLowercase(True)\n",
    "     .setCleanupPatterns(['[^A-Za-z\\']', \n",
    "                          'http.+'])\n",
    ")\n",
    "\n",
    "lemmatizer = (\n",
    "    LemmatizerModel.pretrained()\n",
    "     .setInputCols(['normalized'])\n",
    "     .setOutputCol('lemmatized')\n",
    ")\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner()\n",
    "     .setInputCols(['lemmatized'])\n",
    "     .setOutputCol('unigrams')\n",
    "     .setStopWords(STOP_WORDS)\n",
    ")\n",
    "\n",
    "ngrammer = (\n",
    "    NGramGenerator()\n",
    "    .setInputCols(['lemmatized'])\n",
    "    .setOutputCol('ngrams')\n",
    "    .setN(3)\n",
    "    .setEnableCumulative(True)\n",
    "    .setDelimiter('_')\n",
    ")\n",
    "\n",
    "\n",
    "pos_tagger = (\n",
    "    PerceptronModel.pretrained('pos_anc')\n",
    "    .setInputCols(['document', 'lemmatized'])\n",
    "    .setOutputCol('pos')\n",
    ")\n",
    "\n",
    "finisher = Finisher().setInputCols(['normalized', 'unigrams', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[title: string, score: string, id: string, url: string, comms_num: string, created: string, body: string, timestamp: string]\n"
     ]
    }
   ],
   "source": [
    "# Note: Converting from Pandas df via df = spark.createDataFrame(df_pd) gives\n",
    "# >> WARN  TaskSetManager:66 - Stage 2 contains a task of very large size \n",
    "# >> (1473 KB). The maximum recommended task size is 100 KB.\n",
    "\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "# df = df.repartition(16)   # chosen based WARN's\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1); print(df)\n",
    "# print(f'{df.where(df[\"timestamp\"].isNull()).count()} null timestamp values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# combine text columns and drop unwanted columns\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "print(df)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = (Pipeline()\n",
    "     .setStages([assembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 ngrammer,\n",
    "                 finisher\n",
    "                ]))\n",
    "\n",
    "texts = df.select(\"text\")\n",
    "processed_texts = pipeline.fit(texts).transform(texts)\n",
    "print(processed_texts)\n",
    "# for fair comparison with SpaCy below, should should build pandas dataframe.\n",
    "# df_post = processed_texts.toPandas()  # TaskSetManager:66 - Stage 4 contains a task of very large size\n",
    "# df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to deal with:\n",
    "   - ‚úì long urls\n",
    "   - repeated characters as in \"holdddddd\" and \"woooooo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                   finished_normalized|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[exit, the, system, the, ceo, of, nasdaq, pushed, to, halt, trading, to, give, investors, a, chance, to, recalibrate, their, positions, now, sec, i...|\n",
      "|                                                                                                                                 [wasnt, a, meme, gme]|\n",
      "|                                                                                                 [y'all, broke, it, how, do, we, fix, it, any, advice]|\n",
      "|                                                                                [they're, trying, to, say, this, was, all, done, by, ', nazis, ', now]|\n",
      "|                                                                     [why, is, gme, blank, i, have, screen, shots, am, cali, time, wtf, is, going, on]|\n",
      "|       [robinhood, has, literally, shut, down, the, ability, for, us, to, buy, certain, gme, calls, in, an, effort, to, help, save, the, hedge, funds]|\n",
      "|                                                                                                                          [pltr, thank, you, everyone]|\n",
      "|             [nasdaq, ceo, suggests, halt, to, trading, to, allow, big, investors, to, ', recalibrate, their, positions, ', to, combat, reddit, users]|\n",
      "|[gme, wars, suits, strike, back, warning, for, all, you, fellow, autists, obligatory, im, not, a, financial, advisor, this, is, not, blah, blah, bl...|\n",
      "|[this, is, personal, now, we, need, their, scare, tactics, to, have, the, total, opposite, effect, we, must, use, this, opportunity, to, show, we, ...|\n",
      "|                                                                                         [sec, losers, watch, this, elon, musk, speaks, for, us, boys]|\n",
      "|[with, well, over, a, billion, trades, on, amc, would, have, expected, to, find, a, thread, here, what, am, i, missing, yolo, what, am, i, missing,...|\n",
      "|                                             [buy, games, at, gamestop, donate, to, charity, not, goodwill, value, goes, up, kids, play, video, games]|\n",
      "|                                                                                                                   [fuck, robinhood, palantir, it, is]|\n",
      "|                                                                                                                                                    []|\n",
      "|[were, back, this, was, nice, to, wake, up, to, except, almost, all, brokerages, have, blocked, buying, of, gme, and, amc, among, others, if, you, ...|\n",
      "|[just, introduced, my, mom, to, reddit, and, made, her, follow, this, as, her, first, account, and, she, is, loving, it, thank, you, for, making, h...|\n",
      "|[anyone, else, having, issues, buying, gme, on, robinhood, the, app, wont, show, the, search, results, for, gme, i, noticed, gme, nok, amc, are, no...|\n",
      "|                                                                                                               [trading, uk, now, restricting, access]|\n",
      "|                                                                                              [robinhood, just, made, a, big, mistake, your, thoughts]|\n",
      "|                                                                                               [sndl, its, time, to, buy, don't, wait, and, miss, out]|\n",
      "|                                                                                                       [don't, sell, fight, back, and, hold, nok, gme]|\n",
      "|[trading, just, banned, us, from, buying, gme, hop, on, the, bb, bus, stat, well, squeese, bb, just, as, hard, if, not, more, group, up, n, take, e...|\n",
      "|                                                                                                                         [hold, the, line, dont, sell]|\n",
      "|                                                                                                        [try, use, futu, momo, instead, of, robinhood]|\n",
      "|                                                                                                                        [robinhood, just, started, ww]|\n",
      "|                                                                                                                                   [get, fricked, lol]|\n",
      "|[the, blocking, of, robinhood, trading, and, others, which, platform, is, next, it's, no, secret, that, that, diverse, brokerage, apps, act, like, ...|\n",
      "|                                                                                                               [is, aal, to, the, moon, i, think, yes]|\n",
      "|                                                            [robin, hoods, spam, help, everyone, please, spam, robinhoods, help, center, this, is, bs]|\n",
      "|[robinhood, update, do, you, think, robin, hood, is, trying, to, manipulate, everything, by, trying, to, update, the, app, i, just, got, a, notific...|\n",
      "|[be, careful, with, webull, it's, a, chinaowned, company, even, if, they're, currently, manhattan, based, and, they, are, incredibly, shady, i, wou...|\n",
      "|                                                                            [game, stop, stock, should, buy, now, or, did, i, miss, the, what's, next]|\n",
      "|[flood, robinhood, with, the, mail, they, deserve, we, all, know, they, are, cowards, we, made, them, rich, and, this, is, how, they, repay, us, we...|\n",
      "|                                                             [rh, unable, to, trade, anyone, else, unable, to, trade, in, robinhood, gme, amc, bb, aa]|\n",
      "|[dfv, should, launch, his, own, trading, app, we, are, witnessing, huge, manipulation, by, the, trading, apps, which, have, restricted, the, users,...|\n",
      "|[robinhood, tampering, just, posting, to, let, everyone, know, robinhood, is, blocking, new, purchases, open, a, webull, account, maybe, for, new, ...|\n",
      "|                                                                        [doge, the, crypto, doge, is, moving, faster, then, it, ever, had, right, now]|\n",
      "|                                                                                                                                   [freshly, squeezed]|\n",
      "|                                                                  [i, think, i, like, it, i, have, no, clue, about, this, but, this, sht, is, wiiiild]|\n",
      "|                                                                                                                          [time, to, shop, at, macy's]|\n",
      "|                                                                                                                [how, can, you, buy, if, not, allowed]|\n",
      "|                                                                                                                                    [iag, ibex, today]|\n",
      "|[the, great, meme, stock, wars, if, the, mm, are, forcing, trading, apps, to, do, their, dirty, work, they, must, be, absolutely, desperate, hold, ...|\n",
      "|[rh, not, letting, ppl, purchase, gme, amc, nok, the, list, goes, on, this, is, bs, lets, build, a, brokerage, this, is, fucking, bs, lets, build, ...|\n",
      "|                                                                                                                             [robinhoods, are, frauds]|\n",
      "|                                 [bringing, gold, to, the, moon, global, banks, have, been, shorting, gold, and, silver, good, next, move, investment]|\n",
      "|                                                                                                                      [what, to, choose, amc, bb, nok]|\n",
      "|[gme, short, interest, on, jan, from, ortex, in, case, this, has, not, been, definitively, posted, i'm, posting, this, for, myself, and, others, be...|\n",
      "|      [i, invested, in, ctrm, bb, amc, nok, last, night, on, robinhood, but, now, that, those, stocks, arent, supported, anymore, should, i, withdraw]|\n",
      "|                                                                                                                           [robinhood, has, succumbed]|\n",
      "|[vote, against, more, regulation, the, restriction, under, k, on, the, little, guy, sucks, leave, us, day, traders, alone, i, hear, how, they, keep...|\n",
      "|                                                                                              [they, removed, gme, amc, and, nok, sndl, all, the, way]|\n",
      "|                 [stock, not, supported, robinhood, is, telling, me, that, the, stock, isn't, supported, when, trying, to, buy, nok, what, can, i, do]|\n",
      "|                                                                                                                                  [lets, go, get, fsr]|\n",
      "|                                                                                                                                                    []|\n",
      "|                                                                                                                                [sndl, nakd, thoughts]|\n",
      "|                                          [new, money, old, lady, bringing, in, my, hard, earned, money, from, a, life, time, of, work, dudes, yeehaa]|\n",
      "|[someone, want, to, tell, me, why, i, cant, buy, more, gme, you, can, close, out, your, positions, but, cannot, purchase, additional, shares, anyon...|\n",
      "|[how, can, robinhood, do, this, robhinhood, has, stopped, supporting, gme, amc, nakd, bb, nok, and, who, knows, what, else, just, adding, fuel, to,...|\n",
      "|                                                                                        [pltr, is, the, next, move, buy, and, hold, lets, fucking, go]|\n",
      "|                                                                                                        [upvote, so, everyone, sees, we, got, support]|\n",
      "|                                                                                    [sec, and, cnbc, boomers, trying, to, infiltrate, this, subreddit]|\n",
      "|                                                                                                                                   [amc, better, rise]|\n",
      "|[im, minimum, wage, cant, buy, anywhere, tried, to, buy, shares, of, gme, all, the, cash, i, have, when, it, was, at, like, an, hour, ago, cant, be...|\n",
      "|                                               [robinhood, when, did, robinhood, the, character, fuck, people, over, like, this, buy, doge, buy, doge]|\n",
      "|[star, review, for, robinhood, if, they, wanna, take, away, my, money, ill, take, away, there, stars, we, need, to, hold, them, accountable, for, t...|\n",
      "|                                                                                                [push, gme, to, we, got, this, first, then, the, moon]|\n",
      "|                                                                                                                                                    []|\n",
      "|                                                                                                     [robinhood, just, allowed, me, to, purchase, amc]|\n",
      "|          [hey, invest, in, doge, right, now, everyone, is, pulling, out, of, stonks, and, pull, out, before, this, afternoon, to, trap, the, boomers]|\n",
      "|                   [bbw, anyone, build, a, bear, workshop, offers, the, highest, quality, cotton, linen, friends, to, children, all, over, the, world]|\n",
      "|                                                                                                                              [buy, gm, gme, gme, gme]|\n",
      "|             [we, should, create, our, own, brokerage, with, blackjack, and, hookers, but, seriously, we, need, a, broker, that, is, for, the, people]|\n",
      "|[nok, and, gnus, to, the, let, make, them, pay, for, the, bullshit, ah, and, pm, fellow, retards, let, show, these, butt, lickers, you, dont, fuck,...|\n",
      "|                                                                                                       [we, almost, broke, there, for, a, second, gme]|\n",
      "|[i'm, a, virgin, ape, how, do, i, join, i, have, never, bought, a, single, stock, i, have, been, reading, about, the, gme, rocket, since, last, nig...|\n",
      "|                [webull, still, has, the, big, four, on, their, trading, platform, ready, to, buy, i'm, switching, over, now, see, yall, at, bernie's]|\n",
      "|                                                                                      [use, webull, to, buy, gme, webull, at, the, moment, allows, it]|\n",
      "|                                                                     [td, ameritrade, app, has, crashed, tdameritrade, its, not, letting, me, log, in]|\n",
      "|                                                                                    [bought, nok, yesterday, getting, worried, will, this, still, pop]|\n",
      "|                                                                                                                             [hold, everything, boiss]|\n",
      "|                                         [my, ally, investment, account, won't, let, me, see, my, accounts, i, don't, even, own, any, gme, stonk, wtf]|\n",
      "|                                                                                                               [etoro, open, for, anyone, in, the, uk]|\n",
      "|[finally, i, just, wired, money, to, vanguard, bought, a, gme, share, for, and, set, a, limit, sell, for, press, f, to, pay, respects, to, all, the...|\n",
      "|                                                                                          [webull, having, technical, difficulties, i, call, bullshit]|\n",
      "|[im, not, fcking, selling, this, isnt, about, money, anymore, to, the, moon, yes, but, fuck, melvin, fuck, shitadel, fuck, robinhood, im, not, fuck...|\n",
      "|        [robinhood, anybody, get, why, robinhood, cancelled, my, orders, that, i, put, in, last, night, im, just, a, boy, explain, like, im, retarded]|\n",
      "|                                                                 [now, thats, a, recovery, guy, stonks, go, up, we, are, making, daily, higher, highs]|\n",
      "|                                                                                                  [every, little, helps, no, matter, the, price, hold]|\n",
      "|                                                                                                                                                    []|\n",
      "|[warning, robinhood, not, only, cancels, your, pending, trades, against, your, wishes, it, uses, the, same, verbiage, and, notifications, as, if, y...|\n",
      "|                               [these, fudging, online, broker, wont, let, me, buy, gme, amc, sndl, fking, casino, losing, and, call, for, regulation]|\n",
      "|                                                                                                            [where, do, we, move, to, from, robinhood]|\n",
      "|[uk, press, also, creating, fear, hold, front, page, of, the, guardian, uk, shares, likely, to, fall, live, gamestop, surges, processing, img, veusze]|\n",
      "|                                                                                                                                      [gme, to, pluto]|\n",
      "|                                             [breaking, news, this, just, in, users, around, the, world, are, reportably, ', not, fucking, selling, ']|\n",
      "|                                                                                                                 [unity, gone, but, anyways, buy, gme]|\n",
      "|[robinhood, canceled, my, orders, what, are, we, dumping, money, into, thats, still, on, robinhood, fuck, em, lets, buy, something, still, on, the,...|\n",
      "|[with, robinhood, down, square, cash, is, still, letting, you, buy, amc, and, nok, they, didn't, have, gme, before, this, started, and, it's, the, ...|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_normalized\"]).show(100, truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 3.46 ms, total: 14 ms\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='finished_unigrams',\n",
    "                         outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_texts)\n",
    "tf_result = tf_model.transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.28 ms, total: 12.8 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import IDF\n",
    "idfizer = IDF(inputCol='tf_features', \n",
    "              outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 431 ¬µs, total: 2.89 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "lda = LDA(k=num_topics, \n",
    "          maxIter=max_iter, \n",
    "          featuresCol=\"tf_idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 ms, sys: 9.54 ms, total: 18.8 ms\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------+\n",
      "|topic|                                              topicWords|\n",
      "+-----+--------------------------------------------------------+\n",
      "|    0|                             [buy, gme, short, hold, xb]|\n",
      "|    1|                           [', stock, market, it's, gon]|\n",
      "|    2|[clearinghouse, webull, portfolio, schwab, organization]|\n",
      "|    3|                      [security, margin, gon, uh, cheap]|\n",
      "|    4|                         [fund, hedge, money, let, play]|\n",
      "+-----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 5\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare pipeline time usage with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f58d2553d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f58d2569590>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f58d2830c20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f58d2830d70>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f58d24b7cd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f58d24c6e60>),\n",
       " ('preprocessor', <__main__.FilterTextPreprocessing at 0x7f58d27acdd0>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "data_path = \"reddit_wsb.csv\"\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class FilterTextPreprocessing:\n",
    "    def __init__(self, nlp):\n",
    "        Doc.set_extension('bow', default=[], force=True)\n",
    "        Token.set_extension('keep', default=True, force=True)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        patterns = [\n",
    "            {\"string_id\": \"stop_word\", \"pattern\": [[{\"IS_STOP\": True}]]},\n",
    "            {\"string_id\": \"punctuation\", \"pattern\": [[{\"IS_PUNCT\": True}]]},\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for patt_obj in patterns:\n",
    "            string_id = patt_obj.get('string_id')\n",
    "            pattern = patt_obj.get('pattern')\n",
    "            self.matcher.add(string_id, pattern, on_match=self.on_match)\n",
    "   \n",
    "    def on_match(self, matcher, doc, i, matches):\n",
    "        _, start, end = matches[i]\n",
    "        for tkn in doc[start:end]:\n",
    "            tkn._.keep = False\n",
    "              \n",
    "    def __call__(self, doc) :\n",
    "        self.matcher(doc)\n",
    "        doc._.bow = [tkn.lemma_ for tkn in doc if tkn._.keep]\n",
    "        return doc\n",
    "      \n",
    "#     @classmethod\n",
    "#     def from_pattern_file(cls, nlp, path) :\n",
    "#         patterns = read_json(path)\n",
    "#         return cls(nlp, patterns)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@English.factory(\"preprocessor\")\n",
    "def create_preprocessor(nlp, name):\n",
    "    return FilterTextPreprocessing(nlp)\n",
    "\n",
    "# nlp.select_pipes(enable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"preprocessor\", last=True)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1000\n",
      "i = 2000\n",
      "i = 3000\n",
      "i = 4000\n",
      "i = 5000\n",
      "i = 6000\n",
      "i = 7000\n",
      "i = 8000\n",
      "i = 9000\n",
      "i = 10000\n",
      "i = 11000\n",
      "i = 12000\n",
      "i = 13000\n",
      "i = 14000\n",
      "i = 15000\n",
      "i = 16000\n",
      "i = 17000\n",
      "i = 18000\n",
      "i = 19000\n",
      "i = 20000\n",
      "i = 21000\n",
      "i = 22000\n",
      "i = 23000\n",
      "i = 24000\n",
      "i = 25000\n",
      "CPU times: user 6min 7s, sys: 2.59 s, total: 6min 10s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process(filename):\n",
    "    with open(filename, \"r\") as fobj:\n",
    "        datareader = csv.DictReader(fobj)\n",
    "        for row in datareader:\n",
    "            text = \" \".join([row[\"title\"],\n",
    "                              row[\"body\"]])\n",
    "            yield nlp(text)\n",
    "            \n",
    "gen = process(data_path)\n",
    "\n",
    "words = []\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        doc = next(gen)\n",
    "        words.append(doc._.bow)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f\"i = {i}\")\n",
    "    \n",
    "words =  pd.Series(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [money, send, message, üöÄ, üíé, üôå]\n",
       "1        [Math, Professor, Scott, Steiner, say, number,...\n",
       "2        [exit, system, CEO, NASDAQ, push, halt, tradin...\n",
       "3             [new, SEC, filing, GME, retarded, interpret]\n",
       "4              [distract, GME, think, AMC, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                 [hold, GME, üöÄ, üöÄ, üöÄ]\n",
       "25644                    [AMC, Yolo, Update, Feb, 3, 2021]\n",
       "25645                                         [loss, sell]\n",
       "25646     [post, curiosity, teem, know, store, üëÄ, üíé, üñê, üöÄ]\n",
       "Length: 25647, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [money, send, message]\n",
       "1        [math, professor, scott, steiner, number, spel...\n",
       "2        [exit, system, ceo, nasdaq, push, halt, trade,...\n",
       "3               [new, sec, file, gme, retarded, interpret]\n",
       "4              [distract, gme, think, amc, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                          [hold, gme]\n",
       "25644                             [amc, yolo, update, feb]\n",
       "25645                                         [loss, sell]\n",
       "25646           [dont, post, curiosity, teem, know, store]\n",
       "Name: finished_unigrams, Length: 25647, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.finished_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.946236559139784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "371/18.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed comparison: The sparknlp pipeline took 18.6 seconds, while spaCy took 371 second (20x as long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with Stanza and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 25647 entries, 2021-01-28 21:37:41 to 2021-02-04 07:54:27\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      25647 non-null  object\n",
      " 1   title   25647 non-null  object\n",
      " 2   body    25647 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 801.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.read_csv(data_path,\n",
    "                 index_col=\"timestamp\", \n",
    "                 parse_dates=True, \n",
    "                 keep_default_na=False)\n",
    "# df_pd = df_pd.assign(timestamp=pd.to_datetime(df_pd.timestamp))\n",
    "df_pd = df_pd[[\"id\", \"title\", \"body\"]]\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = df.shape[0]//ddf.shape[0]\n",
    "dfs = [df.iloc[bin_size*i : bin_size*(i+1)] for i in range(ddf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I got in late on GME but I believe in the cause and am willing to lose it all.',\n",
       " \"You guys are amazing. Thank you for sending GME to the moon! I know I'm going to lose most of my money here because I'll hold the line until the end. Let's send a clear message to wall street with GME, BB, AMC, and any others. I've never day traded before but I'm in it now. üöÄ\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = dfs[0]\n",
    "X = df0.iloc[2]\n",
    "X.title, X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 15:42:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-14 15:42:21 INFO: Use device: cpu\n",
      "2021-02-14 15:42:21 INFO: Loading: tokenize\n",
      "2021-02-14 15:42:21 INFO: Loading: pos\n",
      "2021-02-14 15:42:21 INFO: Loading: lemma\n",
      "2021-02-14 15:42:21 INFO: Loading: depparse\n",
      "2021-02-14 15:42:21 INFO: Loading: sentiment\n",
      "2021-02-14 15:42:22 INFO: Loading: ner\n",
      "2021-02-14 15:42:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ You guys are amazing.\n",
      "+ Thank you for sending GME to the moon!\n",
      "- I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "‚ìÉ Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "‚ìÉ I've never day traded before but I'm in it now.\n",
      "‚ìÉ üöÄ\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "text = df.iloc[2].body\n",
    "doc = nlp(text)\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "for sent in doc.sentences:\n",
    "    print(d_sent[sent.sentiment], sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ I just love it when the regulators step in.\n",
      "- That was amazingly boring.\n",
      "+ That was amazingly tolerable.\n",
      "- At least it wasn't boring.\n",
      "CPU times: user 1.43 s, sys: 21.1 ms, total: 1.45 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in [\"I just love it when the regulators step in.\",\n",
    "          \"That was amazingly boring.\",\n",
    "          \"That was amazingly tolerable.\",\n",
    "          \"At least it wasn't boring.\"]:\n",
    "    sentiment = nlp(s).sentences[0].sentiment\n",
    "    print(d_sent[sentiment], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 You guys are amazing.\n",
      "0.0 Thank you for sending GME to the moon!\n",
      "0.0 I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "0.0 Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "0.0 I've never day traded before\n",
      "0.0 but I'm in it now.\n",
      "0.0 üöÄ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = df.iloc[2].body\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.sentiment, sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "martial-individual",
   "metadata": {},
   "source": [
    "# Bag of Words, Bag of Emojis and n-Grams pipeline\n",
    "A work in progress, this pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. The token information is preserved with the special unicode character â“” (a circled-e; U+24d4). This approach has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used as 'decorators'. It is partially a workaround to handle the fact that there is (apparently) no good solution for normalizing long strings of emojis (native to Spark NLP).\n",
    "\n",
    "References: The O'Reilly Spark NLP book, page 76 and https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "variable-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "sys.path.append('..')\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consecutive-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.6 ms, sys: 6.54 ms, total: 12.1 ms\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "# df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))\n",
    "\n",
    "emojis_regex = \"[\"+\"\".join(pipelines.emoji_ranges)+\"]\"\n",
    "\n",
    "texts = (\n",
    "    df.withColumn(\"text_no_emojis\",\n",
    "                  F.regexp_replace(df[\"text\"],\n",
    "                                   emojis_regex, \"â’º\"))\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                  F.regexp_replace(\"text_no_emojis\", \"[â€œâ€]\", \"\\\"\"))\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                F.regexp_replace(\"text_no_emojis\", \"[â€˜â€™]\", \"\\'\"))\n",
    "    # to keep positions of emojis (not necessary, currently)\n",
    "    .select([\"text\", \"text_no_emojis\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "everyday-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 ms, sys: 35.5 ms, total: 255 ms\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = pipelines.build_bowbae_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "further-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.7 ms, sys: 12.3 ms, total: 91 ms\n",
      "Wall time: 512 ms\n",
      "Processed (and counted) 25647 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, text_no_emojis: string, finished_tokenized: array<string>, finished_emojis: array<string>, finished_unigrams: array<string>, finished_naive_ngrams: array<string>, finished_pos_tags: array<string>, finished_ngrams: array<string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compare inference time with and without using light pipeline.\n",
    "# Anecdotally, we get a 10-20% speedup in wall time.\n",
    "# %time processed_texts = pipeline_model.transform(texts)\n",
    "%time processed_texts = light_model.transform(texts)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")\n",
    "processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daily-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                       finished_naive_ngrams|                                             finished_ngrams|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                            [money_sending, sending_message]|                                                          []|\n",
      "|[math_professor, professor_scott, scott_steiner, steiner_...|       [math professor scott steiner, disaster for gamestop]|\n",
      "|[exit_system, ceo_nasdaq, nasdaq_pushed, pushed_halt, hal...|[enough sentiment, long on gme, clear that this is a rigg...|\n",
      "|[new_sec, sec_filing, filing_gme, someone_less, less_reta...|                                 [new sec, gme! can someone]|\n",
      "|[distract_gme, gme_thought, thought_amc, amc_brothers, br...|                                         [distract from gme]|\n",
      "|                                                          []|                                                          []|\n",
      "|[short_stock, stock_expiration, expiration_date, hedgefun...|[short stock, drive the price, short squeeze, next week, ...|\n",
      "|[life_fair, mother_always, always_told, told_complain, co...|[fair. my mother, arbitrary treatment, first authority, f...|\n",
      "|[currentlyholding_amc, amc_nok, nok_retarded, retarded_th...|                                  [move it all to gme today]|\n",
      "|   [nothing_say, say_bruh, bruh_speechless, speechless_moon]|                                    [speechless to the moon]|\n",
      "|[need_keep, keep_movement, movement_going, going_make, ma...|[keep this movement, hard by this pandemic, able to watch...|\n",
      "|              [gme_premarket, premarket_musk, musk_approved]|                                        [gme premarket musk]|\n",
      "|[done_gme, gme_$ag, $ag_$slv, $slv_gentlemans, gentlemans...|[short squeeze, front page, excellent explanation, hard? ...|\n",
      "|[$gme_price, price_nothing, nothing_fundamentals, fundame...|[general today, wealthy yada, rampant on wall, social fab...|\n",
      "|                                              [love_retards]|                                                          []|\n",
      "|                                                  [420_meme]|                                                 [meme. gme]|\n",
      "|                           [mass_relays, relays_&, &_beyond]|                                                          []|\n",
      "|                                      [come_back, turn_tide]|                                          [turn of the tide]|\n",
      "|      [9_words, words_brought, brought_fuckers, fuckers_sec]|                                                          []|\n",
      "|[daily_discussion, discussion_thread, thread_january, jan...|[daily discussion, daily trading, minimum. navigate, dail...|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_naive_ngrams\", \"finished_ngrams\"]).show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thick-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "|                                             finished_ngrams|                                     finished_emojis|\n",
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "|                                                          []|                                        [ğŸš€, ğŸ’, ğŸ™Œ]|\n",
      "|       [math professor scott steiner, disaster for gamestop]|                                                  []|\n",
      "|[enough sentiment, long on gme, clear that this is a rigg...|                                                  []|\n",
      "|                                 [new sec, gme! can someone]|                                                  []|\n",
      "|                                         [distract from gme]|                                                  []|\n",
      "|                                                          []|                                                  []|\n",
      "|[short stock, drive the price, short squeeze, next week, ...|                                                  []|\n",
      "|[fair. my mother, arbitrary treatment, first authority, f...|                        [ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|                                  [move it all to gme today]|                                                  []|\n",
      "|                                    [speechless to the moon]|                        [ğŸš€, ğŸš€, ğŸš€, ğŸ’, ğŸ’, ğŸ‘‹, ğŸ‘‹]|\n",
      "|[keep this movement, hard by this pandemic, able to watch...|                                                  []|\n",
      "|                                        [gme premarket musk]|                                        [ğŸ, ğŸ®, ğŸ’]|\n",
      "|[short squeeze, front page, excellent explanation, hard? ...|            [ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|[general today, wealthy yada, rampant on wall, social fab...|[ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|                                                          []|                                                  []|\n",
      "|                                                 [meme. gme]|                                        [ğŸš€, ğŸš€, ğŸš€]|\n",
      "|                                                          []|                                                  []|\n",
      "|                                          [turn of the tide]|                                                  []|\n",
      "|                                                          []|                                                  []|\n",
      "|[daily discussion, daily trading, minimum. navigate, dail...|                                                  []|\n",
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_ngrams\", \"finished_emojis\"]).show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-scanning",
   "metadata": {},
   "source": [
    "## For fun: Topic Modelling using Latent Dirichlet Allocation on Emojis Only\n",
    "(Will select number of topics later using some clustering validation strategy)\n",
    "\n",
    "(To do: strip unicode control characters or track emoji stringss instead of constituent characters).\n",
    "\n",
    "Will we be able to match these with topics described in more reasonable ways? I doubt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_emojis')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regional-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worthy-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n",
      "|topic|                             topic_words|\n",
      "+-----+----------------------------------------+\n",
      "|    0| [ğŸ’, ğŸ™Œ, ğŸ¦, ï¸, ğŸ‘, ğŸŒ, ğŸš€, ğŸŒ, ğŸŒ•, ğŸ»]|\n",
      "|    1| [ğŸ¤², ğŸ’, â€, ğŸ“ˆ, ğŸš¨, ğŸ¤¬, ğŸ¤, ğŸ‘Š, ğŸ¥², ğŸ’µ]|\n",
      "|    2|[ğŸ’, ğŸš€, ğŸ¤š, ğŸ™Œ, ğŸ’°, ğŸŒš, ğŸ§», ğŸ‘‹, ğŸ˜ª, ğŸ˜Š]|\n",
      "|    3|  [ğŸ˜‚, ğŸ¦€, ğŸ‘, ğŸ¤·, ï¸, ğŸ™, ğŸ§, ğŸ‘‡, â€, ğŸ“¢]|\n",
      "|    4|[ğŸš€, ğŸ’, ğŸŒ™, ğŸŒ‘, ğŸ¦, ğŸ˜¡, ğŸ¥œ, ğŸ¤², ğŸ™Œ, ğŸ‘]|\n",
      "+-----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

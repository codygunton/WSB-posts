{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tough-commodity",
   "metadata": {},
   "source": [
    "# Bag of Words, Bag of Emojis and n-Grams pipeline\n",
    "A work in progress, this pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. The token information is preserved with the special unicode character ‚ìî (a circled-e; U+24d4). This approach has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used as 'decorators'. It is partially a workaround to handle the fact that there is (apparently) no good solution for normalizing long strings of emojis (native to Spark NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quick-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "sys.path.append('..')\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 353 ¬µs, total: 12.8 ms\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "# df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))\n",
    "\n",
    "emojis_regex = \"[\"+\"\".join(pipelines.emoji_ranges)+\"]\"\n",
    "\n",
    "texts = (\n",
    "    df.withColumn(\"text_no_emojis\",\n",
    "                  F.regexp_replace(df[\"text\"],\n",
    "                                   emojis_regex, \"‚í∫\"))\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                  F.regexp_replace(\"text_no_emojis\", \"[‚Äú‚Äù]\", \"\\\"\"))\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                F.regexp_replace(\"text_no_emojis\", \"[‚Äò‚Äô]\", \"\\'\"))\n",
    "    # to keep positions of emojis (not necessary, currently)\n",
    "    .select([\"text\", \"text_no_emojis\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charming-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 21.1 ms, total: 291 ms\n",
      "Wall time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = pipelines.build_bowbae_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sorted-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81 ms, sys: 25.1 ms, total: 106 ms\n",
      "Wall time: 637 ms\n",
      "Processed (and counted) 25647 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, text_no_emojis: string, finished_tokenized: array<string>, finished_emojis: array<string>, finished_unigrams: array<string>, finished_naive_ngrams: array<string>, finished_pos_tags: array<string>, finished_ngrams: array<string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compare inference time with and without using light pipeline.\n",
    "# Anecdotally, we get a 10-20% speedup in wall time.\n",
    "# %time processed_texts = pipeline_model.transform(texts)\n",
    "%time processed_texts = light_model.transform(texts)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")\n",
    "processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                                       finished_naive_ngrams|                                             finished_ngrams|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|                            [money_sending, sending_message]|                                                          []|\n",
      "|[math_professor, professor_scott, scott_steiner, steiner_...|       [math professor scott steiner, disaster for gamestop]|\n",
      "|[exit_system, ceo_nasdaq, nasdaq_pushed, pushed_halt, hal...|[enough sentiment, long on gme, clear that this is a rigg...|\n",
      "|[new_sec, sec_filing, filing_gme, someone_less, less_reta...|                                 [new sec, gme! can someone]|\n",
      "|[distract_gme, gme_thought, thought_amc, amc_brothers, br...|                                         [distract from gme]|\n",
      "|                                                          []|                                                          []|\n",
      "|[short_stock, stock_expiration, expiration_date, hedgefun...|[short stock, drive the price, short squeeze, next week, ...|\n",
      "|[life_fair, mother_always, always_told, told_complain, co...|[fair. my mother, arbitrary treatment, first authority, f...|\n",
      "|[currentlyholding_amc, amc_nok, nok_retarded, retarded_th...|                                  [move it all to gme today]|\n",
      "|   [nothing_say, say_bruh, bruh_speechless, speechless_moon]|                                    [speechless to the moon]|\n",
      "|[need_keep, keep_movement, movement_going, going_make, ma...|[keep this movement, hard by this pandemic, able to watch...|\n",
      "|              [gme_premarket, premarket_musk, musk_approved]|                                        [gme premarket musk]|\n",
      "|[done_gme, gme_$ag, $ag_$slv, $slv_gentlemans, gentlemans...|[short squeeze, front page, excellent explanation, hard? ...|\n",
      "|[$gme_price, price_nothing, nothing_fundamentals, fundame...|[general today, wealthy yada, rampant on wall, social fab...|\n",
      "|                                              [love_retards]|                                                          []|\n",
      "|                                                  [420_meme]|                                                 [meme. gme]|\n",
      "|                           [mass_relays, relays_&, &_beyond]|                                                          []|\n",
      "|                                      [come_back, turn_tide]|                                          [turn of the tide]|\n",
      "|      [9_words, words_brought, brought_fuckers, fuckers_sec]|                                                          []|\n",
      "|[daily_discussion, discussion_thread, thread_january, jan...|[daily discussion, daily trading, minimum. navigate, dail...|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_naive_ngrams\", \"finished_ngrams\"]).show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "typical-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "|                                             finished_ngrams|                                     finished_emojis|\n",
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "|                                                          []|                                        [üöÄ, üíé, üôå]|\n",
      "|       [math professor scott steiner, disaster for gamestop]|                                                  []|\n",
      "|[enough sentiment, long on gme, clear that this is a rigg...|                                                  []|\n",
      "|                                 [new sec, gme! can someone]|                                                  []|\n",
      "|                                         [distract from gme]|                                                  []|\n",
      "|                                                          []|                                                  []|\n",
      "|[short stock, drive the price, short squeeze, next week, ...|                                                  []|\n",
      "|[fair. my mother, arbitrary treatment, first authority, f...|                        [üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ]|\n",
      "|                                  [move it all to gme today]|                                                  []|\n",
      "|                                    [speechless to the moon]|                        [üöÄ, üöÄ, üöÄ, üíé, üíé, üëã, üëã]|\n",
      "|[keep this movement, hard by this pandemic, able to watch...|                                                  []|\n",
      "|                                        [gme premarket musk]|                                        [üçÅ, üéÆ, üíé]|\n",
      "|[short squeeze, front page, excellent explanation, hard? ...|            [üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ]|\n",
      "|[general today, wealthy yada, rampant on wall, social fab...|[üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ, üöÄ]|\n",
      "|                                                          []|                                                  []|\n",
      "|                                                 [meme. gme]|                                        [üöÄ, üöÄ, üöÄ]|\n",
      "|                                                          []|                                                  []|\n",
      "|                                          [turn of the tide]|                                                  []|\n",
      "|                                                          []|                                                  []|\n",
      "|[daily discussion, daily trading, minimum. navigate, dail...|                                                  []|\n",
      "+------------------------------------------------------------+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.select([\"finished_ngrams\", \"finished_emojis\"]).show(truncate=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "successful-maryland",
   "metadata": {},
   "source": [
    "# Topic modeling piplines using Latent Dirichlet Allocation on emojis and on n-grams\n",
    "This pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. We use neural part-of-speech tagging to generate to generate meaningful and relevant n-grams, then do additional normalization for dimensionality reduction. Our approach to handilng emojis has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used more decoratoratively.\n",
    "\n",
    "In our pipeline_development notebook, we tested the performance of our pipeline against a spaCy pipeline and saw a very substantial improvement. TODO: writes specifics here, but a much simpler preprocessing pipeline in spaCy took 6 mins and ours is maybe like 30s (on my laptop)?\n",
    "\n",
    "What's more, using Spark NLP's LightPipeline class, we get a 10-20% speedup in inference.\n",
    "\n",
    "References: The O'Reilly Spark NLP book, page 76 and https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "\n",
    "TODO: \n",
    "- come and set topics after clustering with some appropriate validation technique.\n",
    "- refine emoji matcher regex to allow for certain multi-char strings (but no repetitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from sparknlp.base import LightPipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = f\"{os.environ['JAVA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# to allow importing from parent directory of notebooks folder\n",
    "sys.path.append('..')\n",
    "\n",
    "DATA_PATH = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mature-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.81 ms, sys: 2.98 ms, total: 7.79 ms\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(DATA_PATH,\n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.2, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    "      .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-cheat",
   "metadata": {},
   "source": [
    "## Quick illustration of text processing with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjacent-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"Shouldn't sell ğŸ’ ğŸ™Œ should not sell\",\n",
    "    \"I paid a steep $5ğŸš€ğŸš€ğŸš€\",\n",
    "    \"What's-his-name wasn't selling.\",\n",
    "    \"Don't sell GME, I say. I don't sell.\",\n",
    "    \"He's a seller. I do not sell!\",\n",
    "    \"I'm gonna sell? Should sell!\",\n",
    "    \"I don't see why anybody should ever sell.\",\n",
    "    \"They're there. They've been there.\",\n",
    "    \"Trading, it's good trading. ğŸ‘±â€â™€ï¸\",\n",
    "    \"'It's' was its own problem, wasn't it?\",\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list, \n",
    "                                            \"text_no_emojis\": text_list}))\n",
    "\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(eg_df)\n",
    "processed_egs = pipeline_model.transform(eg_df)\n",
    "processed_egs = pipelines.lda_preproc_finisher(processed_egs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "focused-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>finished_ngrams</th>\n",
       "      <th>finished_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm gonna sell? Should sell!</td>\n",
       "      <td>[should_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trading, it's good trading. ğŸ‘±â€â™€ï¸</td>\n",
       "      <td>[good_trading]</td>\n",
       "      <td>[ğŸ‘±â€â™€ï¸]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I paid a steep $5ğŸš€ğŸš€ğŸš€</td>\n",
       "      <td>[steep_$5]</td>\n",
       "      <td>[ğŸš€, ğŸš€, ğŸš€]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'It's' was its own problem, wasn't it?</td>\n",
       "      <td>[own_problem]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shouldn't sell ğŸ’ ğŸ™Œ should not sell</td>\n",
       "      <td>[should_not_sell, should_not_sell]</td>\n",
       "      <td>[ğŸ’, ğŸ™Œ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Don't sell GME, I say. I don't sell.</td>\n",
       "      <td>[do_not_sell_gme]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I don't see why anybody should ever sell.</td>\n",
       "      <td>[should_ever_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text  \\\n",
       "0               I'm gonna sell? Should sell!   \n",
       "1           Trading, it's good trading. ğŸ‘±â€â™€ï¸   \n",
       "2                       I paid a steep $5ğŸš€ğŸš€ğŸš€   \n",
       "3     'It's' was its own problem, wasn't it?   \n",
       "4         Shouldn't sell ğŸ’ ğŸ™Œ should not sell   \n",
       "5       Don't sell GME, I say. I don't sell.   \n",
       "6  I don't see why anybody should ever sell.   \n",
       "\n",
       "                      finished_ngrams finished_emojis  \n",
       "0                       [should_sell]              []  \n",
       "1                      [good_trading]          [ğŸ‘±â€â™€ï¸]  \n",
       "2                          [steep_$5]       [ğŸš€, ğŸš€, ğŸš€]  \n",
       "3                       [own_problem]              []  \n",
       "4  [should_not_sell, should_not_sell]          [ğŸ’, ğŸ™Œ]  \n",
       "5                   [do_not_sell_gme]              []  \n",
       "6                  [should_ever_sell]              []  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_egs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-bench",
   "metadata": {},
   "source": [
    "## Now fit to WallStreetBets posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "religious-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 14.9 ms, total: 62.8 ms\n",
      "Wall time: 613 ms\n",
      "Processed (and counted) 5182 rows.\n"
     ]
    }
   ],
   "source": [
    "texts = pipelines.preprocess_texts(df)\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)\n",
    "def process_texts():\n",
    "    processed_texts = light_model.transform(texts)\n",
    "    processed_texts = pipelines.lda_preproc_finisher(processed_texts)\n",
    "    return processed_texts\n",
    "%time processed_texts = process_texts()\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aerial-freedom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>finished_ngrams</th>\n",
       "      <th>finished_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exit the system. The CEO of NASDAQ pushed to h...</td>\n",
       "      <td>[will_change, may_have, will_look, should_have...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE. H...</td>\n",
       "      <td>[next_week, may_be, false_expectation, will_se...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Currently Holding AMC and NOK - Is it retarded...</td>\n",
       "      <td>[should_move, gme_today]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We need to stick together and ğŸ’ğŸ– the ever lovi...</td>\n",
       "      <td>[fellow_poors, rise_up, ah_manipulation, will_...</td>\n",
       "      <td>[ğŸ’]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patcher and other media outlets calling this a...</td>\n",
       "      <td>[ponzi_scheme]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>Some loss porn for you degens. $GME to the moo...</td>\n",
       "      <td>[loss_porn]</td>\n",
       "      <td>[ğŸ¦§, ğŸŒ, ğŸ’, ğŸš€, ğŸš€, ğŸŒ•]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>Listen up plebs, Tony hawk just railed a fat l...</td>\n",
       "      <td>[park_lot, chainsmokers_tony_hawk_invest, migh...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>GME Yolo Loss Porn- Day 3, from $23,358k down ...</td>\n",
       "      <td>[financial_advisor, gme_yolo_loss_porn_day, mo...</td>\n",
       "      <td>[ğŸµ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>Hey, go fuck yourselves!</td>\n",
       "      <td>[hey_go_fuck_yourselves]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>Loss Porn. had 200 shares so naturally i bough...</td>\n",
       "      <td>[loss_porn]</td>\n",
       "      <td>[ğŸ’, ğŸ™Œ, ğŸ¦]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4011 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Exit the system. The CEO of NASDAQ pushed to h...   \n",
       "1     SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE. H...   \n",
       "2     Currently Holding AMC and NOK - Is it retarded...   \n",
       "3     We need to stick together and ğŸ’ğŸ– the ever lovi...   \n",
       "4     Patcher and other media outlets calling this a...   \n",
       "...                                                 ...   \n",
       "4006  Some loss porn for you degens. $GME to the moo...   \n",
       "4007  Listen up plebs, Tony hawk just railed a fat l...   \n",
       "4008  GME Yolo Loss Porn- Day 3, from $23,358k down ...   \n",
       "4009                           Hey, go fuck yourselves!   \n",
       "4010  Loss Porn. had 200 shares so naturally i bough...   \n",
       "\n",
       "                                        finished_ngrams     finished_emojis  \n",
       "0     [will_change, may_have, will_look, should_have...                  []  \n",
       "1     [next_week, may_be, false_expectation, will_se...                  []  \n",
       "2                              [should_move, gme_today]                  []  \n",
       "3     [fellow_poors, rise_up, ah_manipulation, will_...                 [ğŸ’]  \n",
       "4                                        [ponzi_scheme]                  []  \n",
       "...                                                 ...                 ...  \n",
       "4006                                        [loss_porn]  [ğŸ¦§, ğŸŒ, ğŸ’, ğŸš€, ğŸš€, ğŸŒ•]  \n",
       "4007  [park_lot, chainsmokers_tony_hawk_invest, migh...                  []  \n",
       "4008  [financial_advisor, gme_yolo_loss_porn_day, mo...                 [ğŸµ]  \n",
       "4009                           [hey_go_fuck_yourselves]                  []  \n",
       "4010                                        [loss_porn]           [ğŸ’, ğŸ™Œ, ğŸ¦]  \n",
       "\n",
       "[4011 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-substance",
   "metadata": {},
   "source": [
    "## Topic Modeling using meaningful n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-melbourne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (and counted) 5182 rows.\n",
      "CPU times: user 45.3 ms, sys: 12.1 ms, total: 57.4 ms\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_ngrams')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separate-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                             topic_words|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0|[call_option, can_be, share_price, imply_volatility, standard_definition, price_risk, interest_rate, good_fight, will...|\n",
      "|    1|[last_week, can_not_buy, melvin_capital, webull_account, big_asshole, massive_conflict, short_position, make_money, n...|\n",
      "|    2|[short_squeeze, gamma_squeeze, financial_advice, would_be, lose_money, fuck_robinhood, can_do, can_hold, development_...|\n",
      "|    3|[will_be, loss_porn, robin_hood, would_be, stock_market, federal_reserve, would_have, should_not_be, buy_gme, financi...|\n",
      "|    4|[wall_street, class_action, market_manipulation, will_see, will_not_be, will_be, free_market, financial_advisor, inve...|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-greeting",
   "metadata": {},
   "source": [
    "## For fun: Topic Modelling using Latent Dirichlet Allocation on Emojis Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conceptual-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (and counted) 5182 rows.\n",
      "CPU times: user 44.4 ms, sys: 10.2 ms, total: 54.5 ms\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_emojis')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automatic-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n",
      "|topic|                             topic_words|\n",
      "+-----+----------------------------------------+\n",
      "|    0|[ğŸš€, ğŸŒ±, ğŸ’, ğŸ¤š, ğŸ‘¿, ğŸ‘¨, ğŸŒ•, ğŸ¤, ğŸ™Œ, ğŸ“„]|\n",
      "|    1|[ğŸš€, ğŸ’, ğŸ™Œ, ğŸ¦, ğŸ‘, ğŸ”¥, ğŸ¤š, ğŸ’ª, ğŸ§¤, ğŸ’°]|\n",
      "|    2|[ğŸ¤², ğŸŒˆ, ğŸ», ğŸ’, ğŸŒ‘, ğŸ‘, ğŸ¥², ğŸ¤”, ğŸ’¥, ğŸ™Œ]|\n",
      "|    3|[ğŸŒ™, ğŸŒš, ğŸ’µ, ğŸ§ƒ, ğŸ, ğŸ‘, ğŸ¿, ğŸ®, ğŸš€, ğŸ˜“]|\n",
      "|    4|[ğŸš¨, ğŸ“ˆ, ğŸ˜ , ğŸ˜¡, ğŸ¤‘, ğŸ¤¬, ğŸ˜‰, ğŸ˜Œ, ğŸŒ, ğŸª]|\n",
      "+-----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-blues",
   "metadata": {},
   "source": [
    "For fun: can you match the topics here with the topics extcated using the emojis? \n",
    "Note: nothing in the method guarantees that this will be possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

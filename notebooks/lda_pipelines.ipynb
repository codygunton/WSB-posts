{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endless-disney",
   "metadata": {},
   "source": [
    "# Topic modeling piplines using Latent Dirichlet Allocation on emojis and on n-grams\n",
    "This pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. We use neural part-of-speech tagging to generate to generate meaningful and relevant n-grams, then do additional normalization for dimensionality reduction. Our approach to handilng emojis has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used more decoratoratively.\n",
    "\n",
    "In our pipeline_development notebook, we tested the performance of our pipeline against a spaCy pipeline and saw a very substantial improvement. TODO: writes specifics here, but a much simpler preprocessing pipeline in spaCy took 6 mins and ours is maybe like 30s (on my laptop)?\n",
    "\n",
    "What's more, using Spark NLP's LightPipeline class, we get a 10-20% speedup in inference.\n",
    "\n",
    "References: The O'Reilly Spark NLP book, page 76 and https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "\n",
    "TODO: \n",
    "- come and set topics after clustering with some appropriate validation technique.\n",
    "- refine emoji matcher regex to allow for certain multi-char strings (but no repetitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "removed-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from sparknlp.base import LightPipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "sys.path.append('..')\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.6 ms, sys: 499 Âµs, total: 8.1 ms\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.2, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    "      .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-hampton",
   "metadata": {},
   "source": [
    "## Quick illustration of text processing with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thorough-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"Shouldn't sell ğŸ’ ğŸ™Œ should not sell\",\n",
    "    \"I paid a steep $5ğŸš€ğŸš€ğŸš€\",\n",
    "    \"What's-his-name wasn't selling.\",\n",
    "    \"Don't sell GME, I say. I don't sell.\",\n",
    "    \"He's a seller. I do not sell!\",\n",
    "    \"I'm gonna sell? Should sell!\",\n",
    "    \"I don't see why anybody should ever sell.\",\n",
    "    \"They're there. They've been there.\",\n",
    "    \"Trading, it's good trading\",\n",
    "    \"'It's' was its own problem, wasn't it?\",\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list, \n",
    "                                            \"text_no_emojis\": text_list}))\n",
    "\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(eg_df)\n",
    "processed_egs = pipeline_model.transform(eg_df)\n",
    "processed_egs = pipelines.lda_preproc_finisher(processed_egs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "herbal-brook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>finished_ngrams</th>\n",
       "      <th>finished_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm gonna sell? Should sell!</td>\n",
       "      <td>[should_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I paid a steep $5ğŸš€ğŸš€ğŸš€</td>\n",
       "      <td>[steep_$5]</td>\n",
       "      <td>[ğŸš€, ğŸš€, ğŸš€]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'It's' was its own problem, wasn't it?</td>\n",
       "      <td>[own_problem]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shouldn't sell ğŸ’ ğŸ™Œ should not sell</td>\n",
       "      <td>[should_not_sell, should_not_sell]</td>\n",
       "      <td>[ğŸ’, ğŸ™Œ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don't sell GME, I say. I don't sell.</td>\n",
       "      <td>[do_not_sell_gme]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't see why anybody should ever sell.</td>\n",
       "      <td>[should_ever_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trading, it's good trading</td>\n",
       "      <td>[good_trade]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text  \\\n",
       "0               I'm gonna sell? Should sell!   \n",
       "1                       I paid a steep $5ğŸš€ğŸš€ğŸš€   \n",
       "2     'It's' was its own problem, wasn't it?   \n",
       "3         Shouldn't sell ğŸ’ ğŸ™Œ should not sell   \n",
       "4       Don't sell GME, I say. I don't sell.   \n",
       "5  I don't see why anybody should ever sell.   \n",
       "6                 Trading, it's good trading   \n",
       "\n",
       "                      finished_ngrams finished_emojis  \n",
       "0                       [should_sell]              []  \n",
       "1                          [steep_$5]       [ğŸš€, ğŸš€, ğŸš€]  \n",
       "2                       [own_problem]              []  \n",
       "3  [should_not_sell, should_not_sell]          [ğŸ’, ğŸ™Œ]  \n",
       "4                   [do_not_sell_gme]              []  \n",
       "5                  [should_ever_sell]              []  \n",
       "6                        [good_trade]              []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_egs.show(truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-hayes",
   "metadata": {},
   "source": [
    "## Now fit to WallStreetBets posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verified-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.4 ms, sys: 10.3 ms, total: 65.6 ms\n",
      "Wall time: 546 ms\n",
      "Processed (and counted) 5182 rows.\n"
     ]
    }
   ],
   "source": [
    "texts = pipelines.preprocess_texts(df)\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)\n",
    "def process_texts():\n",
    "    processed_texts = light_model.transform(texts)\n",
    "    processed_texts = pipelines.lda_preproc_finisher(processed_texts)\n",
    "    return processed_texts\n",
    "%time processed_texts = process_texts()\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southern-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                         finished_ngrams|                         finished_emojis|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|Exit the system. The CEO of NASDAQ pu...|[will_change, may_have, will_look, sh...|                                      []|\n",
      "|SHORT STOCK DOESN'T HAVE AN EXPIRATIO...|[next_week, may_be, false_expectation...|                                      []|\n",
      "|Currently Holding AMC and NOK - Is it...|                [should_move, gme_today]|                                      []|\n",
      "|We need to stick together and ğŸ’ğŸ– th...|[fellow_poors, rise_up, ah_manipulati...|                                    [ğŸ’]|\n",
      "|Patcher and other media outlets calli...|                          [ponzi_scheme]|                                      []|\n",
      "|I'm so proud of how far this subreddi...|[pharmaceutical_company, past_couple,...|                                      []|\n",
      "|Really? I canâ€™t even buy GME or AMC f...|    [can_not_even_buy, not_even_buy_gme]|                                    [ğŸ˜¤]|\n",
      "|Y'all broke it. How do we fix it? Any...|                                [fix_it]|                                      []|\n",
      "|Are we ready to attack the Citadel !!...|                               [citadel]|                                      []|\n",
      "|My brokerage wants to force close my ...|[gme_calls, exception_right_now, big_...|                                      []|\n",
      "|Iacta alea est. I was the lucky one i...|[send_money, can_look, entire_debacle...|                                      []|\n",
      "|               wsb_churchill_speech.mp69|              [wsb_churchill_speechmp69]|                                      []|\n",
      "|Why is GME blank I have 2 screen shot...|                             [cali_time]|                                      []|\n",
      "|The real price for GME is infinity, b...|[call_option_strike, financial_market...|                                      []|\n",
      "|Robinhood has literally shut down the...|                           [certain_gme]|                                      []|\n",
      "|Is it too late to get on the GME spac...|      [m_n, market_opens, gme_spaceship]|[ğŸš€, ğŸš€, ğŸš€, ï¸, ğŸŒš, ğŸŒš, ğŸŒš, ğŸ’, ğŸ™Œ, ?...|\n",
      "|Help an NRI (Indian) get on board the...|[gme_stock, saudi_arabia, mooon_edit_...|                        [ğŸš€, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|Before you sleep tonight. Just think ...|[good_autism, simple_terms, tik_toks_...|                                      []|\n",
      "|Iâ€™ve seen a lot of people talking abo...|[will_make, will_be, old_man, retard_...|                [ğŸ’, ğŸ’, ğŸ’, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|Guys Micheal Burry, Alexandria Ocasio...|[can_see, limit_order, white_house, m...|                                      []|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_texts.show(truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-honduras",
   "metadata": {},
   "source": [
    "## Topic Modeling using meaningful n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "artificial-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (and counted) 5182 rows.\n",
      "CPU times: user 45.8 ms, sys: 9.39 ms, total: 55.2 ms\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_ngrams')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "returning-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                             topic_words|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0|[can_not_buy, td_ameritrade, will_be, fuck_robinhood, short_sell, right_now, can_do, robinhood_customer, will_have, l...|\n",
      "|    1|       [buy_gme, next_week, hold_hold, can_get, investor_day, last_year, should_be, wall_street, will_not_let, buy_$gme]|\n",
      "|    2|[process_img, gme_today, melvin_capital, short_interest, bb_nok, will_win, last_week, buy_more, will_not_allow, bloom...|\n",
      "|    3|[private_boomer, trade_republic, market_cap, first_time, will_be, huge_profits, buy_buy, positions_hf, ark_invest, cr...|\n",
      "|    4|[would_be, short_squeeze, wall_street, gamma_squeeze, will_buy, market_manipulation, can_see, financial_advice, broke...|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-technique",
   "metadata": {},
   "source": [
    "## For fun: Topic Modelling using Latent Dirichlet Allocation on Emojis Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (and counted) 5182 rows.\n",
      "CPU times: user 52.6 ms, sys: 3.31 ms, total: 55.9 ms\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_emojis')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "frozen-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n",
      "|topic|                             topic_words|\n",
      "+-----+----------------------------------------+\n",
      "|    0| [ğŸ‘, ğŸŒ•, ğŸ˜‚, ğŸ’, ğŸ¦, ğŸŒˆ, ğŸ’«, ğŸš€, ğŸ˜, â€]|\n",
      "|    1|[ğŸ˜ˆ, ğŸ¥¶, ğŸš€, ğŸš¨, ğŸŒš, ğŸ”¥, ğŸ˜¢, ğŸ¿, ğŸŒ•, ğŸ’]|\n",
      "|    2|[ğŸš€, ğŸŒš, ğŸ”¥, ğŸ¥œ, ğŸ¥², ğŸ’, ğŸ˜¤, ğŸ‘‹, ğŸ™Œ, ğŸ‘¨]|\n",
      "|    3| [ğŸ’, ğŸ™Œ, ğŸ¤², ğŸ¦, ğŸš€, ï¸, ğŸ¤š, ğŸ¥¥, ğŸ†, ğŸ‘]|\n",
      "|    4|[ğŸŒ™, ğŸ‘, ğŸ¥º, ğŸš€, ğŸŒ—, ğŸŒ, ğŸ˜¡, ğŸ˜ , ğŸª, ğŸ¤¬]|\n",
      "+-----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-standard",
   "metadata": {},
   "source": [
    "For fun: can you match the topics here with the topics extcated using the emojis? \n",
    "Note: nothing in the method guarantees that this will be possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

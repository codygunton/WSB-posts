{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extraordinary-tackle",
   "metadata": {},
   "source": [
    "# Topic modeling piplines using Latent Dirichlet Allocation on emojis and on n-grams\n",
    "This pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. We use neural part-of-speech tagging to generate to generate meaningful and relevant n-grams, then do additional normalization for dimensionality reduction. Our approach to handilng emojis has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used more decoratoratively.\n",
    "\n",
    "In our pipeline_development notebook, we tested the performance of our pipeline against a spaCy pipeline and saw a very substantial improvement. TODO: writes specifics here, but a much simpler preprocessing pipeline in spaCy took 6 mins and ours is maybe like 30s (on my laptop)?\n",
    "\n",
    "What's more, using Spark NLP's LightPipeline class, we get a 10-20% speedup in inference.\n",
    "\n",
    "References: The O'Reilly Spark NLP book, page 76 and https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "\n",
    "TODO: \n",
    "- come and set topics after clustering with some appropriate validation technique.\n",
    "- need to handle \"process_img\" in normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smaller-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from sparknlp.base import LightPipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "sys.path.append('..')\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smaller-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.94 ms, sys: 468 µs, total: 7.4 ms\n",
      "Wall time: 3.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.1, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    "      .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-depression",
   "metadata": {},
   "source": [
    "## Quick illustration of text processing with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excellent-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"Shouldn't sell 💎 🙌 should not sell\",\n",
    "    \"I paid a steep $5🚀🚀🚀\",\n",
    "    \"What's-his-name wasn't selling.\",\n",
    "    \"Don't sell GME, I say. I don't sell.\",\n",
    "    \"He's a seller. I do not sell!\",\n",
    "    \"I'm gonna sell? Should sell!\",\n",
    "    \"I don't see why anybody should ever sell.\",\n",
    "    \"They're there. They've been there.\",\n",
    "    \"Trading, it's good trading\",\n",
    "    \"'It's' was its own problem, wasn't it?\",\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list, \n",
    "                                            \"text_no_emojis\": text_list}))\n",
    "\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(eg_df)\n",
    "processed_egs = pipeline_model.transform(eg_df)\n",
    "processed_egs = pipelines.lda_preproc_finisher(processed_egs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "agreed-horse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>finished_ngrams</th>\n",
       "      <th>finished_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm gonna sell? Should sell!</td>\n",
       "      <td>[should_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I paid a steep $5🚀🚀🚀</td>\n",
       "      <td>[steep_$5]</td>\n",
       "      <td>[🚀, 🚀, 🚀]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'It's' was its own problem, wasn't it?</td>\n",
       "      <td>[own_problem]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shouldn't sell 💎 🙌 should not sell</td>\n",
       "      <td>[should_not_sell, should_not_sell]</td>\n",
       "      <td>[💎, 🙌]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don't sell GME, I say. I don't sell.</td>\n",
       "      <td>[do_not_sell_gme]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't see why anybody should ever sell.</td>\n",
       "      <td>[should_ever_sell]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trading, it's good trading</td>\n",
       "      <td>[good_trade]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text  \\\n",
       "0               I'm gonna sell? Should sell!   \n",
       "1                       I paid a steep $5🚀🚀🚀   \n",
       "2     'It's' was its own problem, wasn't it?   \n",
       "3         Shouldn't sell 💎 🙌 should not sell   \n",
       "4       Don't sell GME, I say. I don't sell.   \n",
       "5  I don't see why anybody should ever sell.   \n",
       "6                 Trading, it's good trading   \n",
       "\n",
       "                      finished_ngrams finished_emojis  \n",
       "0                       [should_sell]              []  \n",
       "1                          [steep_$5]       [🚀, 🚀, 🚀]  \n",
       "2                       [own_problem]              []  \n",
       "3  [should_not_sell, should_not_sell]          [💎, 🙌]  \n",
       "4                   [do_not_sell_gme]              []  \n",
       "5                  [should_ever_sell]              []  \n",
       "6                        [good_trade]              []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_egs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-relaxation",
   "metadata": {},
   "source": [
    "## Now fit to WallStreetBets posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 ms, sys: 2.52 ms, total: 41.7 ms\n",
      "Wall time: 310 ms\n",
      "CPU times: user 19.2 ms, sys: 3.22 ms, total: 22.4 ms\n",
      "Wall time: 250 ms\n",
      "Processed (and counted) 2624 rows.\n"
     ]
    }
   ],
   "source": [
    "texts = pipelines.preprocess_texts(df)\n",
    "\n",
    "pipeline = pipelines.build_lda_preproc_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)\n",
    "%time processed_texts = light_model.transform(texts)\n",
    "%time processed_texts = pipelines.lda_preproc_finisher(processed_texts)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-stake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>finished_ngrams</th>\n",
       "      <th>finished_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exit the system. The CEO of NASDAQ pushed to h...</td>\n",
       "      <td>[will_change, may_have, will_look, should_have...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Currently Holding AMC and NOK - Is it retarded...</td>\n",
       "      <td>[should_move, gme_today]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y'all broke it. How do we fix it? Any advice?</td>\n",
       "      <td>[fix_it]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are we ready to attack the Citadel !!!!. https...</td>\n",
       "      <td>[citadel]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My brokerage wants to force close my GME calls...</td>\n",
       "      <td>[gme_calls, big_risk, unusual_situation, situa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Glad I can't trade after hours when I've had a...</td>\n",
       "      <td>[can_not_trade]</td>\n",
       "      <td>[😮]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Just to clear up the SEC probe misunderstandin...</td>\n",
       "      <td>[world_series, sec_probe_misunderstandings, kn...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>I am your typical retard from Germany. Hold st...</td>\n",
       "      <td>[typical_retard, diamond_hands]</td>\n",
       "      <td>[🚀, 🚀]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>Holding strong here in th UK</td>\n",
       "      <td>[th_uk]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>Loss Porn. had 200 shares so naturally i bough...</td>\n",
       "      <td>[loss_porn]</td>\n",
       "      <td>[💎, 🙌, 🦍]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2030 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Exit the system. The CEO of NASDAQ pushed to h...   \n",
       "1     Currently Holding AMC and NOK - Is it retarded...   \n",
       "2         Y'all broke it. How do we fix it? Any advice?   \n",
       "3     Are we ready to attack the Citadel !!!!. https...   \n",
       "4     My brokerage wants to force close my GME calls...   \n",
       "...                                                 ...   \n",
       "2025  Glad I can't trade after hours when I've had a...   \n",
       "2026  Just to clear up the SEC probe misunderstandin...   \n",
       "2027  I am your typical retard from Germany. Hold st...   \n",
       "2028                       Holding strong here in th UK   \n",
       "2029  Loss Porn. had 200 shares so naturally i bough...   \n",
       "\n",
       "                                        finished_ngrams finished_emojis  \n",
       "0     [will_change, may_have, will_look, should_have...              []  \n",
       "1                              [should_move, gme_today]              []  \n",
       "2                                              [fix_it]              []  \n",
       "3                                             [citadel]              []  \n",
       "4     [gme_calls, big_risk, unusual_situation, situa...              []  \n",
       "...                                                 ...             ...  \n",
       "2025                                    [can_not_trade]             [😮]  \n",
       "2026  [world_series, sec_probe_misunderstandings, kn...              []  \n",
       "2027                    [typical_retard, diamond_hands]          [🚀, 🚀]  \n",
       "2028                                            [th_uk]              []  \n",
       "2029                                        [loss_porn]       [💎, 🙌, 🦍]  \n",
       "\n",
       "[2030 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-rogers",
   "metadata": {},
   "source": [
    "## Topic Modeling using meaningful n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "critical-piano",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 ms, sys: 2.21 ms, total: 41.3 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_ngrams')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "taken-brunei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                                              topic_words|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0|           [should_be, short_squeeze, stock_market, can_buy, end_game, financial_advice, may_have, short_market, can_afford, moass_crash]|\n",
      "|    1|[market_manipulation, will_be, free_market, next_week, retail_investors, robin_hood, fuck_robinhood, ballot_box, might_be, single_person]|\n",
      "|    2|              [wall_street, will_not_be, halt_trade, free_money, will_not_sell, short_sell, may_be, would_like, will_see, short_interest]|\n",
      "|    3|   [last_week, short_interest, buy_gme, quick_setup, x200b_process_img, bloomberg_terminal, can_say, can_get, diamond_hands, process_img]|\n",
      "|    4|  [global_tech, exit_strategy, verify_passport, right_now, will_have, quikpass_check, will_be, federal_reserve, apple_store, stay_strong]|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=160))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-animal",
   "metadata": {},
   "source": [
    "## For fun: Topic Modelling using Latent Dirichlet Allocation on Emojis Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fixed-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 ms, sys: 1.95 ms, total: 49.2 ms\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_emojis')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efficient-benjamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n",
      "|topic|                             topic_words|\n",
      "+-----+----------------------------------------+\n",
      "|    0|[🪐, 🤝, 🥥, 🎲, 😢, 😭, 💎, 📈, 🌴, 🙌]|\n",
      "|    1|[💎, 🤚, 🤑, 🙌, 🚀, 💍, 🤘, 🦧, 👉, 🌝]|\n",
      "|    2|[👎, 🌕, 🌙, 😂, 🌚, 🐸, 😴, 🚀, 👍, 🌗]|\n",
      "|    3|[🚀, 💎, 🙌, 🙏, 👐, 🚨, 🦍, 🤡, 🌈, 🐻]|\n",
      "|    4| [📈, ️, 🤲, 👾, 💈, 🤝, 🎲, 🚀, 🪐, 💎]|\n",
      "+-----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))\n",
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-integration",
   "metadata": {},
   "source": [
    "For fun: can you match the topics here with the topics extcated using the emojis? \n",
    "Note: nothing in the method guarantees that this will be possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

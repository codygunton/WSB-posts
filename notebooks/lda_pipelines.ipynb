{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standing-peter",
   "metadata": {},
   "source": [
    "# Topic modeling piplines using Latent Dirichlet Allocation on emojis and on n-grams\n",
    "A work in progress, th pipeline is designed to preserve symbols, words and phrases of interest (e.g., special vocabulary) in the context of WallStreetBets posts, while splitting off a separate bag of emojis. The token information is preserved with the special unicode character ‚ìî (a circled-e; U+24d4). This approach has advantages and disadvantages. It is probably a useful and efficient way of preserving much of the emoji sentiment (and even the evolution of sentiment throughough a post), and especially so when the emojis are used as 'decorators'. It is partially a workaround to handle the fact that there is (apparently) no good solution for normalizing long strings of emojis (native to Spark NLP).\n",
    "\n",
    "\n",
    "\n",
    "References: The O'Reilly Spark NLP book, page 76 and https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "\n",
    "\n",
    "To do:\n",
    "- Finalize this version of the pipeline and write summary. Do general cleaning up (e.g., of imports)\n",
    "- improve interaction of lemmatization and final n-gram. Re-lemmatize?\n",
    "- user validation techniques for clustering then come back and re-set number of topics.\n",
    "- some punctuation still makes it through to the n-grams because POS model scans partially-noramlized document\n",
    "- ? add custom lemmatization rules as in cant |-> can't (& give up the less common meaning of 'cant')?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increasing-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "\n",
    "spark = sparknlp.start()\n",
    "sys.path.append('..')\n",
    "%aimport pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demonstrated-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 ms, sys: 4.07 ms, total: 12.7 ms\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "\n",
    "df = (df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\"))\n",
    "\n",
    "emojis_regex = \"[\"+\"\".join(pipelines.emoji_ranges)+\"]\"\n",
    "\n",
    "texts = (\n",
    "    # to do: find uniform way of preprocessing usingn pure Spark\n",
    "    #        can only find convoluted ways in Spark NLP.\n",
    "    #        Is using a UDF slower?\n",
    "    df.withColumn(\"text_no_emojis\",\n",
    "                  F.regexp_replace(\"text\",\n",
    "                                   emojis_regex, \" \"))  # replacing with \"\" is bad\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                  F.regexp_replace(\"text_no_emojis\", \"[‚Äú‚Äù]\", \"\\\"\"))\n",
    "    .withColumn(\"text_no_emojis\", \n",
    "                F.regexp_replace(\"text_no_emojis\", \"[‚Äò‚Äô]\", \"\\'\"))\n",
    "    # to keep positions of emojis (not necessary, currently)\n",
    "    .select([\"text\", \"text_no_emojis\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-subsection",
   "metadata": {},
   "source": [
    "Look first at some relevant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distinct-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.8 ms, sys: 11.6 ms, total: 46.3 ms\n",
      "Wall time: 197 ms\n",
      "Columns: {processed_egs.columns)}\n",
      "+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+---------------------------------+\n",
      "|                                              text|                                 finished_unigrams|                            finished_pos_tags|                  finished_ngrams|\n",
      "+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+---------------------------------+\n",
      "|                               I paid $5. Did you?|                             [i, pay, $5, do, you]|                      [NNP, VB, NN, VBP, PRP]|                        [paid $5]|\n",
      "|            'It's' was its own problem, wasn't it?|     [it, have, be, it, own, problem, be, not, it]|     [PRP, VBP, VB, PRP, JJ, NN, VB, RB, PRP]|                    [own problem]|\n",
      "|                   What's-his-name wasn't selling.|          [what, have, he, name, be, not, selling]|             [WP, VBP, PRP, VBP, VB, RB, VBG]|                               []|\n",
      "|              Don't sell GME, I say. I don't sell.|    [do, not, sell, gme, i, say, i, do, not, sell]|[VBP, RB, VB, NN, NNP, VBP, NNP, VBP, RB, VB]|                 [don't sell gme]|\n",
      "|                     He's a seller. I do not sell!|           [he, have, a, seller, i, do, not, sell]|         [PRP, VBP, DT, NN, NNP, VBP, RB, VB]|                               []|\n",
      "|                   Shouldn't sell. Should not sell|            [should, not, sell, should, not, sell]|                     [MD, RB, VB, MD, RB, VB]|[shouldn't sell, should not sell]|\n",
      "|                      I'm gonna sell? Should sell!|                [i, be, gonna, sell, should, sell]|                   [NNP, VB, VBG, VB, MD, VB]|                    [should sell]|\n",
      "|         I don't see why anybody should ever sell.|[i, do, not, see, why, anybody, should, ever, s...|      [NNP, VBP, RB, VB, WRB, NN, MD, RB, VB]|               [should ever sell]|\n",
      "|Some say one musn't hold. Rubbish! One should h...|[some, say, one, mu, not, hold, rubbish, one, s...|    [DT, VBP, CD, NN, RB, VB, NN, CD, MD, VB]|                    [should hold]|\n",
      "|                They're there. They've been there.|                [theyre, there, theyve, be, there]|                         [NN, EX, NN, VB, EX]|                               []|\n",
      "|         Trading, good trading, and good companies|      [trading, good, trading, and, good, company]|                     [NN, JJ, NN, CC, JJ, NN]|     [good trading, good compani]|\n",
      "+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"I paid $5. Did you?\",\n",
    "    \"'It's' was its own problem, wasn't it?\",\n",
    "    \"What's-his-name wasn't selling.\",\n",
    "    \"Don't sell GME, I say. I don't sell.\",\n",
    "    \"He's a seller. I do not sell!\",\n",
    "    \"Shouldn't sell. Should not sell\",\n",
    "    \"I'm gonna sell? Should sell!\",\n",
    "    \"I don't see why anybody should ever sell.\",\n",
    "    \"Some say one musn't hold. Rubbish! One should hold.\",\n",
    "    \"They're there. They've been there.\",\n",
    "    \"Trading, good trading, and good companies\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list, \n",
    "                                            \"text_no_emojis\": text_list}))\n",
    "\n",
    "pipeline = pipelines.build_lda_pipeline()\n",
    "pipeline_model = pipeline.fit(eg_df)\n",
    "light_model = LightPipeline(pipeline_model)\n",
    "# We compare inference time with and without using light pipeline.\n",
    "# Anecdotally, we get a 10-20% speedup in wall time.\n",
    "# %time processed_texts = pipeline_model.transform(texts)\n",
    "%time processed_egs = light_model.transform(eg_df)\n",
    "print(\"Columns: {processed_egs.columns)}\")\n",
    "(processed_egs.select([\"text\", \n",
    "                      \"finished_unigrams\", \n",
    "                      \"finished_pos_tags\", \n",
    "                      \"finished_ngrams\"])\n",
    " .show(truncate=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-developer",
   "metadata": {},
   "source": [
    "Now fit to WSB posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qualified-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 ms, sys: 6.2 ms, total: 45.7 ms\n",
      "Wall time: 188 ms\n",
      "Processed (and counted) 1326 rows.\n",
      "+------------------------------------------------------------+------------------------------------------------------------+---------------+\n",
      "|                                                        text|                                             finished_ngrams|finished_emojis|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+---------------+\n",
      "|Exit the system. The CEO of NASDAQ pushed to halt trading...|[nasdaq push, halt tradi, give investor, disallowing buy,...|             []|\n",
      "|                             420 wasn‚Äôt a meme. GME üöÄ üöÄ üöÄ|                                                          []|   [üöÄ, üöÄ, üöÄ]|\n",
      "|               Y'all broke it. How do we fix it? Any advice?|                                               [y'all broke]|             []|\n",
      "|   They're trying to say this was all done by 'Nazis' now...|                                               [they're try]|             []|\n",
      "|Why is GME blank I have 2 screen shots 2am cali time. Wtf...|                                         [cali time, is gme]|             []|\n",
      "|Robinhood has literally shut down the ability for us to b...|                         [certain gme, gme call, hedge fund]|             []|\n",
      "|                               PLTR üöÄ thank you everyone!!!|                                                          []|           [üöÄ]|\n",
      "|Nasdaq CEO Suggests Halt to Trading to Allow Big Investor...|[big investor, nasdaq ceo suggests halt, reddit user, com...|             []|\n",
      "|GME Wars: Suits Strike Back. WARNING FOR ALL YOU FELLOW A...|[gme wars suits strike, financial advisor, blah blah blah...|             []|\n",
      "|This is personal now. We need their scare tactics to have...|[total opposite effect, must use, will not be, making mon...|             []|\n",
      "|SEC LOSERS WATCH THIS. https://youtu.be/cRNypdYQoWk ELON ...|                                          [sec losers watch]|             []|\n",
      "|With well over a billion trades on $AMC would have expect...|[would have, same momentum, new trade, waiting game, righ...|             []|\n",
      "|Buy games at GameStop, donate to charity (not GoodWill). ...|[buy game, gamestop, donate, value go, kids play video game]|             []|\n",
      "|                             Fuck Robinhood! Palantir it is!|                                            [fuck robinhood]|             []|\n",
      "|                                                      420.69|                                                          []|             []|\n",
      "|We‚Äôre back!!. This was nice to wake up to! Except almost ...|                                               [blocked buy]|             []|\n",
      "|Just introduced my mom to Reddit. And made her follow thi...|                                    [first account, is lovi]|             []|\n",
      "|Anyone else having issues buying GME on robinhood? The ap...|[buying gme, search result, won't show, noticed gme nok, ...|             []|\n",
      "|                     Trading212 (UK) now restricting access!|                         [trading212 uk, restricting access]|             []|\n",
      "|          Robinhood just made a big mistake.. Your thoughts?|                                               [big mistake]|             []|\n",
      "+------------------------------------------------------------+------------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipelines.build_lda_pipeline()\n",
    "pipeline_model = pipeline.fit(texts)\n",
    "light_model = LightPipeline(pipeline_model)\n",
    "%time processed_texts = light_model.transform(texts)\n",
    "print(f\"Processed (and counted) {df.count()} rows.\")\n",
    "(processed_texts.select([\"text\", \n",
    "                         \"finished_ngrams\", \n",
    "                         \"finished_emojis\"])\n",
    " .show(truncate=60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-stephen",
   "metadata": {},
   "source": [
    "## Topic Modeling using meaningful n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passing-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 ms, sys: 9.41 ms, total: 46.5 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_ngrams')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-excess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                                                                                     topic_words|\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    0|                [gme fall, market manipulation, would be, could be, don't know, limit match, $110k today tomorrow, $gme ride, emergency fund, gamestop. go fuck]|\n",
      "|    1|                   [pop, go, high performance, worth contact, short seller, real time, house rep, net revenue, creator peripheral, moon insert emojis, old lady]|\n",
      "|    2|[short position, stock account, do not need, don't have, gme share, don't sell, doge buy doge buy doge buy doge buy doge buy doge, gme go, institutional inve...|\n",
      "|    3|            [market share, taibbi savage, wall street, short interest, high performance, retail brokerage, former hedge, will shoot, current downward, might be]|\n",
      "|    4|                          [hedge fund, will be, retail investor, daily average trade, would have, last quarter, daily trade, trade growth, last week, can't buy]|\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=160))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-enterprise",
   "metadata": {},
   "source": [
    "## For fun: Topic Modelling using Latent Dirichlet Allocation on Emojis Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressed-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 ms, sys: 4.61 ms, total: 31 ms\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_model = (\n",
    "    CountVectorizer()\n",
    "    .setInputCol('finished_emojis')\n",
    "    .setOutputCol('tfs')\n",
    "    .fit(processed_texts)\n",
    ")\n",
    "lda_feats = tf_model.transform(processed_texts)\n",
    "\n",
    "idf_model = (\n",
    "    IDF()\n",
    "    .setInputCol('tfs')\n",
    "    .setOutputCol('idfs')\n",
    "    .fit(lda_feats)\n",
    ")\n",
    "lda_feats = idf_model.transform(lda_feats).select([\"tfs\", \"idfs\"])\n",
    "\n",
    "lda = (\n",
    "    LDA()\n",
    "    .setFeaturesCol('idfs')\n",
    "    .setK(5)\n",
    "    .setMaxIter(5)\n",
    ")\n",
    "\n",
    "lda_model = lda.fit(lda_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banned-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "useful-referral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n",
      "|topic|                             topic_words|\n",
      "+-----+----------------------------------------+\n",
      "|    0|  [üëé, ü••, ü§ë, Ô∏è, üìà, üå¥, ü§∑, üöÄ, üòÇ, ‚Äç]|\n",
      "|    1|[üíé, üêª, üåà, üëê, üöÄ, üö®, ü§≤, ü§î, üôå, üåù]|\n",
      "|    2|[üöÄ, üòå, ü™ê, ü§≤, üåò, üåà, üêª, üåï, üçå, üôè]|\n",
      "|    3|[üôå, üòî, ü¶ç, üíé, üé•, üçø, ü§ô, üçå, ü™ê, üôè]|\n",
      "|    4| [üåë, ü§°, üçå, üê∏, üöÄ, üåï, ü™ê, ü¶ç, Ô∏è, üé•]|\n",
      "+-----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(lda_model\n",
    " .describeTopics()\n",
    " .withColumn('topic_words', udf_to_words(F.col('termIndices')))\n",
    " .select([\"topic\", \"topic_words\"])\n",
    " .show(truncate=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-hanging",
   "metadata": {},
   "source": [
    "For fun: can you match the topics here with the topics extcated using the emojis? \n",
    "Note: nothing in the method guarantees that this will be possible\n",
    "\n",
    "    +-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    |topic|                                                                                                                                                     topic_words|\n",
    "    +-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    |    0|                [gme fall, market manipulation, would be, could be, don't know, limit match, $110k today tomorrow, $gme ride, emergency fund, gamestop. go fuck]|\n",
    "    |    1|                   [pop, go, high performance, worth contact, short seller, real time, house rep, net revenue, creator peripheral, moon insert emojis, old lady]|\n",
    "    |    2|[short position, stock account, do not need, don't have, gme share, don't sell, doge buy doge buy doge buy doge buy doge buy doge, gme go, institutional inve...|\n",
    "    |    3|            [market share, taibbi savage, wall street, short interest, high performance, retail brokerage, former hedge, will shoot, current downward, might be]|\n",
    "    |    4|                          [hedge fund, will be, retail investor, daily average trade, would have, last quarter, daily trade, trade growth, last week, can't buy]|\n",
    "    +-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

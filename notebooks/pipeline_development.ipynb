{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This is the notebook I used while developing the pipeline\n",
    "References:\n",
    "  - https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "  - and the O'Reilly Spark NLP book, page 76.\n",
    "  \n",
    "Cf https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/codelabs/spark-nlp/topic_model.py, which seems to be very coarse (but I should run my data against it, I guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "The character encoding of the csv file:\n",
      "../data/reddit_wsb.csv: application/csv; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "print(\"\\nThe character encoding of the csv file:\")\n",
    "! file -i {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set multiline=True, we have to use use Java 8. Even still, the column body containing commas within quotes containing quotes, and this confused the csv parser. Solved following https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "sys.path.append('..')\n",
    "%aimport lda_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Converting from Pandas df via df = spark.createDataFrame(df_pd) gives\n",
    "# >> WARN  TaskSetManager:66 - Stage 2 contains a task of very large size \n",
    "# >> (1473 KB). The maximum recommended task size is 100 KB.\n",
    "\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "# print(f'{df.where(df[\"timestamp\"].isNull()).count()} null timestamp values.')\n",
    "\n",
    "# combine text columns and drop unwanted columns\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "texts = df.select(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = lda_pipeline.build_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[text: string, finished_normalized: array<string>]\n",
      "CPU times: user 145 ms, sys: 34.4 ms, total: 180 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = lda_pipeline.build_pipeline()\n",
    "processed_texts = pipeline.fit(texts).transform(texts)\n",
    "print(processed_texts)\n",
    "pddf = processed_texts.toPandas()\n",
    "def examine(i):\n",
    "    sep_string = \"\\n\" + \"-\"*100 + \"\\n\"\n",
    "    print(*pddf.iloc[i], sep=sep_string)\n",
    "i=74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOK and GNUS to the ğŸŒğŸš€ğŸš€ğŸš€let make them pay for the bullshit AH and PMğŸ¤‘ğŸ¤‘. Fellow retards let show these butt lickers you donâ€™t fuck with us and make them go broke for trying to hold us down ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['nok', 'gnus', 'ğŸŒğŸš€ğŸš€ğŸš€let', 'make', 'pay', 'bullshit', 'ah', 'pmğŸ¤‘ğŸ¤‘', 'fellow', 'retard', 'show', 'butt', 'lickers', 'fuck', 'make', 'break', 'hold', 'ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸ¤‘ğŸš€ğŸš€ğŸš€ğŸš€']\n"
     ]
    }
   ],
   "source": [
    "examine(i)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assembler -> tokenizer -> cleaner -> lemmatizer -> normalizer ->\n",
    "with stopwords_cleaner given from pretrained\n",
    "\n",
    "### Some inspection results (varying i-values in examine(i) & using earlier version of lda_pipeline.py)\n",
    "  1. emoji's dropped\n",
    "  2. \"y'all\" |-> \"yall\"\n",
    "  3. becomes one word\n",
    "  4. \"am\" actually comes from \"2am\"; should let numerals survive\n",
    "  8. \"I'm\" |-> im; \"its\" and \"lets\" and \"thats\" and \"isnt\" survive; 2008 is dropped\n",
    "  \n",
    " Conclusions:\n",
    "   - âœ“ should keep: numerals, $, &\n",
    "   - âœ“ long urls\n",
    "   - repeated characters as in \"holdddddd\" and \"woooooo\" and ğŸš€, ğŸš€, ğŸš€\n",
    "   - âœ“ contractions not handled properly\n",
    "       added contractions with RIGHT SINGLE QUOTATION MARK to stopwords list\n",
    "   - âœ“ keep emojis\n",
    "   - handle words like isnt and lets that should have an apostrophe.\n",
    "   - \"don't sell\" should be an exception?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------------------------+\n",
      "|text                                        |finished_normalized         |\n",
      "+--------------------------------------------+----------------------------+\n",
      "|The S&P will go down and we'll have $100000.|[s&p, $100000]              |\n",
      "|Will, let's be sure your calculation is ğŸ’¯. |[calculation, ğŸ’¯]           |\n",
      "|It's was its own problem, 0 wasn't it?      |[problem, 0]                |\n",
      "|420 wasnâ€™t a meme. GME ğŸš€ ğŸš€ ğŸš€             |[420, meme, gme, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|whaaaatttT???                               |[whaaaatttt]                |\n",
      "|ğŸ™… ğŸ™…ğŸ» ğŸ™…ğŸ¼ ğŸ™…ğŸ½ ğŸ™…ğŸ¾ ğŸ™…ğŸ¿                 |[ğŸ™…, ğŸ™…, ğŸ™…, ğŸ™…, ğŸ™…, ğŸ™…]    |\n",
      "+--------------------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"The S&P will go down and we'll have $100000.\",\n",
    "    \"Will, let's be sure your calculation is ğŸ’¯.\",\n",
    "    \"It's was its own problem, 0 wasn't it?\",\n",
    "    \"420 wasnâ€™t a meme. GME ğŸš€ ğŸš€ ğŸš€\",\n",
    "    \"whaaaatttT???\", \n",
    "    \"ğŸ™… ğŸ™…ğŸ» ğŸ™…ğŸ¼ ğŸ™…ğŸ½ ğŸ™…ğŸ¾ ğŸ™…ğŸ¿\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_texts = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "eg_processed_texts = pipeline.fit(eg_texts).transform(eg_texts)\n",
    "\n",
    "eg_processed_texts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "# tokenizer.addSplitChars(\n",
    "#     unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# )\n",
    "\n",
    "# char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "#               'RIGHT SINGLE QUOTATION MARK',\n",
    "#               'LEFT DOUBLE QUOTATION MARK',\n",
    "#               'RIGHT DOUBLE QUOTATION MARK']\n",
    "# for name in char_names:\n",
    "#     tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "# char = unicodedata.lookup('APOSTROPHE')\n",
    "# replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# stopwords = stopwords_cleaner.getStopWords()\n",
    "# for s in stopwords_cleaner.getStopWords():\n",
    "#     if char in s:\n",
    "#         stopwords.append(s.replace(char, replacement))\n",
    "# stopwords.sort()\n",
    "# stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols(['tokenized', \n",
    "                   'cleaned',\n",
    "    ])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 tokenizer,\n",
    "                                 stopwords_cleaner,\n",
    "                                 finisher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a's able about above according accordingly across actually after afterwards again against ain't ainâ€™t all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't arenâ€™t around as aside ask asking associated at available away awfully aâ€™s b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant canâ€™t cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't couldnâ€™t course currently câ€™mon câ€™s d definitely described despite did didn't didnâ€™t different do does doesn't doesnâ€™t doing don't done donâ€™t down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't hadnâ€™t happens hardly has hasn't hasnâ€™t have haven't havenâ€™t having he he's hello help hence her here here's hereafter hereby herein hereupon hereâ€™s hers herself heâ€™s hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't isnâ€™t it it'd it'll it's its itself itâ€™d itâ€™ll itâ€™s iâ€™d iâ€™ll iâ€™m iâ€™ve j just k keep keeps kept know known knows l last lately later latter latterly least less lest let let's letâ€™s like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't shouldnâ€™t since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats thatâ€™s the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon thereâ€™s these they they'd they'll they're they've theyâ€™d theyâ€™ll theyâ€™re theyâ€™ve think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two tâ€™s u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't wasnâ€™t way we we'd we'll we're we've welcome well went were weren't werenâ€™t weâ€™d weâ€™ll weâ€™re weâ€™ve what what's whatever whatâ€™s when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whereâ€™s whether which while whither who who's whoever whole whom whose whoâ€™s why will willing wish with within without won't wonder wonâ€™t would would wouldn't wouldnâ€™t x y yes yet you you'd you'll you're you've your yours yourself yourselves youâ€™d youâ€™ll youâ€™re youâ€™ve z zero\n"
     ]
    }
   ],
   "source": [
    "print(*stopwords_cleaner.getStopWords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.getContextChars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                               finished_normalized|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|              It's was its own problem, wasn't it?|                                         [problem]|\n",
      "|                   420 wasnâ€™t a meme. GME ğŸš€ ğŸš€ ğŸš€|                      [420, meme, gme, ğŸš€, ğŸš€, ğŸš€]|\n",
      "|halt trading â€œto give investors a chance to rec...|[halt, trade, to, give, investor, chance, recal...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"It's was its own problem, wasn't it?\",\n",
    "    \"420 wasnâ€™t a meme. GME ğŸš€ ğŸš€ ğŸš€\",\n",
    "    \"halt trading â€œto give investors a chance to recalibrate their positionsâ€.\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('â€™', '0x2019') (\"'\", '0x27')\n"
     ]
    }
   ],
   "source": [
    "# a-ha!\n",
    "s1 = text_list[3]\n",
    "s2 = \"420 wasn't a meme. GME ğŸš€ ğŸš€ ğŸš€\"\n",
    "assert len(s1) == len(s2)\n",
    "for c1, c2 in zip(s1, s2):\n",
    "    ord1, ord2 = ord(c1), ord(c2)\n",
    "    if ord1 != ord2:\n",
    "        print((c1, hex(ord1)), (c2, hex(ord2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between \"apostrophe\" and \"right single quotation mark\". examine(0) suggests that I have a problem with \"* double quotation mark\" as well, leading to preservation of \"to\" when I process \"â€œto\". On page 281 they replace these things one-by-one using pythongs str.replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT SINGLE QUOTATION MARK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â€˜', 'â€™', 'â€œ', 'â€']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(unicodedata.name(\"\\u2018\"))\n",
    "char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "              'RIGHT SINGLE QUOTATION MARK',\n",
    "              'LEFT DOUBLE QUOTATION MARK',\n",
    "              'RIGHT DOUBLE QUOTATION MARK']\n",
    "[unicodedata.lookup(name) for name in char_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ™…</td>\n",
       "      <td>ğŸ™†</td>\n",
       "      <td>ğŸ™‡</td>\n",
       "      <td>ğŸ™‹</td>\n",
       "      <td>ğŸ™Œ</td>\n",
       "      <td>ğŸ™</td>\n",
       "      <td>ğŸ™</td>\n",
       "      <td>ğŸ™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ™…ğŸ»</td>\n",
       "      <td>ğŸ™†ğŸ»</td>\n",
       "      <td>ğŸ™‡ğŸ»</td>\n",
       "      <td>ğŸ™‹ğŸ»</td>\n",
       "      <td>ğŸ™ŒğŸ»</td>\n",
       "      <td>ğŸ™ğŸ»</td>\n",
       "      <td>ğŸ™ğŸ»</td>\n",
       "      <td>ğŸ™ğŸ»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ™…ğŸ¼</td>\n",
       "      <td>ğŸ™†ğŸ¼</td>\n",
       "      <td>ğŸ™‡ğŸ¼</td>\n",
       "      <td>ğŸ™‹ğŸ¼</td>\n",
       "      <td>ğŸ™ŒğŸ¼</td>\n",
       "      <td>ğŸ™ğŸ¼</td>\n",
       "      <td>ğŸ™ğŸ¼</td>\n",
       "      <td>ğŸ™ğŸ¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ™…ğŸ½</td>\n",
       "      <td>ğŸ™†ğŸ½</td>\n",
       "      <td>ğŸ™‡ğŸ½</td>\n",
       "      <td>ğŸ™‹ğŸ½</td>\n",
       "      <td>ğŸ™ŒğŸ½</td>\n",
       "      <td>ğŸ™ğŸ½</td>\n",
       "      <td>ğŸ™ğŸ½</td>\n",
       "      <td>ğŸ™ğŸ½</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ™…ğŸ¾</td>\n",
       "      <td>ğŸ™†ğŸ¾</td>\n",
       "      <td>ğŸ™‡ğŸ¾</td>\n",
       "      <td>ğŸ™‹ğŸ¾</td>\n",
       "      <td>ğŸ™ŒğŸ¾</td>\n",
       "      <td>ğŸ™ğŸ¾</td>\n",
       "      <td>ğŸ™ğŸ¾</td>\n",
       "      <td>ğŸ™ğŸ¾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ğŸ™…ğŸ¿</td>\n",
       "      <td>ğŸ™†ğŸ¿</td>\n",
       "      <td>ğŸ™‡ğŸ¿</td>\n",
       "      <td>ğŸ™‹ğŸ¿</td>\n",
       "      <td>ğŸ™ŒğŸ¿</td>\n",
       "      <td>ğŸ™ğŸ¿</td>\n",
       "      <td>ğŸ™ğŸ¿</td>\n",
       "      <td>ğŸ™ğŸ¿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1   2   3   4   5   6   7   8\n",
       "0   ğŸ™…   ğŸ™†   ğŸ™‡   ğŸ™‹   ğŸ™Œ   ğŸ™   ğŸ™   ğŸ™\n",
       "1  ğŸ™…ğŸ»  ğŸ™†ğŸ»  ğŸ™‡ğŸ»  ğŸ™‹ğŸ»  ğŸ™ŒğŸ»  ğŸ™ğŸ»  ğŸ™ğŸ»  ğŸ™ğŸ»\n",
       "2  ğŸ™…ğŸ¼  ğŸ™†ğŸ¼  ğŸ™‡ğŸ¼  ğŸ™‹ğŸ¼  ğŸ™ŒğŸ¼  ğŸ™ğŸ¼  ğŸ™ğŸ¼  ğŸ™ğŸ¼\n",
       "3  ğŸ™…ğŸ½  ğŸ™†ğŸ½  ğŸ™‡ğŸ½  ğŸ™‹ğŸ½  ğŸ™ŒğŸ½  ğŸ™ğŸ½  ğŸ™ğŸ½  ğŸ™ğŸ½\n",
       "4  ğŸ™…ğŸ¾  ğŸ™†ğŸ¾  ğŸ™‡ğŸ¾  ğŸ™‹ğŸ¾  ğŸ™ŒğŸ¾  ğŸ™ğŸ¾  ğŸ™ğŸ¾  ğŸ™ğŸ¾\n",
       "5  ğŸ™…ğŸ¿  ğŸ™†ğŸ¿  ğŸ™‡ğŸ¿  ğŸ™‹ğŸ¿  ğŸ™ŒğŸ¿  ğŸ™ğŸ¿  ğŸ™ğŸ¿  ğŸ™ğŸ¿"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emojis = pd.read_csv(\"../emojis.txt\", sep=\"\\t\", header=None)\n",
    "emojis = emojis.drop(columns=[0]).applymap(lambda x: x.strip())\n",
    "L = list(emojis[1])\n",
    "emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that we understand the decoding of emojis we compare with the \"Emoji Modifiers\" section of the wikipedia article https://en.wikipedia.org/wiki/Emoticons_(Unicode_block). \n",
    "\n",
    "> Five symbol modifier characters were added with Unicode 8.0 to provide a range of skin tones for human emoji. These modifiers are called EMOJI MODIFIER FITZPATRICK TYPE-1-2, -3, -4, -5, and -6 (U+1F3FBâ€“U+1F3FF): ğŸ» ğŸ¼ ğŸ½ ğŸ¾ ğŸ¿. They are based on the Fitzpatrick scale for classifying human skin color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™… ['0x1f645']\n",
      "ğŸ™…ğŸ» ['0x1f645', '0x1f3fb']\n",
      "ğŸ™…ğŸ¼ ['0x1f645', '0x1f3fc']\n",
      "ğŸ™…ğŸ½ ['0x1f645', '0x1f3fd']\n",
      "ğŸ™…ğŸ¾ ['0x1f645', '0x1f3fe']\n",
      "ğŸ™…ğŸ¿ ['0x1f645', '0x1f3ff']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "eg = list(emojis[1])\n",
    "for s in eg:\n",
    "    print(s, [hex(ord(c)) for c in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: '\\u...' is for 16-bit hex values, while '\\U...' is for 32-bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™…ğŸ¿\n",
      "ğŸ™… + ğŸ¿ = ğŸ™…ğŸ¿\n",
      "ğŸ» ğŸ¼ ğŸ½ ğŸ¾ ğŸ¿ ğŸ€ ğŸ\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001f645'+'\\U0001f3ff')\n",
    "print(chr(0x1f645),\"+\",chr(0x1f3ff),\n",
    "      \"=\", chr(0x1f645) + chr(0x1f3ff))\n",
    "\n",
    "print(*[chr(n) for n in range(0x1f3fb, 0x1f3ff+3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can just strip `chr(n) for n in range(0x1f3fb, 0x1f3ff+1)` if we don't think there's any useful content in the skin colors used. Of course, in some applications one would definitely want to keep this data, but I don't see a reason to in this context. Why? AFAIK no special meaning to different colors; small set; no demographic questions; even if I wanted to use the skin color info, are there actually good studies about, e.g., how to adjust observed rates to estimate user demograpphics?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "# tokenizer.addSplitChars(\n",
    "#     unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# )\n",
    "\n",
    "# char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "#               'RIGHT SINGLE QUOTATION MARK',\n",
    "#               'LEFT DOUBLE QUOTATION MARK',\n",
    "#               'RIGHT DOUBLE QUOTATION MARK']\n",
    "# for name in char_names:\n",
    "#     tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "# char = unicodedata.lookup('APOSTROPHE')\n",
    "# replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# stopwords = stopwords_cleaner.getStopWords()\n",
    "# for s in stopwords_cleaner.getStopWords():\n",
    "#     if char in s:\n",
    "#         stopwords.append(s.replace(char, replacement))\n",
    "# stopwords.sort()\n",
    "# stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "lemmatizer = (\n",
    "    LemmatizerModel.pretrained()\n",
    "    .setInputCols(['cleaned'])\n",
    "    .setOutputCol('lemmatized')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols([# 'tokenized', \n",
    "                   # 'cleaned',\n",
    "                   # 'lemmatized',\n",
    "                   'normalized'\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: http://unicode.org/faq/emoji_dingbats.html\n",
    "\n",
    "*Q: Can you point me to some examples of emoji characters in Unicode?*\n",
    "\n",
    "*A: The emoji are spread throughout many blocks of Unicode. See Unicode Emoji Charts for a listing of the emoji characters.*\n",
    "\n",
    "I broke out some my exploration of emojis into a separate notebook because it'll probably be useful to have that down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+---------------------------------------------+\n",
      "|text                                  |finished_normalized                          |\n",
      "+--------------------------------------+---------------------------------------------+\n",
      "|ğŸ§¸ ğŸ‚ ğŸ‘±â€â™€ï¸ ğŸ’ğŸ¤² ğŸ§»ğŸ¤² ğŸ®ğŸ›‘ ğŸš€ ğŸ“ˆ ğŸ—   |[ğŸ§¸, ğŸ‚, ğŸ‘±â€â™€ï¸, ğŸ’ğŸ¤², ğŸ§»ğŸ¤², ğŸ®ğŸ›‘, ğŸš€, ğŸ“ˆ, ğŸ—]|\n",
      "|ğŸŒ€ ğŸŒ¤ ğŸ ğŸ€ ğŸ“¿ ğŸ• ğŸ—º ğŸš€ ğŸ¤Œ ğŸ¥‡ ğŸ¥º ğŸ§ ğŸª|[ğŸŒ¤, ğŸ, ğŸ€, ğŸ—º, ğŸš€, ğŸ¤Œ, ğŸ¥‡, ğŸ§]             |\n",
      "+--------------------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing unicode ranges in regular expressions\n",
    "\n",
    "keep_regex = \"\".join(\n",
    "    ['[^0-9A-Za-z$&%',\n",
    "     # stop sign and characters for becky \n",
    "     # (and gendering in general) are special cases\n",
    "     '\\u200d\\u2640\\u2641\\u26A5\\ufe0f\\U0001f6d1',\n",
    "     # now some emoji ranges that cover the ones most\n",
    "     # commonly used in WSB posts     \n",
    "     '\\U0001f324-\\U0001f393',\n",
    "     '\\U0001f39e-\\U0001f3f0',\n",
    "     '\\U0001f400-\\U0001f4fd',\n",
    "     '\\U0001f5fa-\\U0001f64f',\n",
    "     '\\U0001f680-\\U0001f6c5',\n",
    "     '\\U0001f90c-\\U0001f93a',\n",
    "     '\\U0001f947-\\U0001f978',\n",
    "     '\\U0001f9cd-\\U0001f9ff]'])\n",
    "normalizer = (\n",
    "    Normalizer()\n",
    "    .setInputCols(['lemmatized'])\n",
    "    .setOutputCol('normalized')\n",
    "    .setLowercase(True)\n",
    "    .setCleanupPatterns([keep_regex,\n",
    "                         'http.*'])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 tokenizer,\n",
    "                                 stopwords_cleaner,\n",
    "                                 lemmatizer,\n",
    "                                 normalizer,\n",
    "                                 finisher])\n",
    "text_list = [\n",
    "    # these should be kept\n",
    "    \"ğŸ§¸ ğŸ‚ ğŸ‘±â€â™€ï¸ ğŸ’ğŸ¤² ğŸ§»ğŸ¤² ğŸ®ğŸ›‘ ğŸš€ ğŸ“ˆ ğŸ—\",\n",
    "    # some of these should be dropped; cf emojis.ipynb\n",
    "    \"ğŸŒ€ ğŸŒ¤ ğŸ ğŸ€ ğŸ“¿ ğŸ• ğŸ—º ğŸš€ ğŸ¤Œ ğŸ¥‡ ğŸ¥º ğŸ§ ğŸª\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, I will just keep all of the emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine particular pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 3.46 ms, total: 14 ms\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='finished_unigrams',\n",
    "                         outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_texts)\n",
    "tf_result = tf_model.transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.28 ms, total: 12.8 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import IDF\n",
    "idfizer = IDF(inputCol='tf_features', \n",
    "              outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 431 Âµs, total: 2.89 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "lda = LDA(k=num_topics, \n",
    "          maxIter=max_iter, \n",
    "          featuresCol=\"tf_idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 ms, sys: 9.54 ms, total: 18.8 ms\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------+\n",
      "|topic|                                              topicWords|\n",
      "+-----+--------------------------------------------------------+\n",
      "|    0|                             [buy, gme, short, hold, xb]|\n",
      "|    1|                           [', stock, market, it's, gon]|\n",
      "|    2|[clearinghouse, webull, portfolio, schwab, organization]|\n",
      "|    3|                      [security, margin, gon, uh, cheap]|\n",
      "|    4|                         [fund, hedge, money, let, play]|\n",
      "+-----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 5\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare pipeline time usage with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f58d2553d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f58d2569590>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f58d2830c20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f58d2830d70>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f58d24b7cd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f58d24c6e60>),\n",
       " ('preprocessor', <__main__.FilterTextPreprocessing at 0x7f58d27acdd0>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "data_path = \"reddit_wsb.csv\"\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class FilterTextPreprocessing:\n",
    "    def __init__(self, nlp):\n",
    "        Doc.set_extension('bow', default=[], force=True)\n",
    "        Token.set_extension('keep', default=True, force=True)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        patterns = [\n",
    "            {\"string_id\": \"stop_word\", \"pattern\": [[{\"IS_STOP\": True}]]},\n",
    "            {\"string_id\": \"punctuation\", \"pattern\": [[{\"IS_PUNCT\": True}]]},\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for patt_obj in patterns:\n",
    "            string_id = patt_obj.get('string_id')\n",
    "            pattern = patt_obj.get('pattern')\n",
    "            self.matcher.add(string_id, pattern, on_match=self.on_match)\n",
    "   \n",
    "    def on_match(self, matcher, doc, i, matches):\n",
    "        _, start, end = matches[i]\n",
    "        for tkn in doc[start:end]:\n",
    "            tkn._.keep = False\n",
    "              \n",
    "    def __call__(self, doc) :\n",
    "        self.matcher(doc)\n",
    "        doc._.bow = [tkn.lemma_ for tkn in doc if tkn._.keep]\n",
    "        return doc\n",
    "      \n",
    "#     @classmethod\n",
    "#     def from_pattern_file(cls, nlp, path) :\n",
    "#         patterns = read_json(path)\n",
    "#         return cls(nlp, patterns)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@English.factory(\"preprocessor\")\n",
    "def create_preprocessor(nlp, name):\n",
    "    return FilterTextPreprocessing(nlp)\n",
    "\n",
    "# nlp.select_pipes(enable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"preprocessor\", last=True)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1000\n",
      "i = 2000\n",
      "i = 3000\n",
      "i = 4000\n",
      "i = 5000\n",
      "i = 6000\n",
      "i = 7000\n",
      "i = 8000\n",
      "i = 9000\n",
      "i = 10000\n",
      "i = 11000\n",
      "i = 12000\n",
      "i = 13000\n",
      "i = 14000\n",
      "i = 15000\n",
      "i = 16000\n",
      "i = 17000\n",
      "i = 18000\n",
      "i = 19000\n",
      "i = 20000\n",
      "i = 21000\n",
      "i = 22000\n",
      "i = 23000\n",
      "i = 24000\n",
      "i = 25000\n",
      "CPU times: user 6min 7s, sys: 2.59 s, total: 6min 10s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process(filename):\n",
    "    with open(filename, \"r\") as fobj:\n",
    "        datareader = csv.DictReader(fobj)\n",
    "        for row in datareader:\n",
    "            text = \" \".join([row[\"title\"],\n",
    "                              row[\"body\"]])\n",
    "            yield nlp(text)\n",
    "            \n",
    "gen = process(data_path)\n",
    "\n",
    "words = []\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        doc = next(gen)\n",
    "        words.append(doc._.bow)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f\"i = {i}\")\n",
    "    \n",
    "words =  pd.Series(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [money, send, message, ğŸš€, ğŸ’, ğŸ™Œ]\n",
       "1        [Math, Professor, Scott, Steiner, say, number,...\n",
       "2        [exit, system, CEO, NASDAQ, push, halt, tradin...\n",
       "3             [new, SEC, filing, GME, retarded, interpret]\n",
       "4              [distract, GME, think, AMC, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                 [hold, GME, ğŸš€, ğŸš€, ğŸš€]\n",
       "25644                    [AMC, Yolo, Update, Feb, 3, 2021]\n",
       "25645                                         [loss, sell]\n",
       "25646     [post, curiosity, teem, know, store, ğŸ‘€, ğŸ’, ğŸ–, ğŸš€]\n",
       "Length: 25647, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [money, send, message]\n",
       "1        [math, professor, scott, steiner, number, spel...\n",
       "2        [exit, system, ceo, nasdaq, push, halt, trade,...\n",
       "3               [new, sec, file, gme, retarded, interpret]\n",
       "4              [distract, gme, think, amc, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                          [hold, gme]\n",
       "25644                             [amc, yolo, update, feb]\n",
       "25645                                         [loss, sell]\n",
       "25646           [dont, post, curiosity, teem, know, store]\n",
       "Name: finished_unigrams, Length: 25647, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.finished_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pipeline = lda_pipeline.build_pipeline()\n",
    "# processed_texts = pipeline.fit(texts).transform(texts)\n",
    "# print(processed_texts)\n",
    "\n",
    "# for fair comparison with SpaCy below, should build pandas dataframe.\n",
    "# will throw TaskSetManager:66 - Stage 4 contains a task of very large size\n",
    "# df_post = processed_texts.toPandas()  \n",
    "# df_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.946236559139784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "371/18.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed comparison: The sparknlp pipeline took 18.6 seconds, while spaCy took 371 second (20x as long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with Stanza and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 25647 entries, 2021-01-28 21:37:41 to 2021-02-04 07:54:27\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      25647 non-null  object\n",
      " 1   title   25647 non-null  object\n",
      " 2   body    25647 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 801.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.read_csv(data_path,\n",
    "                 index_col=\"timestamp\", \n",
    "                 parse_dates=True, \n",
    "                 keep_default_na=False)\n",
    "# df_pd = df_pd.assign(timestamp=pd.to_datetime(df_pd.timestamp))\n",
    "df_pd = df_pd[[\"id\", \"title\", \"body\"]]\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = df.shape[0]//ddf.shape[0]\n",
    "dfs = [df.iloc[bin_size*i : bin_size*(i+1)] for i in range(ddf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I got in late on GME but I believe in the cause and am willing to lose it all.',\n",
       " \"You guys are amazing. Thank you for sending GME to the moon! I know I'm going to lose most of my money here because I'll hold the line until the end. Let's send a clear message to wall street with GME, BB, AMC, and any others. I've never day traded before but I'm in it now. ğŸš€\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = dfs[0]\n",
    "X = df0.iloc[2]\n",
    "X.title, X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 15:42:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-14 15:42:21 INFO: Use device: cpu\n",
      "2021-02-14 15:42:21 INFO: Loading: tokenize\n",
      "2021-02-14 15:42:21 INFO: Loading: pos\n",
      "2021-02-14 15:42:21 INFO: Loading: lemma\n",
      "2021-02-14 15:42:21 INFO: Loading: depparse\n",
      "2021-02-14 15:42:21 INFO: Loading: sentiment\n",
      "2021-02-14 15:42:22 INFO: Loading: ner\n",
      "2021-02-14 15:42:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ You guys are amazing.\n",
      "+ Thank you for sending GME to the moon!\n",
      "- I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "â“ƒ Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "â“ƒ I've never day traded before but I'm in it now.\n",
      "â“ƒ ğŸš€\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "text = df.iloc[2].body\n",
    "doc = nlp(text)\n",
    "d_sent = {0:\"-\", 1:\"â“ƒ\", 2:\"+\"}\n",
    "for sent in doc.sentences:\n",
    "    print(d_sent[sent.sentiment], sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ I just love it when the regulators step in.\n",
      "- That was amazingly boring.\n",
      "+ That was amazingly tolerable.\n",
      "- At least it wasn't boring.\n",
      "CPU times: user 1.43 s, sys: 21.1 ms, total: 1.45 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in [\"I just love it when the regulators step in.\",\n",
    "          \"That was amazingly boring.\",\n",
    "          \"That was amazingly tolerable.\",\n",
    "          \"At least it wasn't boring.\"]:\n",
    "    sentiment = nlp(s).sentences[0].sentiment\n",
    "    print(d_sent[sentiment], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 You guys are amazing.\n",
      "0.0 Thank you for sending GME to the moon!\n",
      "0.0 I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "0.0 Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "0.0 I've never day traded before\n",
      "0.0 but I'm in it now.\n",
      "0.0 ğŸš€\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = df.iloc[2].body\n",
    "d_sent = {0:\"-\", 1:\"â“ƒ\", 2:\"+\"}\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.sentiment, sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This is the notebook I used while developing the pipeline\n",
    "References:\n",
    "  - https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "  - and the O'Reilly Spark NLP book, page 76.\n",
    "  \n",
    "Cf https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/codelabs/spark-nlp/topic_model.py, which seems to be very coarse (but I should run my data against it, I guess).\n",
    "\n",
    "See Conclusion section at the bottom for some final comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The character encoding of the csv file:\n",
      "../data/reddit_wsb.csv: application/csv; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "print(\"\\nThe character encoding of the csv file:\")\n",
    "! file -i {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set multiline=True, we have to use use Java 8. Even still, the column body containing commas within quotes containing quotes, and this confused the csv parser. Solved following https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()\n",
    "sys.path.append('..')\n",
    "%aimport lda_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Converting from Pandas df via df = spark.createDataFrame(df_pd) gives\n",
    "# >> WARN  TaskSetManager:66 - Stage 2 contains a task of very large size \n",
    "# >> (1473 KB). The maximum recommended task size is 100 KB.\n",
    "\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "# print(f'{df.where(df[\"timestamp\"].isNull()).count()} null timestamp values.')\n",
    "\n",
    "# combine text columns and drop unwanted columns\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "texts = df.select(\"text\")\n",
    "\n",
    "pipeline = lda_pipeline.build_unigram_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+---------------------------------+\n",
      "|text                                               |finished_unigrams                |\n",
      "+---------------------------------------------------+---------------------------------+\n",
      "|The S&P will go down and we'll have $100000.       |[s&p, go, $100000]               |\n",
      "|Will, let's be sure your calculation lets us be 💯.|[sure, calculation, we, 💯]      |\n",
      "|Wasnt was its own problem, wasn't it?              |[problem]                        |\n",
      "|420 wasn’t a meme. GME 🚀 🚀 🚀                    |[420, meme, gme, 🚀🚀🚀]         |\n",
      "|Don't sell 👱‍♀️, yall shouldn't sell              |[dontsell, 👱‍♀️, shouldntsell]  |\n",
      "|Y'all: do not sell, should not sell, never sell    |[notsell, notsell, neversell]    |\n",
      "|🙅 🙅🏻 🙅🏼 🙅🏽 🙅🏾 🙅🏿                        |[🙅, 🙅, 🙅, 🙅, 🙅, 🙅]         |\n",
      "|stop, game stop, game stonk, cody                  |[stop, gamestop, gamestonk, cody]|\n",
      "+---------------------------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = lda_pipeline.build_unigram_pipeline()\n",
    "\n",
    "text_list = [\n",
    "    \"The S&P will go down and we'll have $100000.\",\n",
    "    \"Will, let's be sure your calculation lets us be 💯.\",\n",
    "    \"Wasnt was its own problem, wasn't it?\",\n",
    "    \"420 wasn’t a meme. GME 🚀 🚀 🚀\",\n",
    "    \"Don't sell 👱‍♀️, yall shouldn't sell\",\n",
    "    \"Y'all: do not sell, should not sell, never sell\",\n",
    "    \"🙅 🙅🏻 🙅🏼 🙅🏽 🙅🏾 🙅🏿\",\n",
    "    \"stop, game stop, game stonk, cody\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_texts = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "eg_processed_texts = pipeline.fit(eg_texts).transform(eg_texts)\n",
    "\n",
    "eg_processed_texts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[text: string, finished_unigrams: array<string>]\n",
      "CPU times: user 130 ms, sys: 24.4 ms, total: 154 ms\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_texts = pipeline.fit(texts).transform(texts)\n",
    "print(processed_texts)\n",
    "pddf = processed_texts.toPandas()\n",
    "def examine(i):\n",
    "    sep_string = \"\\n\" + \"-\"*100 + \"\\n\"\n",
    "    print(*pddf.iloc[i], sep=sep_string)\n",
    "i = 74 # 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robinhood should rebrand to Sheriff of Nottingham! What a joke...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['robinhood', 'rebrand', 'sheriff', 'nottingham', 'joke']\n"
     ]
    }
   ],
   "source": [
    "examine(i)\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assembler -> tokenizer -> cleaner -> lemmatizer -> normalizer ->\n",
    "with stopwords_cleaner given from pretrained\n",
    "\n",
    "### Some inspection results (varying i-values in examine(i) & using earlier version of lda_pipeline.py)\n",
    "  1. emoji's dropped\n",
    "  2. \"y'all\" |-> \"yall\"\n",
    "  3. becomes one word\n",
    "  4. \"am\" actually comes from \"2am\"; should let numerals survive\n",
    "  8. \"I'm\" |-> im; \"its\" and \"thats\" and \"isnt\" survive (\"lets is a lost cause\"); 2008 is dropped\n",
    "  \n",
    " Conclusions:\n",
    "   - ✓ should keep: numerals, $, &\n",
    "   - ✓ long urls\n",
    "   - IOU: repeated characters as in \"holdddddd\" and \"woooooo\" and 🚀, 🚀, 🚀\n",
    "   - ✓ contractions not handled properly\n",
    "       added contractions with RIGHT SINGLE QUOTATION MARK to stopwords list\n",
    "   - ✓ keep emojis\n",
    "   - ✓ handle words like isnt and that should have an apostrophe.\n",
    "   - ✓ \"don't sell\" should be an exception?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "# tokenizer.addSplitChars(\n",
    "#     unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# )\n",
    "\n",
    "# char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "#               'RIGHT SINGLE QUOTATION MARK',\n",
    "#               'LEFT DOUBLE QUOTATION MARK',\n",
    "#               'RIGHT DOUBLE QUOTATION MARK']\n",
    "# for name in char_names:\n",
    "#     tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "# char = unicodedata.lookup('APOSTROPHE')\n",
    "# replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# stopwords = stopwords_cleaner.getStopWords()\n",
    "# for s in stopwords_cleaner.getStopWords():\n",
    "#     if char in s:\n",
    "#         stopwords.append(s.replace(char, replacement))\n",
    "# stopwords.sort()\n",
    "# stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols(['tokenized', \n",
    "                   'cleaned',\n",
    "    ])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 tokenizer,\n",
    "                                 stopwords_cleaner,\n",
    "                                 finisher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a's able about above according accordingly across actually after afterwards again against ain't all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't course currently d definitely described despite did didn't different do does doesn't doing don't done down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't happens hardly has hasn't have haven't having he he's hello help hence her here here's hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't it it'd it'll it's its itself j just k keep keeps kept know knows known l last lately later latter latterly least less lest let let's like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would would wouldn't x y yes yet you you'd you'll you're you've your yours yourself yourselves z zero\n"
     ]
    }
   ],
   "source": [
    "print(*stopwords_cleaner.getStopWords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.getContextChars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                                finished_tokenized|                                  finished_cleaned|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|              It's was its own problem, wasn't it?|  [It's, was, its, own, problem, ,, wasn't, it, ?]|                                   [problem, ,, ?]|\n",
      "|                   420 wasn’t a meme. GME 🚀 🚀 🚀|        [420, wasn’t, a, meme, ., GME, 🚀, 🚀, 🚀]|           [420, wasn’t, meme, ., GME, 🚀, 🚀, 🚀]|\n",
      "|halt trading “to give investors a chance to rec...|[halt, trading, “to, give, investors, a, chance...|[halt, trading, “to, give, investors, chance, r...|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"It's was its own problem, wasn't it?\",\n",
    "    \"420 wasn’t a meme. GME 🚀 🚀 🚀\",\n",
    "    \"halt trading “to give investors a chance to recalibrate their positions”.\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('’', '0x2019') (\"'\", '0x27')\n"
     ]
    }
   ],
   "source": [
    "# a-ha!\n",
    "s1 = text_list[1]\n",
    "s2 = \"420 wasn't a meme. GME 🚀 🚀 🚀\"\n",
    "assert len(s1) == len(s2)\n",
    "for c1, c2 in zip(s1, s2):\n",
    "    ord1, ord2 = ord(c1), ord(c2)\n",
    "    if ord1 != ord2:\n",
    "        print((c1, hex(ord1)), (c2, hex(ord2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between \"apostrophe\" and \"right single quotation mark\". examine(0) suggests that I have a problem with \"* double quotation mark\" as well, leading to preservation of \"to\" when I process \"“to\". On page 281 they replace these things one-by-one using pythongs str.replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT SINGLE QUOTATION MARK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‘', '’', '“', '”']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(unicodedata.name(\"\\u2018\"))\n",
    "char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "              'RIGHT SINGLE QUOTATION MARK',\n",
    "              'LEFT DOUBLE QUOTATION MARK',\n",
    "              'RIGHT DOUBLE QUOTATION MARK']\n",
    "[unicodedata.lookup(name) for name in char_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = pd.DataFrame([\n",
    "    ['🙅','🙆','🙇','🙋','🙌','🙍','🙎','🙏'],\n",
    "    ['🙅🏻','🙆🏻','🙇🏻','🙋🏻','🙌🏻','🙍🏻','🙎🏻','🙏🏻'],\n",
    "    ['🙅🏼','🙆🏼','🙇🏼','🙋🏼','🙌🏼','🙍🏼','🙎🏼','🙏🏼'],\n",
    "    ['🙅🏽','🙆🏽','🙇🏽','🙋🏽','🙌🏽','🙍🏽','🙎🏽','🙏🏽'],\n",
    "    ['🙅🏾','🙆🏾','🙇🏾','🙋🏾','🙌🏾','🙍🏾','🙎🏾','🙏🏾'],\n",
    "    ['🙅🏿','🙆🏿','🙇🏿','🙋🏿','🙌🏿','🙍🏿','🙎🏿','🙏🏿'],\n",
    "    ['🙅','🙆','🙇','🙋','🙌','🙍','🙎','🙏'],\n",
    "    ['🙅🏻','🙆🏻','🙇🏻','🙋🏻','🙌🏻','🙍🏻','🙎🏻','🙏🏻'],\n",
    "    ['🙅🏼','🙆🏼','🙇🏼','🙋🏼','🙌🏼','🙍🏼','🙎🏼','🙏🏼'],\n",
    "    ['🙅🏽','🙆🏽','🙇🏽','🙋🏽','🙌🏽','🙍🏽','🙎🏽','🙏🏽'],\n",
    "    ['🙅🏾','🙆🏾','🙇🏾','🙋🏾','🙌🏾','🙍🏾','🙎🏾','🙏🏾'],\n",
    "    ['🙅🏿','🙆🏿','🙇🏿','🙋🏿','🙌🏿','🙍🏿','🙎🏿','🙏🏿']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that we understand the decoding of emojis we compare with the \"Emoji Modifiers\" section of the wikipedia article https://en.wikipedia.org/wiki/Emoticons_(Unicode_block). \n",
    "\n",
    "> Five symbol modifier characters were added with Unicode 8.0 to provide a range of skin tones for human emoji. These modifiers are called EMOJI MODIFIER FITZPATRICK TYPE-1-2, -3, -4, -5, and -6 (U+1F3FB–U+1F3FF): 🏻 🏼 🏽 🏾 🏿. They are based on the Fitzpatrick scale for classifying human skin color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙆 ['0x1f646']\n",
      "🙆🏻 ['0x1f646', '0x1f3fb']\n",
      "🙆🏼 ['0x1f646', '0x1f3fc']\n",
      "🙆🏽 ['0x1f646', '0x1f3fd']\n",
      "🙆🏾 ['0x1f646', '0x1f3fe']\n",
      "🙆🏿 ['0x1f646', '0x1f3ff']\n",
      "🙆 ['0x1f646']\n",
      "🙆🏻 ['0x1f646', '0x1f3fb']\n",
      "🙆🏼 ['0x1f646', '0x1f3fc']\n",
      "🙆🏽 ['0x1f646', '0x1f3fd']\n",
      "🙆🏾 ['0x1f646', '0x1f3fe']\n",
      "🙆🏿 ['0x1f646', '0x1f3ff']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "eg = list(emojis[1])\n",
    "for s in eg:\n",
    "    print(s, [hex(ord(c)) for c in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: '\\u...' is for 16-bit hex values, while '\\U...' is for 32-bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙅🏿\n",
      "🙅 + 🏿 = 🙅🏿\n",
      "🏻 🏼 🏽 🏾 🏿 🐀 🐁\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001f645'+'\\U0001f3ff')\n",
    "print(chr(0x1f645),\"+\",chr(0x1f3ff),\n",
    "      \"=\", chr(0x1f645) + chr(0x1f3ff))\n",
    "\n",
    "print(*[chr(n) for n in range(0x1f3fb, 0x1f3ff+3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can just strip `chr(n) for n in range(0x1f3fb, 0x1f3ff+1)` if we don't think there's any useful content in the skin colors used. Of course, in some applications one would definitely want to keep this data, but I don't see a reason to in this context. Why? AFAIK no special meaning to different colors; small set; no demographic questions; even if I wanted to use the skin color info, are there actually good studies about, e.g., how to adjust observed rates to estimate user demograpphics?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "# tokenizer.addSplitChars(\n",
    "#     unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# )\n",
    "\n",
    "# char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "#               'RIGHT SINGLE QUOTATION MARK',\n",
    "#               'LEFT DOUBLE QUOTATION MARK',\n",
    "#               'RIGHT DOUBLE QUOTATION MARK']\n",
    "# for name in char_names:\n",
    "#     tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "# char = unicodedata.lookup('APOSTROPHE')\n",
    "# replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# stopwords = stopwords_cleaner.getStopWords()\n",
    "# for s in stopwords_cleaner.getStopWords():\n",
    "#     if char in s:\n",
    "#         stopwords.append(s.replace(char, replacement))\n",
    "# stopwords.sort()\n",
    "# stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "lemmatizer = (\n",
    "    LemmatizerModel.pretrained()\n",
    "    .setInputCols(['cleaned'])\n",
    "    .setOutputCol('lemmatized')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols([# 'tokenized', \n",
    "                   # 'cleaned',\n",
    "                   # 'lemmatized',\n",
    "                   'normalized'\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: http://unicode.org/faq/emoji_dingbats.html\n",
    "\n",
    "*Q: Can you point me to some examples of emoji characters in Unicode?*\n",
    "\n",
    "*A: The emoji are spread throughout many blocks of Unicode. See Unicode Emoji Charts for a listing of the emoji characters.*\n",
    "\n",
    "I broke out some my exploration of emojis into a separate notebook because it'll probably be useful to have that down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+---------------------------------------------+\n",
      "|text                                  |finished_normalized                          |\n",
      "+--------------------------------------+---------------------------------------------+\n",
      "|🧸 🐂 👱‍♀️ 💎🤲 🧻🤲 🎮🛑 🚀 📈 🍗   |[🧸, 🐂, 👱‍♀️, 💎🤲, 🧻🤲, 🎮🛑, 🚀, 📈, 🍗]|\n",
      "|🌀 🌤 🎞 🐀 📿 🕐 🗺 🚀 🤌 🥇 🥺 🧍 🪐|[🌤, 🎞, 🐀, 🗺, 🚀, 🤌, 🥇, 🧍]             |\n",
      "+--------------------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing unicode ranges in regular expressions\n",
    "\n",
    "keep_regex = \"\".join(\n",
    "    ['[^0-9A-Za-z$&%',\n",
    "     # stop sign and characters for becky \n",
    "     # (and gendering in general) are special cases\n",
    "     '\\u200d\\u2640\\u2641\\u26A5\\ufe0f\\U0001f6d1',\n",
    "     # now some emoji ranges that cover the ones most\n",
    "     # commonly used in WSB posts     \n",
    "     '\\U0001f324-\\U0001f393',\n",
    "     '\\U0001f39e-\\U0001f3f0',\n",
    "     '\\U0001f400-\\U0001f4fd',\n",
    "     '\\U0001f5fa-\\U0001f64f',\n",
    "     '\\U0001f680-\\U0001f6c5',\n",
    "     '\\U0001f90c-\\U0001f93a',\n",
    "     '\\U0001f947-\\U0001f978',\n",
    "     '\\U0001f9cd-\\U0001f9ff]'])\n",
    "normalizer = (\n",
    "    Normalizer()\n",
    "    .setInputCols(['lemmatized'])\n",
    "    .setOutputCol('normalized')\n",
    "    .setLowercase(True)\n",
    "    .setCleanupPatterns([keep_regex,\n",
    "                         'http.*'])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 tokenizer,\n",
    "                                 stopwords_cleaner,\n",
    "                                 lemmatizer,\n",
    "                                 normalizer,\n",
    "                                 finisher])\n",
    "text_list = [\n",
    "    # these should be kept\n",
    "    \"🧸 🐂 👱‍♀️ 💎🤲 🧻🤲 🎮🛑 🚀 📈 🍗\",\n",
    "    # some of these should be dropped; cf emojis.ipynb\n",
    "    \"🌀 🌤 🎞 🐀 📿 🕐 🗺 🚀 🤌 🥇 🥺 🧍 🪐\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, I will just keep all of the emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "+--------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|text                                                    |finished_sentences                                         |finished_tokenized                                                         |\n",
      "+--------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|Can this tell 'what is truly a sentence'? Or can it not.|[Can this tell 'what is truly a sentence'?, Or can it not.]|[Can, this, tell, ', what, is, truly, a, sentence, '?, Or, can, it, not, .]|\n",
      "|Uhhh.... ok.                                            |[Uhhh., ., ., ., ok.]                                      |[Uhhh, ., ., ., ., ok, .]                                                  |\n",
      "+--------------------------------------------------------+-----------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "sentence_detector = (\n",
    "    SentenceDetector()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('sentences')\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['sentences'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols(['sentences', \n",
    "                   'tokenized'\n",
    "    ])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer,\n",
    "                                 finisher])\n",
    "\n",
    "text_list = [\n",
    "    # these should be kept\n",
    "    \"Can this tell 'what is truly a sentence'? Or can it not.\",\n",
    "    # some of these should be dropped; cf emojis.ipynb\n",
    "    \"Uhhh.... ok.\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sparknlp.annotator.Tokenizer'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\S+ sell', '\\\\S+ hold']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(assembler,\n",
    " sentence_detector,\n",
    " tokenizer,\n",
    " stopwords_cleaner,\n",
    " lemmatizer,\n",
    " normalizer,\n",
    " finisher) = lda_pipeline.get_unigram_pipeline_components()\n",
    "\n",
    "tokenizer.setExceptions([\"\\S+ sell\", \"\\S+ hold\"])\n",
    "tokenizer.setCaseSensitiveExceptions(True)\n",
    "\n",
    "pipeline = lda_pipeline.build_unigram_pipeline(\n",
    "    (assembler,\n",
    "     sentence_detector,\n",
    "     tokenizer,\n",
    "     stopwords_cleaner,\n",
    "     lemmatizer,\n",
    "     normalizer,\n",
    "     finisher\n",
    "    )\n",
    ")\n",
    "T = pipeline.getStages()[2]\n",
    "print(type(T))\n",
    "T.getExceptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+---------------------------------+\n",
      "|text                                                |finished_unigrams                |\n",
      "+----------------------------------------------------+---------------------------------+\n",
      "|Don't sell, I say, don't sell.                      |[dontsell, dontsell]             |\n",
      "|Do not sell, do not sell!                           |[notsell, notsell]               |\n",
      "|Shouldn't sell. Should not sell                     |[shouldntsell, notsell]          |\n",
      "|Why not sell? Shoudl sell!                          |[notsell, shoudlsell]            |\n",
      "|I don't see why anybody should ever sell.           |[eversell]                       |\n",
      "|Some say one mustn't hold. Rubbish! One should hold.|[mustnthold, rubbish, shouldhold]|\n",
      "+----------------------------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Don't sell, I say, don't sell.\",\n",
    "    \"Do not sell, do not sell!\",\n",
    "    \"Shouldn't sell. Should not sell\",\n",
    "    \"Why not sell? Shoudl sell!\",\n",
    "    \"I don't see why anybody should ever sell.\",\n",
    "    \"Some say one mustn't hold. Rubbish! One should hold.\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀, ROCKET\n"
     ]
    }
   ],
   "source": [
    "! cat matcher_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[^0-9A-Za-z$&%=\\u200d♀♁⚥️🌀-🌡🌤-🎓🎞-🏰🐀-📽📿-🔽🕐-🕧🗺-🙏🚀-🛅\\U0001f90c-🤺🥇-\\U0001f978🥺-\\U0001f9cb\\U0001f9cd-🧿\\U0001fa90-\\U0001faa8]',\n",
       " 'http.*',\n",
       " '(🚀){3,}\\x01']"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "(assembler,\n",
    " sentence_detector,\n",
    " tokenizer,\n",
    " stopwords_cleaner,\n",
    " lemmatizer,\n",
    " normalizer,\n",
    " finisher) = lda_pipeline.get_unigram_pipeline_components()\n",
    "\n",
    "pipeline = lda_pipeline.build_unigram_pipeline(\n",
    "    pipeline_components = (assembler,\n",
    "                           sentence_detector,\n",
    "                           tokenizer,\n",
    "                           stopwords_cleaner,\n",
    "                           lemmatizer,\n",
    "                           normalizer,\n",
    "                           finisher)\n",
    ")\n",
    "\n",
    "(assembler,\n",
    " sentence_detector,\n",
    " tokenizer,\n",
    " stopwords_cleaner,\n",
    " lemmatizer,\n",
    " normalizer,\n",
    " finisher) = pipeline.getStages()\n",
    "\n",
    "# ! echo \"[🚀|🚀\\s+]+~rockets\" > matcher_rules.csv\n",
    "# matcher = RegexMatcher()\n",
    "# matcher.setExternalRules(\"matcher_rules.csv\", delimiter=\"~\")\n",
    "# matcher.setInputCols([\"document\"]).setOutputCol(\"matcher_output\")\n",
    "# tokenizer.setInputCols([\"matcher_output\"])\n",
    "\n",
    "# ! echo \"🚀+,🚀\" > slangDict.csv\n",
    "# normalizer.setSlangDictionary(\"slangDict.csv\", delimiter=\",\")\n",
    "\n",
    "ngrammer = (\n",
    "    NGramGenerator()\n",
    "    .setN(3)\n",
    "    .setEnableCumulative(False)\n",
    "    .setDelimiter(\"_\")\n",
    ")\n",
    "\n",
    "(ngrammer.setInputCols([\"unigrams\"])\n",
    " .setOutputCol(\"ngrams\"))\n",
    "\n",
    "finisher.setInputCols([# \"tokenized\",\n",
    "                       # \"unigrams\", \n",
    "#                        \"ngrams\"\n",
    "                        \"matcher_output\"\n",
    "                      ])\n",
    "\n",
    "\n",
    "# from sparknlp.annotator import RegexTokenizer\n",
    "# tokenizer = RegexTokenizer()\n",
    "# tokenizer_pattern = \"\".join([\n",
    "#     \"\\s+\"\n",
    "# #     \"|(🚀\\s+){3,}|(🚀){3,}|(💎🙌){2,}\"\n",
    "# ])\n",
    "# # tokenizer_pattern= \"\\s+\"\n",
    "# tokenizer.setPattern(tokenizer_pattern)\n",
    "# tokenizer.setInputCols([\"sentences\"])\n",
    "# tokenizer.setOutputCol(\"tokenized\")\n",
    "# print(tokenizer.getPattern())\n",
    "\n",
    "# from sparknlp.annotator import DocumentNormalizer\n",
    "# doc_normalizer = DocumentNormalizer()\n",
    "\n",
    "pipeline = (\n",
    "    Pipeline().setStages([\n",
    "        assembler,\n",
    "        matcher,\n",
    "#         sentence_detector,\n",
    "#         tokenizer,\n",
    "#         stopwords_cleaner,\n",
    "#         lemmatizer,\n",
    "#         normalizer,\n",
    "#         ngrammer,\n",
    "         finisher\n",
    "    ])\n",
    ")\n",
    "\n",
    "cleanup_patterns = normalizer.getCleanupPatterns()\n",
    "cleanup_patterns.append(\"(🚀){3,}\\1\")\n",
    "normalizer.setCleanupPatterns(cleanup_patterns)\n",
    "normalizer.getCleanupPatterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------+\n",
      "|text                                      |finished_matcher_output|\n",
      "+------------------------------------------+-----------------------+\n",
      "|Does doo-dad split? Does it split   right?|[ ,  ,  ,  ,  ,    ]   |\n",
      "|🚀                                        |[🚀]                   |\n",
      "|Hey 🚀🚀🚀🚀🚀                            |[ 🚀🚀🚀🚀🚀]          |\n",
      "|Hey 🚀 🚀 🚀 🚀 🚀                        |[ 🚀 🚀 🚀 🚀 🚀]      |\n",
      "|💎🙌 💎🙌 💎🙌💎🙌                        |[ ,  ]                 |\n",
      "+------------------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Does doo-dad split? Does it split   right?\",\n",
    "    \"🚀\",\n",
    "    \"Hey 🚀🚀🚀🚀🚀\",\n",
    "    \"Hey 🚀 🚀 🚀 🚀 🚀\",\n",
    "    \"💎🙌 💎🙌 💎🙌💎🙌\"\n",
    "]\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.select([\"text\", \"finished_matcher_output\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|Does doo-dad split? Does it split   right?|\n",
      "|🚀                                        |\n",
      "|Hey 🚀🚀🚀🚀🚀                            |\n",
      "|Hey 🚀 🚀 🚀 🚀 🚀                        |\n",
      "|💎🙌 💎🙌 💎🙌💎🙌                        |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eg_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          reeeplaced|\n",
      "+--------------------+\n",
      "|Does doo-dad spli...|\n",
      "|              Hey 🚀|\n",
      "|              Hey 🚀|\n",
      "|  💎🙌 💎🙌 💎🙌💎🙌|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eg_df.select(F.regexp_replace('text', r'((🚀\\s*){2,})', '🚀').alias('reeeplaced')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------------------------------------+-----------------------------+\n",
      "|text                                      |finished_tokenized                                  |finished_unigrams            |\n",
      "+------------------------------------------+----------------------------------------------------+-----------------------------+\n",
      "|Does doo-dad split? Does it split   right?|[Does, doo-dad, split, ?, Does, it, split, right, ?]|[doodad, split, split, right]|\n",
      "|Hey 🚀🚀🚀🚀🚀                            |[Hey, 🚀🚀🚀🚀🚀]                                   |[hey, 🚀🚀🚀🚀🚀]            |\n",
      "|Hey 🚀 🚀 🚀 🚀 🚀                        |[Hey, 🚀 🚀 🚀 🚀 🚀]                               |[hey, 🚀🚀🚀🚀🚀]            |\n",
      "|💎🙌 💎🙌 💎🙌💎🙌                        |[💎🙌 💎🙌 💎🙌💎🙌]                                |[💎🙌💎🙌💎🙌💎🙌]           |\n",
      "+------------------------------------------+----------------------------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Extract a specific group matched by a Java regex, from the specified string column.\n",
       "If the regex did not match, or the specified group did not match, an empty string is returned.\n",
       "\n",
       ">>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
       ">>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
       "[Row(d='100')]\n",
       ">>> df = spark.createDataFrame([('foo',)], ['str'])\n",
       ">>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
       "[Row(d='')]\n",
       ">>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
       ">>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
       "[Row(d='')]\n",
       "\n",
       ".. versionadded:: 1.5\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.regexp_extract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[^0-9A-Za-z$&%=\\u200d♀♁⚥️🌀-🌡🌤-🎓🎞-🏰🐀-📽📿-🔽🕐-🕧🗺-🙏🚀-🛅\\U0001f90c-🤺🥇-\\U0001f978🥺-\\U0001f9cb\\U0001f9cd-🧿\\U0001fa90-\\U0001faa8]',\n",
       " 'http.*']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.getCleanupPatterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "texts = df.select(\"text\")\n",
    "\n",
    "# pipeline = lda_pipeline.build_pipeline()\n",
    "processed_texts = pipeline.fit(texts).transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.09 ms, sys: 4.02 ms, total: 10.1 ms\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# tfizer = CountVectorizer(inputCol='finished_normalized',\n",
    "#                          outputCol='tf_features')\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='finished_ngrams',\n",
    "                         outputCol='tf_features')\n",
    "\n",
    "tf_model = tfizer.fit(processed_texts)\n",
    "tf_result = tf_model.transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import IDF\n",
    "idfizer = IDF(inputCol='tf_features', \n",
    "              outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 238 µs, sys: 3.57 ms, total: 3.81 ms\n",
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "lda = LDA(k=num_topics, \n",
    "          maxIter=max_iter, \n",
    "          featuresCol=\"tf_idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 ms, sys: 4.91 ms, total: 22 ms\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "lda_result = topics.select('topicWords').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: aoc_ted_cruz gme_bb_amc buy_gme_bb 🙌_💎_🙌 💎_🙌_💎 paper_trade_contest head_spce_🌕 definitely_nothing_suspicious 🦍_🚀🚀🚀_head spce_🌕_next \n",
      "\n",
      "Topic 1: hold_hold_hold buy_doge_buy doge_buy_doge buy_buy_buy fucking_wall_street disclaimer_financial_advice halt_amc_trade volatility_ig_people extreme_volatility_ig gamestop_amc_due \n",
      "\n",
      "Topic 2: amc_bb_nok buy_spy_put gme_amc_bb play_spy_put loss_porn_wifes leave_going_hold keep_nakd_strong dip_aint_fucking boyfriend_tell_tohold wifes_boyfriend_tell \n",
      "\n",
      "Topic 3: 🚀_🚀_🚀 want_play_dirty occupy_wall_street tendie_rangers_tohold government_infiltrate_disrupt movement_die_like like_occupy_wall past_indicator_government important_movement_die take_step_prevent \n",
      "\n",
      "Topic 4: robinhood_cancel_order class_action_lawsuit aint_much_honest much_honest_work history_strap_boy strap_boy_girl fukkkin_history_strap gme_gang_make gang_make_fukkkin make_fukkkin_history \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(lda_result.shape[0]):\n",
    "    print(f\"Topic {i}:\", \n",
    "          *lda_result.iloc[i].topicWords,\n",
    "         \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare pipeline time usage with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f58d2553d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f58d2569590>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f58d2830c20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f58d2830d70>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f58d24b7cd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f58d24c6e60>),\n",
       " ('preprocessor', <__main__.FilterTextPreprocessing at 0x7f58d27acdd0>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "data_path = \"reddit_wsb.csv\"\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class FilterTextPreprocessing:\n",
    "    def __init__(self, nlp):\n",
    "        Doc.set_extension('bow', default=[], force=True)\n",
    "        Token.set_extension('keep', default=True, force=True)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        patterns = [\n",
    "            {\"string_id\": \"stop_word\", \"pattern\": [[{\"IS_STOP\": True}]]},\n",
    "            {\"string_id\": \"punctuation\", \"pattern\": [[{\"IS_PUNCT\": True}]]},\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for patt_obj in patterns:\n",
    "            string_id = patt_obj.get('string_id')\n",
    "            pattern = patt_obj.get('pattern')\n",
    "            self.matcher.add(string_id, pattern, on_match=self.on_match)\n",
    "   \n",
    "    def on_match(self, matcher, doc, i, matches):\n",
    "        _, start, end = matches[i]\n",
    "        for tkn in doc[start:end]:\n",
    "            tkn._.keep = False\n",
    "              \n",
    "    def __call__(self, doc) :\n",
    "        self.matcher(doc)\n",
    "        doc._.bow = [tkn.lemma_ for tkn in doc if tkn._.keep]\n",
    "        return doc\n",
    "      \n",
    "#     @classmethod\n",
    "#     def from_pattern_file(cls, nlp, path) :\n",
    "#         patterns = read_json(path)\n",
    "#         return cls(nlp, patterns)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@English.factory(\"preprocessor\")\n",
    "def create_preprocessor(nlp, name):\n",
    "    return FilterTextPreprocessing(nlp)\n",
    "\n",
    "# nlp.select_pipes(enable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"preprocessor\", last=True)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1000\n",
      "i = 2000\n",
      "i = 3000\n",
      "i = 4000\n",
      "i = 5000\n",
      "i = 6000\n",
      "i = 7000\n",
      "i = 8000\n",
      "i = 9000\n",
      "i = 10000\n",
      "i = 11000\n",
      "i = 12000\n",
      "i = 13000\n",
      "i = 14000\n",
      "i = 15000\n",
      "i = 16000\n",
      "i = 17000\n",
      "i = 18000\n",
      "i = 19000\n",
      "i = 20000\n",
      "i = 21000\n",
      "i = 22000\n",
      "i = 23000\n",
      "i = 24000\n",
      "i = 25000\n",
      "CPU times: user 6min 7s, sys: 2.59 s, total: 6min 10s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process(filename):\n",
    "    with open(filename, \"r\") as fobj:\n",
    "        datareader = csv.DictReader(fobj)\n",
    "        for row in datareader:\n",
    "            text = \" \".join([row[\"title\"],\n",
    "                              row[\"body\"]])\n",
    "            yield nlp(text)\n",
    "            \n",
    "gen = process(data_path)\n",
    "\n",
    "words = []\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        doc = next(gen)\n",
    "        words.append(doc._.bow)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f\"i = {i}\")\n",
    "    \n",
    "words =  pd.Series(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [money, send, message, 🚀, 💎, 🙌]\n",
       "1        [Math, Professor, Scott, Steiner, say, number,...\n",
       "2        [exit, system, CEO, NASDAQ, push, halt, tradin...\n",
       "3             [new, SEC, filing, GME, retarded, interpret]\n",
       "4              [distract, GME, think, AMC, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                 [hold, GME, 🚀, 🚀, 🚀]\n",
       "25644                    [AMC, Yolo, Update, Feb, 3, 2021]\n",
       "25645                                         [loss, sell]\n",
       "25646     [post, curiosity, teem, know, store, 👀, 💎, 🖐, 🚀]\n",
       "Length: 25647, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [money, send, message]\n",
       "1        [math, professor, scott, steiner, number, spel...\n",
       "2        [exit, system, ceo, nasdaq, push, halt, trade,...\n",
       "3               [new, sec, file, gme, retarded, interpret]\n",
       "4              [distract, gme, think, amc, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                          [hold, gme]\n",
       "25644                             [amc, yolo, update, feb]\n",
       "25645                                         [loss, sell]\n",
       "25646           [dont, post, curiosity, teem, know, store]\n",
       "Name: finished_unigrams, Length: 25647, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.finished_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pipeline = lda_pipeline.build_pipeline()\n",
    "# processed_texts = pipeline.fit(texts).transform(texts)\n",
    "# print(processed_texts)\n",
    "\n",
    "# for fair comparison with SpaCy below, should build pandas dataframe.\n",
    "# will throw TaskSetManager:66 - Stage 4 contains a task of very large size\n",
    "# df_post = processed_texts.toPandas()  \n",
    "# df_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.946236559139784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "371/18.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed comparison: The sparknlp pipeline took 18.6 seconds, while spaCy took 371 second (20x as long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with Stanza and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 25647 entries, 2021-01-28 21:37:41 to 2021-02-04 07:54:27\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      25647 non-null  object\n",
      " 1   title   25647 non-null  object\n",
      " 2   body    25647 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 801.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.read_csv(data_path,\n",
    "                 index_col=\"timestamp\", \n",
    "                 parse_dates=True, \n",
    "                 keep_default_na=False)\n",
    "# df_pd = df_pd.assign(timestamp=pd.to_datetime(df_pd.timestamp))\n",
    "df_pd = df_pd[[\"id\", \"title\", \"body\"]]\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = df.shape[0]//ddf.shape[0]\n",
    "dfs = [df.iloc[bin_size*i : bin_size*(i+1)] for i in range(ddf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I got in late on GME but I believe in the cause and am willing to lose it all.',\n",
       " \"You guys are amazing. Thank you for sending GME to the moon! I know I'm going to lose most of my money here because I'll hold the line until the end. Let's send a clear message to wall street with GME, BB, AMC, and any others. I've never day traded before but I'm in it now. 🚀\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = dfs[0]\n",
    "X = df0.iloc[2]\n",
    "X.title, X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 15:42:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-14 15:42:21 INFO: Use device: cpu\n",
      "2021-02-14 15:42:21 INFO: Loading: tokenize\n",
      "2021-02-14 15:42:21 INFO: Loading: pos\n",
      "2021-02-14 15:42:21 INFO: Loading: lemma\n",
      "2021-02-14 15:42:21 INFO: Loading: depparse\n",
      "2021-02-14 15:42:21 INFO: Loading: sentiment\n",
      "2021-02-14 15:42:22 INFO: Loading: ner\n",
      "2021-02-14 15:42:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ You guys are amazing.\n",
      "+ Thank you for sending GME to the moon!\n",
      "- I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "Ⓝ Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "Ⓝ I've never day traded before but I'm in it now.\n",
      "Ⓝ 🚀\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "text = df.iloc[2].body\n",
    "doc = nlp(text)\n",
    "d_sent = {0:\"-\", 1:\"Ⓝ\", 2:\"+\"}\n",
    "for sent in doc.sentences:\n",
    "    print(d_sent[sent.sentiment], sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ I just love it when the regulators step in.\n",
      "- That was amazingly boring.\n",
      "+ That was amazingly tolerable.\n",
      "- At least it wasn't boring.\n",
      "CPU times: user 1.43 s, sys: 21.1 ms, total: 1.45 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in [\"I just love it when the regulators step in.\",\n",
    "          \"That was amazingly boring.\",\n",
    "          \"That was amazingly tolerable.\",\n",
    "          \"At least it wasn't boring.\"]:\n",
    "    sentiment = nlp(s).sentences[0].sentiment\n",
    "    print(d_sent[sentiment], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 You guys are amazing.\n",
      "0.0 Thank you for sending GME to the moon!\n",
      "0.0 I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "0.0 Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "0.0 I've never day traded before\n",
      "0.0 but I'm in it now.\n",
      "0.0 🚀\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = df.iloc[2].body\n",
    "d_sent = {0:\"-\", 1:\"Ⓝ\", 2:\"+\"}\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.sentiment, sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This has notebook has grown to an unwieldy size. I accomplished a lot of what I wanted, but, after lots of playing around, I can't find a good way of normalizing repetitions. This if particular interest in the case of emojis. For instance, I would like to do replacement as in\n",
    " - 🚀       maps to 🚀\n",
    " - 🚀🚀...🚀 maps to 🚀🚀\n",
    " - 🚀\\s    maps to 🚀\n",
    "but I can't do this all simultaneously.\n",
    "\n",
    "Some notes (re SparkNLP 2.7.3):\n",
    " - I can do F.regex_replace at the document level, but this has to be done for one output character at a time.\n",
    " - I could probably try to use  UDF, but I think that would be a significant slowdown.\n",
    " - I can set tokenizer rules to get rid of long strings of a single emoji. Probably can deal with space-separated emojis at the same time. Can also use normalizer rules to do the same thing?\n",
    " - It seems complicated to set your own rules for a stemmer or a lemmatizer. \n",
    " \n",
    "I tried lots of things, but in the end I have settled on just extracting a string of all of the emojis that appear as a feature separate from the texts. Texts that use the character as a standing for a word will be ruined, those that use them as punctuation will become ungrammatical, but those that use them as decoration will be well-preserved. Below is the final form of lda_pipeline.py representative of how the file looked during the writing of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import (Tokenizer,\n",
    "                                SentenceDetector,\n",
    "                                Normalizer,\n",
    "                                LemmatizerModel,\n",
    "                                StopWordsCleaner,\n",
    "                                # NGramGenerator,\n",
    "                                # PerceptronModel\n",
    "                                )\n",
    "from pyspark.ml import Pipeline\n",
    "import unicodedata\n",
    "\n",
    "from download_pretrained import PretrainedCacheManager\n",
    "\n",
    "emoji_ranges = [\n",
    "        # ranges covering all emojis expressible\n",
    "        # using only one unicode character.\n",
    "        '\\U0001f300-\\U0001f321',\n",
    "        '\\U0001f324-\\U0001f393',\n",
    "        '\\U0001f39e-\\U0001f3f0',\n",
    "        '\\U0001f400-\\U0001f4fd',\n",
    "        '\\U0001f4ff-\\U0001f53d',\n",
    "        '\\U0001f550-\\U0001f567',\n",
    "        '\\U0001f5fa-\\U0001f64f',\n",
    "        '\\U0001f680-\\U0001f6c5',\n",
    "        '\\U0001f90c-\\U0001f93a',\n",
    "        '\\U0001f947-\\U0001f978',\n",
    "        '\\U0001f97a-\\U0001f9cb',\n",
    "        '\\U0001f9cd-\\U0001f9ff',\n",
    "        '\\U0001fa90-\\U0001faa8'\n",
    "]\n",
    "\n",
    "\n",
    "def get_unigram_pipeline_components():\n",
    "    # get acche manager to avoid repeated downloads\n",
    "    cache_manager = PretrainedCacheManager()\n",
    "    cache_manager.get_pretrained_components()\n",
    "    # this is a dict with entries as in\n",
    "    # ('lemmatizer', path-to-downloaded-unzipped-lemmatizer)\n",
    "    pretrained_components = cache_manager.pretrained_components\n",
    "\n",
    "    # get document assembler\n",
    "    assembler = DocumentAssembler()\n",
    "\n",
    "    # get sentence detector\n",
    "    sentence_detector = SentenceDetector()\n",
    "\n",
    "    # build tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # add ['‘', '’', '“', '”'] as context characters\n",
    "    # doing this in a verbose way because it may be clarifying.\n",
    "    char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "                  'RIGHT SINGLE QUOTATION MARK',\n",
    "                  'LEFT DOUBLE QUOTATION MARK',\n",
    "                  'RIGHT DOUBLE QUOTATION MARK']\n",
    "    for name in char_names:\n",
    "        tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "    # now set exceptions.\n",
    "    # 1) to preserve word preceding \"sell\" and \"hold\",\n",
    "    #    so that, e.g., \"don't sell\" is not normalized to \"sell\".\n",
    "    # 2) don't split \"game stop\" (...or \"game stonk\")\n",
    "    # 3) if an emoji is repeated with spaces, don't split\n",
    "    #    so we can normalize later\n",
    "    tokenizer.setExceptions([\"\\S+ sell\",\n",
    "                             \"\\S+ hold\",\n",
    "                             \"game [stop|stonk]\",\n",
    "                             \"\\U0001f680\\s+\", # rocket ship\n",
    "                             \"\\U0001f48e\\U0001f64c\\s+\"\n",
    "                            ])\n",
    "    tokenizer.setCaseSensitiveExceptions(True)\n",
    "\n",
    "    # built stopwords cleaner\n",
    "    stopwords_cleaner = (\n",
    "        StopWordsCleaner()\n",
    "        # on second thought, maybe the larger list of stopwords that I\n",
    "        # was downloading was too expansive. e.g., it has the word \"example\"\n",
    "        # .load(pretrained_components[\"stopwords\"])\n",
    "        .setCaseSensitive(False)\n",
    "    )\n",
    "    # for each stopword involving an apostrophe ('), append\n",
    "    # a version of the stopword using the character (’) instead,\n",
    "    # and a version with the apostrophe missing\n",
    "    char = unicodedata.lookup('APOSTROPHE')\n",
    "    replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "    stopwords = stopwords_cleaner.getStopWords()\n",
    "    stopwords += [\"y'all\", \"yall\"]\n",
    "    for s in stopwords_cleaner.getStopWords():\n",
    "        if char in s:\n",
    "            stopwords.append(s.replace(char, replacement))\n",
    "            stopwords.append(s.replace(char, \"\"))\n",
    "    stopwords.sort()\n",
    "    stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "    # build lemmatizer\n",
    "    lemmatizer = (\n",
    "        LemmatizerModel().load(pretrained_components[\"lemmatizer\"])\n",
    "    )\n",
    "\n",
    "    # build normalizer\n",
    "    normalizer = (\n",
    "        Normalizer()\n",
    "        .setLowercase(True)\n",
    "    )\n",
    "    # this does not keep all emojis, but it keeps a lot of them.\n",
    "    # for instance, it does not distinguish skin color, but it has\n",
    "    # enough characters to express the Becky emoji.\n",
    "    keeper_regex = ''.join([\n",
    "        '[^0-9A-Za-z$&%=',\n",
    "        # special characters for Becky\n",
    "        '\\u200d\\u2640\\u2641\\u26A5\\ufe0f',\n",
    "        ''.join(emoji_ranges),\n",
    "        ']'\n",
    "    ])\n",
    "    normalizer.setCleanupPatterns([keeper_regex,\n",
    "                                   'http.*'])\n",
    "\n",
    "    # build finisher\n",
    "    finisher = Finisher()\n",
    "\n",
    "    return (assembler, sentence_detector, tokenizer,\n",
    "            stopwords_cleaner, lemmatizer, normalizer, finisher)\n",
    "\n",
    "\n",
    "def get_Ngram_pipeline_components(N=2):\n",
    "    return\n",
    "\n",
    "\n",
    "def build_unigram_pipeline(pipeline_components=None):\n",
    "    # get_pipeline_components\n",
    "    if not pipeline_components:\n",
    "        _ = get_unigram_pipeline_components()\n",
    "    else:\n",
    "        _ = pipeline_components\n",
    "    (assembler,\n",
    "     sentence_detector,\n",
    "     tokenizer,\n",
    "     stopwords_cleaner,\n",
    "     lemmatizer,\n",
    "     normalizer,\n",
    "     finisher) = _\n",
    "\n",
    "    # assemble the pipeline\n",
    "    (assembler\n",
    "     .setInputCol('text')\n",
    "     .setOutputCol('document'))\n",
    "\n",
    "    (sentence_detector\n",
    "     .setInputCols(['document'])\n",
    "     .setOutputCol('sentences'))\n",
    "\n",
    "    (tokenizer\n",
    "     .setInputCols(['sentences'])\n",
    "     .setOutputCol('tokenized'))\n",
    "\n",
    "    (stopwords_cleaner\n",
    "     .setInputCols(['tokenized'])\n",
    "     .setOutputCol('cleaned'))\n",
    "\n",
    "    (lemmatizer\n",
    "     .setInputCols(['cleaned'])\n",
    "     .setOutputCol('lemmatized'))\n",
    "\n",
    "    (normalizer\n",
    "     .setInputCols(['lemmatized'])\n",
    "     .setOutputCol('unigrams'))\n",
    "\n",
    "    (finisher\n",
    "     .setInputCols(['unigrams']))\n",
    "\n",
    "    pipeline = (Pipeline()\n",
    "                .setStages([assembler,\n",
    "                            sentence_detector,\n",
    "                            tokenizer,\n",
    "                            stopwords_cleaner,\n",
    "                            lemmatizer,\n",
    "                            normalizer,\n",
    "                            finisher]))\n",
    "\n",
    "    # to do: try LightPipeline as in\n",
    "    # https://nlp.johnsnowlabs.com/docs/en/concepts#lightpipeline\n",
    "\n",
    "    return pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

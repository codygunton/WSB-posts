{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP\n",
    "and the O'Reilly Spark NLP book, page 76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "The character encoding of the csv file:\n",
      "../data/reddit_wsb.csv: application/csv; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "data_path = \"../data/reddit_wsb.csv\"\n",
    "print(\"\\nThe character encoding of the csv file:\")\n",
    "! file -i {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set multiline=True, we have to use use Java 8. Even still, the column body containing commas within quotes containing quotes, and this confused the csv parser. Solved following https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "%aimport lda_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine particular pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('tokenized')\n",
    ")\n",
    "\n",
    "# tokenizer.addSplitChars(\n",
    "#     unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# )\n",
    "\n",
    "# char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "#               'RIGHT SINGLE QUOTATION MARK',\n",
    "#               'LEFT DOUBLE QUOTATION MARK',\n",
    "#               'RIGHT DOUBLE QUOTATION MARK']\n",
    "# for name in char_names:\n",
    "#     tokenizer.addContextChars(unicodedata.lookup(name))\n",
    "\n",
    "\n",
    "stopwords_cleaner = (\n",
    "    StopWordsCleaner.pretrained(\"stopwords_en\", \"en\")\n",
    "    .setInputCols(['tokenized'])\n",
    "    .setOutputCol('cleaned')\n",
    "    .setCaseSensitive(False)\n",
    ")\n",
    "\n",
    "# char = unicodedata.lookup('APOSTROPHE')\n",
    "# replacement = unicodedata.lookup('RIGHT SINGLE QUOTATION MARK')\n",
    "# stopwords = stopwords_cleaner.getStopWords()\n",
    "# for s in stopwords_cleaner.getStopWords():\n",
    "#     if char in s:\n",
    "#         stopwords.append(s.replace(char, replacement))\n",
    "# stopwords.sort()\n",
    "# stopwords_cleaner.setStopWords(stopwords)\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols(['tokenized', 'cleaned'])\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,\n",
    "                                 tokenizer,\n",
    "                                 stopwords_cleaner,\n",
    "                                 finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------+\n",
      "|text                               |finished_normalized|\n",
      "+-----------------------------------+-------------------+\n",
      "|Cody wrote one example text.       |[cody, write, text]|\n",
      "|It's not much better than the rest!|[rest]             |\n",
      "+-----------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Cody wrote one example text.\",\n",
    "    \"It's not much better than the rest!\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a's able about above according accordingly across actually after afterwards again against ain't all allow allows almost alone along already also although always am among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren't around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c'mon c's came can can't cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn't course currently d definitely described despite did didn't different do does doesn't doing don't done down downwards during e each edu eg eight either else elsewhere enough entirely especially et etc even ever every everybody everyone everything everywhere ex exactly example except f far few fifth first five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn't happens hardly has hasn't have haven't having he he's hello help hence her here here's hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i'd i'll i'm i've ie if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn't it it'd it'll it's its itself j just k keep keeps kept know knows known l last lately later latter latterly least less lest let let's like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly much must my myself n name namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right s said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn't since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would would wouldn't x y yes yet you you'd you'll you're you've your yours yourself yourselves z zero\n"
     ]
    }
   ],
   "source": [
    "print(*stopwords_cleaner.getStopWords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.getContextChars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|text                                                                     |finished_tokenized                                                                     |finished_cleaned                                                         |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|It's was its own problem, wasn't it?                                     |[It's, was, its, own, problem, ,, wasn't, it, ?]                                       |[problem, ,, ?]                                                          |\n",
      "|420 wasn‚Äôt a meme. GME üöÄ üöÄ üöÄ                                          |[420, wasn‚Äôt, a, meme, ., GME, üöÄ, üöÄ, üöÄ]                                             |[420, wasn‚Äôt, meme, ., GME, üöÄ, üöÄ, üöÄ]                                  |\n",
      "|halt trading ‚Äúto give investors a chance to recalibrate their positions‚Äù.|[halt, trading, ‚Äúto, give, investors, a, chance, to, recalibrate, their, positions‚Äù, .]|[halt, trading, ‚Äúto, give, investors, chance, recalibrate, positions‚Äù, .]|\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"It's was its own problem, wasn't it?\",\n",
    "    \"420 wasn‚Äôt a meme. GME üöÄ üöÄ üöÄ\",\n",
    "    \"halt trading ‚Äúto give investors a chance to recalibrate their positions‚Äù.\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_df = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result = pipeline_model.transform(eg_df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Converting from Pandas df via df = spark.createDataFrame(df_pd) gives\n",
    "# >> WARN  TaskSetManager:66 - Stage 2 contains a task of very large size \n",
    "# >> (1473 KB). The maximum recommended task size is 100 KB.\n",
    "\n",
    "df = spark.read.csv(data_path, \n",
    "                    header=True,\n",
    "                    multiLine=True, \n",
    "                    quote=\"\\\"\", \n",
    "                    escape=\"\\\"\")\n",
    "\n",
    "df = df.sample(withReplacement=False, fraction=0.05, seed=1)\n",
    "# print(f'{df.where(df[\"timestamp\"].isNull()).count()} null timestamp values.')\n",
    "\n",
    "# combine text columns and drop unwanted columns\n",
    "df = (\n",
    "    df.withColumn(\"text\", \n",
    "               F.concat_ws(\". \", df.title, df.body))\n",
    " .drop(\"title\", \"body\", \"url\", \"comms_num\", \"created\")\n",
    ")\n",
    "\n",
    "texts = df.select(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[text: string, finished_normalized: array<string>]\n",
      "CPU times: user 163 ms, sys: 37.4 ms, total: 201 ms\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline = lda_pipeline.build_pipeline()\n",
    "processed_texts = pipeline.fit(texts).transform(texts)\n",
    "print(processed_texts)\n",
    "\n",
    "pddf = processed_texts.toPandas()\n",
    "def examine(i):\n",
    "    sep_string = \"\\n\" + \"-\"*100 + \"\\n\"\n",
    "    print(*pddf.iloc[i], sep=sep_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GME Wars: Suits Strike Back. WARNING FOR ALL YOU FELLOW AUTISTS. Obligatory I‚Äôm not a financial advisor this is not blah blah blah.\n",
      "\n",
      "Okay GME gang. We‚Äôve got the shorts on the ropes. They‚Äôre bleeding hard and all the billionaire suits are shitting themselves out of fear. They‚Äôve tried false information, getting on CNBC and screaming for regulation of our ‚Äúmarket manipulation,‚Äù trying to get us shut down, Biden administration tweet, saying shorts have already covered, etc. None of it has worked so far and they‚Äôre still bleeding money.\n",
      "\n",
      "Now they‚Äôre truly desperate. What‚Äôs something they haven‚Äôt tried quite yet but very well could be setting up to do?\n",
      "\n",
      "MARKET CRASH\n",
      "\n",
      "Yes, that‚Äôs right. I believe with what I‚Äôm currently seeing, they are going to crash the market and blame it on us. Do I think you need to be concerned about GME? No, because holding it should still work and they still have to cover anyway, but this is definitely to try to get you scared and sell GME and so they can take your money back when you reinvest elsewhere.\n",
      "\n",
      "I don‚Äôt think I have to tell many of you that the current levels are at/above where the market was valued in 2008 before the crash according to the Buffet indicator. All this says is that we‚Äôre probably in a market bubble, and it‚Äôs been at these levels for months now. But guess what just started getting talked about by all the suits? That‚Äôs right. A bubble. And who are they blaming? Us.\n",
      "\n",
      "Now that I have your attention, let‚Äôs look at a few other things.\n",
      "\n",
      "[This is the Volatility Index the past week.](https://imgur.com/gallery/C9ETmMR) Notice how it‚Äôs been steadily rising and suddenly spiked today? 40 is typically the resistance before the market makes a downturn. If this thing gets above that we‚Äôll probably be seeing the market dropping quickly.\n",
      "\n",
      "Additionally, there the spurts of high volume absolutely dumping stocks for short amounts of time. That‚Äôs one of the first things I noticed before the March crash and I‚Äôve been seeing it get more and more abundant the last few weeks and picking up a lot today. [Notice the large red volume candles on the weekly SPY chart.](https://imgur.com/a/w0y33Lq) (Robinhood screenshot on a DD post??? Yes, I know. I‚Äôm on my phone with limited time. Deal with it lol.)\n",
      "\n",
      "Now you might say, ‚Äúit‚Äôs earnings season and there have been some bad earnings so far and that‚Äôs expected in this tough time.‚Äù Well yes, but there‚Äôs the Apple earnings today. They beat earnings pretty nicely and what did the stock do? [You guessed it (I hope).](https://imgur.com/a/Txc7po6)\n",
      "\n",
      "Currently we‚Äôre seeing fundamentally strong companies as well as the S&P 500 beginning to drop and the volatility increase market wide. You could say this is ‚Äúsmart money‚Äù pulling out because they see the overvalued stocks. However, there were many worldwide financial issues sparking before the March crash. It just needed a catalyst, AKA global pandemic. Now we‚Äôre in a similar boat. Market issues in need of a catalyst.\n",
      "\n",
      "What is that catalyst this time around? That‚Äôs right. A bunch of ‚Äúmillennials on a message board who have no place in stocks creating a market bubble.‚Äù Basically this isn‚Äôt smart money pulling out here. It‚Äôs MAD MONEY (hi Cramer). They are declaring WAR. They are pissed that we are ‚Äústealing‚Äù their billions on billions of dollars from them and they will do whatever it takes to get you to sell and take your money away from you again, including wrecking the market and the economy once again. The suits are out for blood now.\n",
      "\n",
      "Do I think this will effect GME much? Only if you don‚Äôt sell. All this is scare tactics and ways to win your money back if keeping GME suppressed fails even more than it already has. There are hardly any shares to short if any at all and retail investors and autists are likely the ones holding most of the shares that could be sold.\n",
      "\n",
      "Be careful out there autists\n",
      "\n",
      "TLDR: GME TO VALHALLA. Rest of the market, potentially to the depths of Avernus soon.\n",
      "\n",
      "Positions, GME Feb 19th $115 calls\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['gme', 'wars', 'suits', 'strike', 'back', 'warning', 'fellow', 'autists', 'obligatory', 'financial', 'advisor', 'blah', 'blah', 'blah', 'gme', 'gang', 'shorts', 'rope', 'bleed', 'hard', 'billionaire', 'suit', 'shit', 'fear', 'false', 'information', 'cnbc', 'scream', 'regulation', 'market', 'manipulation', 'shut', 'biden', 'administration', 'tweet', 'shorts', 'cover', 'work', 'bleed', 'money', 'desperate', 'set', 'market', 'crash', 'crash', 'market', 'blame', 'concerned', 'gme', 'hold', 'work', 'cover', 'scared', 'sell', 'gme', 'money', 'back', 'reinvest', 'current', 'level', 'atabove', 'market', 'value', '2008', 'crash', 'buffet', 'indicator', 'market', 'bubble', 'level', 'month', 'guess', 'start', 'talk', 'suit', 'bubble', 'blame', 'attention', 'thing', 'this', 'volatility', 'index', 'past', 'week', 'notice', 'steadily', 'rise', 'suddenly', 'spike', 'today', '40', 'typically', 'resistance', 'market', 'make', 'downturn', 'thing', 'market', 'drop', 'quickly', 'additionally', 'spurt', 'high', 'volume', 'absolutely', 'dump', 'stock', 'short', 'amount', 'time', 'thing', 'notice', 'march', 'crash', 'abundant', 'week', 'pick', 'lot', 'today', 'notice', 'large', 'red', 'volume', 'candle', 'weekly', 'spy', 'chart', 'robinhood', 'screenshot', 'dd', 'post', 'phone', 'limit', 'time', 'deal', 'lol', 'earnings', 'season', 'bad', 'earnings', 'expect', 'tough', 'time', 'apple', 'earnings', 'today', 'beat', 'earnings', 'pretty', 'nicely', 'stock', 'you', 'guess', 'hope', 'fundamentally', 'strong', 'company', 's&p', '500', 'begin', 'drop', 'volatility', 'increase', 'market', 'wide', 'smart', 'money', 'pull', 'overvalue', 'stock', 'worldwide', 'financial', 'issue', 'spark', 'march', 'crash', 'need', 'catalyst', 'aka', 'global', 'pandemic', 'similar', 'boat', 'market', 'issue', 'catalyst', 'catalyst', 'time', 'bunch', 'millennials', 'message', 'board', 'place', 'stock', 'create', 'market', 'bubble', 'basically', 'smart', 'money', 'pull', 'mad', 'money', 'cramer', 'declare', 'war', 'piss', 'steal', 'billions', 'billions', 'dollar', 'take', 'sell', 'money', 'include', 'wreck', 'market', 'economy', 'suit', 'blood', 'effect', 'gme', 'sell', 'scare', 'tactic', 'way', 'win', 'money', 'back', 'keep', 'gme', 'suppress', 'fail', 'share', 'short', 'retail', 'investor', 'autists', 'hold', 'share', 'sell', 'careful', 'autists', 'tldr', 'gme', 'valhalla', 'rest', 'market', 'potentially', 'depth', 'avernus', 'positions', 'gme', 'feb', '19th', '$115', 'call']\n"
     ]
    }
   ],
   "source": [
    "examine(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assembler -> tokenizer -> cleaner -> lemmatizer -> normalizer ->\n",
    "with stopwords_cleaner given from pretrained\n",
    "\n",
    "### Some inspection results (varying i-values in examine(i) below\n",
    "  1. emoji's dropped\n",
    "  2. \"y'all\" |-> \"yall\"\n",
    "  3. becomes one word\n",
    "  4. \"am\" actually comes from \"2am\"; should let numerals survive\n",
    "  8. \"I'm\" |-> im; \"its\" and \"lets\" and \"thats\" and \"isnt\" survive; 2008 is dropped\n",
    "  \n",
    " Conclusions:\n",
    "   - ‚úì should keep: numerals, $, &\n",
    "   - ‚úì long urls\n",
    "   - repeated characters as in \"holdddddd\" and \"woooooo\" and üöÄ, üöÄ, üöÄ\n",
    "   - ‚úì contractions not handled properly\n",
    "       added contractions with RIGHT SINGLE QUOTATION MARK to stopwords list\n",
    "   - keep emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------+\n",
      "|text                                        |finished_normalized|\n",
      "+--------------------------------------------+-------------------+\n",
      "|The S&P will go down and we'll have $100000.|[s&p, $100000]     |\n",
      "|Will, let's be sure your calculation is üíØ. |[calculation]      |\n",
      "|It's was its own problem, 0 wasn't it?      |[problem, 0]       |\n",
      "|420 wasn‚Äôt a meme. GME üöÄ üöÄ üöÄ             |[420, meme, gme]   |\n",
      "|whaaaatttT???                               |[whaaaatttt]       |\n",
      "+--------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"The S&P will go down and we'll have $100000.\",\n",
    "    \"Will, let's be sure your calculation is üíØ.\",\n",
    "    \"It's was its own problem, 0 wasn't it?\",\n",
    "    \"420 wasn‚Äôt a meme. GME üöÄ üöÄ üöÄ\",\n",
    "    \"whaaaatttT???\"\n",
    "]\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "eg_texts = spark.createDataFrame(pd.DataFrame({\"text\": text_list}))\n",
    "\n",
    "pipeline = lda_pipeline.build_pipeline()\n",
    "eg_processed_texts = pipeline.fit(eg_texts).transform(eg_texts)\n",
    "\n",
    "eg_processed_texts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('‚Äô', '0x2019') (\"'\", '0x27')\n"
     ]
    }
   ],
   "source": [
    "# a-ha!\n",
    "s1 = text_list[3]\n",
    "s2 = \"420 wasn't a meme. GME üöÄ üöÄ üöÄ\"\n",
    "assert len(s1) == len(s2)\n",
    "for c1, c2 in zip(s1, s2):\n",
    "    ord1, ord2 = ord(c1), ord(c2)\n",
    "    if ord1 != ord2:\n",
    "        print((c1, hex(ord1)), (c2, hex(ord2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between \"apostrophe\" and \"right single quotation mark\". examine(0) suggests that I have a problem with \"* double quotation mark\" as well, leading to preservation of \"to\" when I process \"‚Äúto\". On page 281 they replace these things one-by-one using pythongs str.replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT SINGLE QUOTATION MARK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‚Äò', '‚Äô', '‚Äú', '‚Äù']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(unicodedata.name(\"\\u2018\"))\n",
    "char_names = ['LEFT SINGLE QUOTATION MARK',\n",
    "              'RIGHT SINGLE QUOTATION MARK',\n",
    "              'LEFT DOUBLE QUOTATION MARK',\n",
    "              'RIGHT DOUBLE QUOTATION MARK']\n",
    "[unicodedata.lookup(name) for name in char_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 3.46 ms, total: 14 ms\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tfizer = CountVectorizer(inputCol='finished_unigrams',\n",
    "                         outputCol='tf_features')\n",
    "tf_model = tfizer.fit(processed_texts)\n",
    "tf_result = tf_model.transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.28 ms, total: 12.8 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import IDF\n",
    "idfizer = IDF(inputCol='tf_features', \n",
    "              outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 431 ¬µs, total: 2.89 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "lda = LDA(k=num_topics, \n",
    "          maxIter=max_iter, \n",
    "          featuresCol=\"tf_idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 ms, sys: 9.54 ms, total: 18.8 ms\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------+\n",
      "|topic|                                              topicWords|\n",
      "+-----+--------------------------------------------------------+\n",
      "|    0|                             [buy, gme, short, hold, xb]|\n",
      "|    1|                           [', stock, market, it's, gon]|\n",
      "|    2|[clearinghouse, webull, portfolio, schwab, organization]|\n",
      "|    3|                      [security, margin, gon, uh, cheap]|\n",
      "|    4|                         [fund, hedge, money, let, play]|\n",
      "+-----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 5\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare pipeline time usage with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f58d2553d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f58d2569590>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f58d2830c20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f58d2830d70>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f58d24b7cd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f58d24c6e60>),\n",
       " ('preprocessor', <__main__.FilterTextPreprocessing at 0x7f58d27acdd0>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "data_path = \"reddit_wsb.csv\"\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class FilterTextPreprocessing:\n",
    "    def __init__(self, nlp):\n",
    "        Doc.set_extension('bow', default=[], force=True)\n",
    "        Token.set_extension('keep', default=True, force=True)\n",
    "        \n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        patterns = [\n",
    "            {\"string_id\": \"stop_word\", \"pattern\": [[{\"IS_STOP\": True}]]},\n",
    "            {\"string_id\": \"punctuation\", \"pattern\": [[{\"IS_PUNCT\": True}]]},\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for patt_obj in patterns:\n",
    "            string_id = patt_obj.get('string_id')\n",
    "            pattern = patt_obj.get('pattern')\n",
    "            self.matcher.add(string_id, pattern, on_match=self.on_match)\n",
    "   \n",
    "    def on_match(self, matcher, doc, i, matches):\n",
    "        _, start, end = matches[i]\n",
    "        for tkn in doc[start:end]:\n",
    "            tkn._.keep = False\n",
    "              \n",
    "    def __call__(self, doc) :\n",
    "        self.matcher(doc)\n",
    "        doc._.bow = [tkn.lemma_ for tkn in doc if tkn._.keep]\n",
    "        return doc\n",
    "      \n",
    "#     @classmethod\n",
    "#     def from_pattern_file(cls, nlp, path) :\n",
    "#         patterns = read_json(path)\n",
    "#         return cls(nlp, patterns)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@English.factory(\"preprocessor\")\n",
    "def create_preprocessor(nlp, name):\n",
    "    return FilterTextPreprocessing(nlp)\n",
    "\n",
    "# nlp.select_pipes(enable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"preprocessor\", last=True)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1000\n",
      "i = 2000\n",
      "i = 3000\n",
      "i = 4000\n",
      "i = 5000\n",
      "i = 6000\n",
      "i = 7000\n",
      "i = 8000\n",
      "i = 9000\n",
      "i = 10000\n",
      "i = 11000\n",
      "i = 12000\n",
      "i = 13000\n",
      "i = 14000\n",
      "i = 15000\n",
      "i = 16000\n",
      "i = 17000\n",
      "i = 18000\n",
      "i = 19000\n",
      "i = 20000\n",
      "i = 21000\n",
      "i = 22000\n",
      "i = 23000\n",
      "i = 24000\n",
      "i = 25000\n",
      "CPU times: user 6min 7s, sys: 2.59 s, total: 6min 10s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process(filename):\n",
    "    with open(filename, \"r\") as fobj:\n",
    "        datareader = csv.DictReader(fobj)\n",
    "        for row in datareader:\n",
    "            text = \" \".join([row[\"title\"],\n",
    "                              row[\"body\"]])\n",
    "            yield nlp(text)\n",
    "            \n",
    "gen = process(data_path)\n",
    "\n",
    "words = []\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        doc = next(gen)\n",
    "        words.append(doc._.bow)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1\n",
    "    if i%1000 == 0:\n",
    "        print(f\"i = {i}\")\n",
    "    \n",
    "words =  pd.Series(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [money, send, message, üöÄ, üíé, üôå]\n",
       "1        [Math, Professor, Scott, Steiner, say, number,...\n",
       "2        [exit, system, CEO, NASDAQ, push, halt, tradin...\n",
       "3             [new, SEC, filing, GME, retarded, interpret]\n",
       "4              [distract, GME, think, AMC, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                 [hold, GME, üöÄ, üöÄ, üöÄ]\n",
       "25644                    [AMC, Yolo, Update, Feb, 3, 2021]\n",
       "25645                                         [loss, sell]\n",
       "25646     [post, curiosity, teem, know, store, üëÄ, üíé, üñê, üöÄ]\n",
       "Length: 25647, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [money, send, message]\n",
       "1        [math, professor, scott, steiner, number, spel...\n",
       "2        [exit, system, ceo, nasdaq, push, halt, trade,...\n",
       "3               [new, sec, file, gme, retarded, interpret]\n",
       "4              [distract, gme, think, amc, brother, aware]\n",
       "                               ...                        \n",
       "25642                                               [sign]\n",
       "25643                                          [hold, gme]\n",
       "25644                             [amc, yolo, update, feb]\n",
       "25645                                         [loss, sell]\n",
       "25646           [dont, post, curiosity, teem, know, store]\n",
       "Name: finished_unigrams, Length: 25647, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.finished_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pipeline = lda_pipeline.build_pipeline()\n",
    "# processed_texts = pipeline.fit(texts).transform(texts)\n",
    "# print(processed_texts)\n",
    "\n",
    "# for fair comparison with SpaCy below, should build pandas dataframe.\n",
    "# will throw TaskSetManager:66 - Stage 4 contains a task of very large size\n",
    "# df_post = processed_texts.toPandas()  \n",
    "# df_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.946236559139784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "371/18.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed comparison: The sparknlp pipeline took 18.6 seconds, while spaCy took 371 second (20x as long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with Stanza and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 25647 entries, 2021-01-28 21:37:41 to 2021-02-04 07:54:27\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      25647 non-null  object\n",
      " 1   title   25647 non-null  object\n",
      " 2   body    25647 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 801.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.read_csv(data_path,\n",
    "                 index_col=\"timestamp\", \n",
    "                 parse_dates=True, \n",
    "                 keep_default_na=False)\n",
    "# df_pd = df_pd.assign(timestamp=pd.to_datetime(df_pd.timestamp))\n",
    "df_pd = df_pd[[\"id\", \"title\", \"body\"]]\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = df.shape[0]//ddf.shape[0]\n",
    "dfs = [df.iloc[bin_size*i : bin_size*(i+1)] for i in range(ddf.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I got in late on GME but I believe in the cause and am willing to lose it all.',\n",
       " \"You guys are amazing. Thank you for sending GME to the moon! I know I'm going to lose most of my money here because I'll hold the line until the end. Let's send a clear message to wall street with GME, BB, AMC, and any others. I've never day traded before but I'm in it now. üöÄ\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = dfs[0]\n",
    "X = df0.iloc[2]\n",
    "X.title, X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 15:42:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-02-14 15:42:21 INFO: Use device: cpu\n",
      "2021-02-14 15:42:21 INFO: Loading: tokenize\n",
      "2021-02-14 15:42:21 INFO: Loading: pos\n",
      "2021-02-14 15:42:21 INFO: Loading: lemma\n",
      "2021-02-14 15:42:21 INFO: Loading: depparse\n",
      "2021-02-14 15:42:21 INFO: Loading: sentiment\n",
      "2021-02-14 15:42:22 INFO: Loading: ner\n",
      "2021-02-14 15:42:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ You guys are amazing.\n",
      "+ Thank you for sending GME to the moon!\n",
      "- I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "‚ìÉ Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "‚ìÉ I've never day traded before but I'm in it now.\n",
      "‚ìÉ üöÄ\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(\"en\")\n",
    "text = df.iloc[2].body\n",
    "doc = nlp(text)\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "for sent in doc.sentences:\n",
    "    print(d_sent[sent.sentiment], sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ I just love it when the regulators step in.\n",
      "- That was amazingly boring.\n",
      "+ That was amazingly tolerable.\n",
      "- At least it wasn't boring.\n",
      "CPU times: user 1.43 s, sys: 21.1 ms, total: 1.45 s\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for s in [\"I just love it when the regulators step in.\",\n",
    "          \"That was amazingly boring.\",\n",
    "          \"That was amazingly tolerable.\",\n",
    "          \"At least it wasn't boring.\"]:\n",
    "    sentiment = nlp(s).sentences[0].sentiment\n",
    "    print(d_sent[sentiment], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for word in doc.sentences[0].words:\n",
    "    print(word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 You guys are amazing.\n",
      "0.0 Thank you for sending GME to the moon!\n",
      "0.0 I know I'm going to lose most of my money here because I'll hold the line until the end.\n",
      "0.0 Let's send a clear message to wall street with GME, BB, AMC, and any others.\n",
      "0.0 I've never day traded before\n",
      "0.0 but I'm in it now.\n",
      "0.0 üöÄ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = df.iloc[2].body\n",
    "d_sent = {0:\"-\", 1:\"‚ìÉ\", 2:\"+\"}\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.sentiment, sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "know\n",
      "you\n",
      "be\n",
      "trouble\n",
      "when\n",
      "you\n",
      "walk\n",
      "in\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I knew you were trouble when you walked in!\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
